{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy import interpolate\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = 'data/raw/Bloomberg_Data.xlsx'\n",
    "\n",
    "# Define which sheets use column C instead of column B\n",
    "use_column_c = [\n",
    "    \"US_Building_Permits\",\n",
    "    \"US _BP_Single_Housing\",\n",
    "    \"US_Housing_Start\",\n",
    "    \"US_New_Home_Sales\",\n",
    "    \"US_Existing_Home _Sales\",\n",
    "    \"US Existing_Single_Home_Sales\",\n",
    "    \"CAD_Housing_Start\"\n",
    "]\n",
    "\n",
    "# Define sheets to ignore\n",
    "sheets_to_ignore = [\n",
    "    \"US_Population_Growth_Rate_Bloom\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Core Data Processing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(date):\n",
    "    \"\"\"Normalize date to end of month, with special handling for quarterly data\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    # Get the last day of the month\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Create end of month date\n",
    "    if month == 12:\n",
    "        end_of_month = datetime(year, 12, 31)\n",
    "    else:\n",
    "        end_of_month = datetime(year, month + 1, 1) - pd.Timedelta(days=1)\n",
    "    \n",
    "    return end_of_month.date()\n",
    "\n",
    "def normalize_quarterly_date(date):\n",
    "    \"\"\"Normalize quarterly date to end of quarter\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Map to end of quarter\n",
    "    if month in [1, 2, 3]:  # Q1\n",
    "        return datetime(year, 3, 31).date()\n",
    "    elif month in [4, 5, 6]:  # Q2\n",
    "        return datetime(year, 6, 30).date()\n",
    "    elif month in [7, 8, 9]:  # Q3\n",
    "        return datetime(year, 9, 30).date()\n",
    "    else:  # Q4\n",
    "        return datetime(year, 12, 31).date()\n",
    "\n",
    "def extract_sheet_data(file_path, sheet_name, use_col_c):\n",
    "    \"\"\"Extract data from a specific sheet with improved data cleaning\"\"\"\n",
    "    # Determine which column to use\n",
    "    data_column = 'C' if sheet_name in use_col_c else 'B'\n",
    "    \n",
    "    # Read the sheet starting from row 7 (index 6 in pandas)\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "    \n",
    "    # Extract dates from column A and values from the appropriate column\n",
    "    # Row 7 in Excel is index 6 in pandas (0-indexed)\n",
    "    dates = df.iloc[6:, 0]  # Column A, starting from row 7\n",
    "    \n",
    "    if data_column == 'C':\n",
    "        values = df.iloc[6:, 2]  # Column C\n",
    "    else:\n",
    "        values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create a temporary dataframe\n",
    "    temp_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where value is NaN or empty\n",
    "    temp_df = temp_df.dropna(subset=['value'])\n",
    "    \n",
    "    # Remove rows where date is NaN\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Determine the appropriate date normalization based on sheet name\n",
    "    # Population growth data needs special quarterly normalization\n",
    "    if sheet_name == 'US_Population_Growth_Rate_FRED':\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_quarterly_date)\n",
    "    else:\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Rename value column to sheet name\n",
    "    temp_df = temp_df.rename(columns={'value': sheet_name})\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "def is_row_worth_keeping(row, important_columns, min_important_values=5):\n",
    "    \"\"\"\n",
    "    Determine if a row is worth keeping based on the number of important values.\n",
    "    A row is worth keeping if it has at least min_important_values non-null values\n",
    "    in important columns (excluding CPI-only rows).\n",
    "    \"\"\"\n",
    "    # Count non-null values in important columns\n",
    "    non_null_count = row[important_columns].notna().sum()\n",
    "    \n",
    "    # Special case: if the row only has CPI data, drop it\n",
    "    if non_null_count == 0 and row.get('US_CPI') is not None:\n",
    "        return False\n",
    "    \n",
    "    # Keep rows with sufficient important data\n",
    "    return non_null_count >= min_important_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Canada-US Softwood Lumber Exports Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_softwood_data(file_path):\n",
    "    \"\"\"Extract and process CAD_Softwood_Export_to_US data with STL decomposition for missing values\"\"\"\n",
    "    \n",
    "    # Read the softwood export sheet\n",
    "    df = pd.read_excel(file_path, sheet_name='CAD_Softwood_Export_to_US', header=None)\n",
    "    \n",
    "    # Extract dates and values (data starts at row 6, index 6)\n",
    "    dates = df.iloc[6:, 0]  # Column A\n",
    "    values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create dataframe\n",
    "    softwood_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where both date and value are NaN\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Convert dates to datetime and normalize to end of month\n",
    "    softwood_df['date'] = pd.to_datetime(softwood_df['date'])\n",
    "    softwood_df['date'] = softwood_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    softwood_df = softwood_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Extracted {len(softwood_df)} monthly data points\")\n",
    "    print(f\"Date range: {softwood_df['date'].min()} to {softwood_df['date'].max()}\")\n",
    "    print(f\"Missing values: {softwood_df['value'].isna().sum()}\")\n",
    "    \n",
    "    return softwood_df\n",
    "\n",
    "def impute_missing_values_stl(df):\n",
    "    \"\"\"Impute missing values using STL decomposition with seasonal interpolation\"\"\"\n",
    "    \n",
    "    # Convert values to numeric to ensure proper data type\n",
    "    df_copy = df.copy()\n",
    "    df_copy['value'] = pd.to_numeric(df_copy['value'], errors='coerce')\n",
    "    \n",
    "    # Create a complete date range for monthly data\n",
    "    start_date = df_copy['date'].min()\n",
    "    end_date = df_copy['date'].max()\n",
    "    complete_dates = pd.date_range(start=start_date, end=end_date, freq='ME')  # Use 'ME' instead of 'M'\n",
    "    complete_dates = [normalize_date(d) for d in complete_dates]\n",
    "    \n",
    "    # Create complete dataframe\n",
    "    complete_df = pd.DataFrame({'date': complete_dates})\n",
    "    complete_df = complete_df.merge(df_copy, on='date', how='left')\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = complete_df['value'].notna().sum()\n",
    "    total_count = len(complete_df)\n",
    "    \n",
    "    print(f\"Data completeness: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    if non_null_count < 24:  # Need at least 2 years of data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        complete_df = complete_df.set_index('date')\n",
    "        \n",
    "        # Store original missing mask before filling\n",
    "        original_missing_mask = complete_df['value'].isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = complete_df['value'].ffill().bfill()\n",
    "        \n",
    "        # Perform STL decomposition\n",
    "        try:\n",
    "            # Use the working parameters: seasonal=11, period=12\n",
    "            stl = STL(ts_filled, seasonal=11, period=12, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Use seasonal component for interpolation of missing values\n",
    "            seasonal_component = result.seasonal\n",
    "            trend_component = result.trend\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            # For missing values, use trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                # Fill missing values with trend + seasonal\n",
    "                complete_df.loc[original_missing_mask, 'value'] = (\n",
    "                    trend_component[original_missing_mask] + \n",
    "                    seasonal_component[original_missing_mask]\n",
    "                )\n",
    "            \n",
    "            print(\"STL decomposition completed successfully\")\n",
    "            print(f\"Imputed {original_missing_mask.sum()} missing values using STL\")\n",
    "            \n",
    "            # Show some statistics\n",
    "            print(f\"STL Statistics - Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"STL Statistics - Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    \n",
    "    # Reset index and return\n",
    "    complete_df = complete_df.reset_index()\n",
    "    return complete_df\n",
    "\n",
    "def aggregate_monthly_to_quarterly(df):\n",
    "    \"\"\"Aggregate monthly data to quarterly data\"\"\"\n",
    "    \n",
    "    # Convert date column to datetime for proper resampling\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "    \n",
    "    # Set date as index for resampling\n",
    "    df_indexed = df_copy.set_index('date')\n",
    "    \n",
    "    # Resample to quarterly (end of quarter) and sum the values\n",
    "    quarterly_df = df_indexed.resample('QE').sum().reset_index()  # Use 'QE' instead of 'Q'\n",
    "    \n",
    "    # Convert quarterly dates to end of quarter format\n",
    "    quarterly_df['date'] = quarterly_df['date'].apply(normalize_quarterly_date)\n",
    "    \n",
    "    # Rename the value column\n",
    "    quarterly_df = quarterly_df.rename(columns={'value': 'CAD_Softwood_Export_to_US'})\n",
    "    \n",
    "    print(f\"Aggregated to {len(quarterly_df)} quarterly data points\")\n",
    "    print(f\"Quarterly date range: {quarterly_df['date'].min()} to {quarterly_df['date'].max()}\")\n",
    "    \n",
    "    return quarterly_df\n",
    "\n",
    "def impute_master_df_softwood_stl(master_df):\n",
    "    \"\"\"\n",
    "    Impute missing values in CAD_Softwood_Export_to_US using STL decomposition.\n",
    "    Handles edge missing values by extrapolating trend + seasonal components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    master_df : pandas.DataFrame\n",
    "        Master dataframe with 'Date' and 'CAD_Softwood_Export_to_US' columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Updated dataframe with imputed softwood values\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STL Imputation for Master DataFrame Softwood Values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = master_df.copy()\n",
    "    \n",
    "    # Extract Date and CAD_Softwood_Export_to_US columns\n",
    "    softwood_data = df_copy[['Date', 'CAD_Softwood_Export_to_US']].copy()\n",
    "    \n",
    "    # Convert Date to datetime and sort\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date'])\n",
    "    softwood_data = softwood_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Check initial missing values\n",
    "    initial_missing = softwood_data['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    total_values = len(softwood_data)\n",
    "    \n",
    "    print(f\"Initial analysis:\")\n",
    "    print(f\"- Total data points: {total_values}\")\n",
    "    print(f\"- Missing values: {initial_missing} ({initial_missing/total_values*100:.1f}%)\")\n",
    "    print(f\"- Date range: {softwood_data['Date'].min().date()} to {softwood_data['Date'].max().date()}\")\n",
    "    \n",
    "    if initial_missing == 0:\n",
    "        print(\"No missing values found. Returning original dataframe.\")\n",
    "        return df_copy\n",
    "    \n",
    "    # Convert values to numeric\n",
    "    softwood_data['CAD_Softwood_Export_to_US'] = pd.to_numeric(\n",
    "        softwood_data['CAD_Softwood_Export_to_US'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = softwood_data['CAD_Softwood_Export_to_US'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nSTL Decomposition Setup:\")\n",
    "    print(f\"- Non-null values: {non_null_count}\")\n",
    "    print(f\"- Data completeness: {non_null_count/total_values*100:.1f}%\")\n",
    "    \n",
    "    if non_null_count < 8:  # Need at least 2 years of quarterly data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        softwood_ts = softwood_data.set_index('Date')['CAD_Softwood_Export_to_US']\n",
    "        \n",
    "        # Store original missing mask\n",
    "        original_missing_mask = softwood_ts.isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = softwood_ts.ffill().bfill()\n",
    "        \n",
    "        try:\n",
    "            # Perform STL decomposition with quarterly parameters\n",
    "            print(\"Performing STL decomposition...\")\n",
    "            print(\"- Parameters: seasonal=11, period=4 (quarterly), robust=True\")\n",
    "            \n",
    "            stl = STL(ts_filled, seasonal=11, period=4, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Extract components\n",
    "            trend_component = result.trend\n",
    "            seasonal_component = result.seasonal\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            print(\"STL decomposition completed successfully!\")\n",
    "            print(f\"- Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"- Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            print(f\"- Residual std: {residual_component.std():.0f}\")\n",
    "            \n",
    "            # Impute missing values using trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                imputed_values = trend_component[original_missing_mask] + seasonal_component[original_missing_mask]\n",
    "                softwood_ts.loc[original_missing_mask] = imputed_values\n",
    "                \n",
    "                print(f\"\\nImputation Results:\")\n",
    "                print(f\"- Imputed {original_missing_mask.sum()} missing values\")\n",
    "                print(f\"- Imputed value range: {imputed_values.min():.0f} to {imputed_values.max():.0f}\")\n",
    "                \n",
    "                # Show some examples of imputed values\n",
    "                imputed_indices = softwood_ts.index[original_missing_mask]\n",
    "                print(f\"- Sample imputed dates: {[d.date() for d in imputed_indices[:3]]}\")\n",
    "            \n",
    "            # Update the dataframe\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_ts.values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    \n",
    "    # Ensure Date column is in the same format as the original dataframe\n",
    "    # Convert back to the original Date format (object type with date objects)\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date']).dt.date\n",
    "    \n",
    "    # Create a mapping dictionary for imputed values to avoid merge duplicates\n",
    "    imputed_mapping = dict(zip(softwood_data['Date'], softwood_data['CAD_Softwood_Export_to_US']))\n",
    "    \n",
    "    # Apply imputed values directly to avoid merge duplicates\n",
    "    df_copy['CAD_Softwood_Export_to_US'] = df_copy['Date'].map(imputed_mapping).fillna(df_copy['CAD_Softwood_Export_to_US'])\n",
    "    \n",
    "    # Final verification\n",
    "    final_missing = df_copy['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"- Missing values after imputation: {final_missing}\")\n",
    "    print(f\"- Imputation success: {'✓' if final_missing == 0 else '✗'}\")\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Canada-US Softwood Lumber Exports Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CAD_Softwood_Export_to_US data\n",
    "print(\"Processing CAD_Softwood_Export_to_US data...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract monthly softwood data\n",
    "softwood_monthly = extract_softwood_data(file_path)\n",
    "\n",
    "# Impute missing values using STL decomposition\n",
    "print(\"\\nImputing missing values using STL decomposition...\")\n",
    "softwood_complete = impute_missing_values_stl(softwood_monthly)\n",
    "\n",
    "# Aggregate monthly data to quarterly\n",
    "print(\"\\nAggregating monthly data to quarterly...\")\n",
    "softwood_quarterly = aggregate_monthly_to_quarterly(softwood_complete)\n",
    "\n",
    "# Check for duplicate dates and remove them\n",
    "print(\"\\nChecking for duplicate dates...\")\n",
    "initial_count = len(softwood_quarterly)\n",
    "duplicate_mask = softwood_quarterly.duplicated(subset=['date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate date(s). Removing duplicates...\")\n",
    "    duplicate_dates = softwood_quarterly[duplicate_mask]['date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    softwood_quarterly = softwood_quarterly[~duplicate_mask].reset_index(drop=True)\n",
    "    final_count = len(softwood_quarterly)\n",
    "    print(f\"Removed {initial_count - final_count} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate dates found.\")\n",
    "\n",
    "print(f\"\\nFinal softwood quarterly data: {len(softwood_quarterly)} data points\")\n",
    "print(\"Sample of processed data:\")\n",
    "print(softwood_quarterly.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### STL Decomposition: Mathematical Foundation and Rationale\n",
    "\n",
    "**STL (Seasonal and Trend decomposition using Loess)** is a robust time series decomposition method that separates a time series into three components:\n",
    "\n",
    "#### Mathematical Model\n",
    "For a time series $Y_t$, STL decomposes it as:\n",
    "$$Y_t = T_t + S_t + R_t$$\n",
    "\n",
    "Where:\n",
    "- **$T_t$** = Trend component (long-term movement)\n",
    "- **$S_t$** = Seasonal component (recurring patterns within a year)\n",
    "- **$R_t$** = Residual component (irregular fluctuations)\n",
    "\n",
    "#### Why STL for Canada-US Softwood Lumber Exports Data?\n",
    "\n",
    "1. **Seasonal Nature of Construction**: Softwood lumber exports exhibit strong seasonal patterns due to:\n",
    "   - Construction activity peaks in spring/summer\n",
    "   - Winter slowdowns in building activity\n",
    "   - Weather-dependent construction cycles\n",
    "\n",
    "2. **Robust to Outliers**: STL uses Loess (Locally Weighted Scatterplot Smoothing) which is:\n",
    "   - Less sensitive to extreme values than traditional methods\n",
    "   - Handles irregular patterns better than moving averages\n",
    "   - Preserves local patterns while smoothing global trends\n",
    "\n",
    "3. **Flexible Seasonal Patterns**: Unlike fixed seasonal models, STL:\n",
    "   - Allows seasonal patterns to evolve over time\n",
    "   - Handles changing amplitude of seasonal effects\n",
    "   - Adapts to structural breaks in the data\n",
    "\n",
    "#### Our Implementation Parameters\n",
    "\n",
    "- **`seasonal=11`**: Uses 11-point seasonal window for monthly data\n",
    "- **`period=12`**: Assumes 12-month seasonal cycle (annual pattern)\n",
    "- **`robust=True`**: Uses robust statistics to handle outliers\n",
    "\n",
    "#### Missing Value Imputation Strategy\n",
    "\n",
    "For missing values at time $t$, we estimate:\n",
    "$$\\hat{Y}_t = \\hat{T}_t + \\hat{S}_t$$\n",
    "\n",
    "This approach:\n",
    "- Preserves the underlying seasonal structure\n",
    "- Maintains trend consistency\n",
    "- Provides more realistic estimates than simple interpolation\n",
    "- Accounts for the specific month's typical seasonal behavior\n",
    "\n",
    "#### Advantages Over Alternatives\n",
    "\n",
    "- **vs. Linear Interpolation**: Captures seasonal patterns, not just linear trends\n",
    "- **vs. Moving Averages**: More flexible and robust to outliers\n",
    "- **vs. ARIMA**: Simpler, more interpretable, and handles missing values naturally\n",
    "- **vs. Simple Seasonal Decomposition**: More robust and handles irregular patterns better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file to get all sheet names\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "sheet_names = excel_file.sheet_names\n",
    "\n",
    "print(f\"Found {len(sheet_names)} sheets in the Excel file\\n\")\n",
    "\n",
    "# Extract data from all sheets\n",
    "all_dataframes = []\n",
    "\n",
    "# Add the processed softwood quarterly data first\n",
    "print(f\"Adding processed softwood data...\", end=' ')\n",
    "all_dataframes.append(softwood_quarterly)\n",
    "print(f\"✓ ({len(softwood_quarterly)} data points)\")\n",
    "\n",
    "for sheet_name in sheet_names:\n",
    "    # Skip ignored sheets and the softwood sheet (already processed)\n",
    "    if sheet_name in sheets_to_ignore or sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "        if sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "            print(f\"Skipping: {sheet_name} (processed separately)\")\n",
    "        else:\n",
    "            print(f\"Skipping: {sheet_name} (ignored)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing: {sheet_name}...\", end=' ')\n",
    "    try:\n",
    "        df = extract_sheet_data(file_path, sheet_name, use_column_c)\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"✓ ({len(df)} data points)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on the date column\n",
    "print(\"\\nMerging all data into master dataframe...\")\n",
    "\n",
    "master_df = all_dataframes[0]\n",
    "for df in all_dataframes[1:]:\n",
    "    master_df = master_df.merge(df, on='date', how='outer')\n",
    "\n",
    "# Sort by date (most recent first)\n",
    "master_df = master_df.sort_values('date', ascending=False)\n",
    "\n",
    "# Rename date column to 'Date'\n",
    "master_df = master_df.rename(columns={'date': 'Date'})\n",
    "\n",
    "# Reset index\n",
    "master_df = master_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"Total rows: {len(master_df)}\")\n",
    "\n",
    "# Identify important columns (all except Date and US_CPI)\n",
    "important_cols = [col for col in master_df.columns if col not in ['Date', 'US_CPI']]\n",
    "\n",
    "# Apply improved filtering logic\n",
    "print(f\"\\nApplying improved filtering logic...\")\n",
    "\n",
    "# Create a mask for rows worth keeping\n",
    "keep_mask = master_df.apply(lambda row: is_row_worth_keeping(row, important_cols, min_important_values=8), axis=1)\n",
    "\n",
    "# Filter the dataframe\n",
    "master_df_filtered = master_df[keep_mask].copy()\n",
    "\n",
    "print(f\"\\nAfter improved filtering (minimum 8 important non-null values, excluding CPI-only rows):\")\n",
    "print(f\"Total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Rows removed: {len(master_df) - len(master_df_filtered)}\")\n",
    "\n",
    "# Show some statistics about the filtering\n",
    "print(f\"\\nFiltering statistics:\")\n",
    "print(f\"- Rows with only CPI data: {len(master_df[(master_df[important_cols].notna().sum(axis=1) == 0) & (master_df['US_CPI'].notna())])}\")\n",
    "print(f\"- Rows with 1-7 important values: {len(master_df[(master_df[important_cols].notna().sum(axis=1) >= 1) & (master_df[important_cols].notna().sum(axis=1) < 8)])}\")\n",
    "print(f\"- Rows with 8+ important values: {len(master_df[master_df[important_cols].notna().sum(axis=1) >= 8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData Quality Check - Detailed analysis of filtered data:\")\n",
    "check_cols = [col for col in master_df_filtered.columns if col != 'Date']\n",
    "important_cols_check = [col for col in check_cols if col != 'US_CPI']\n",
    "\n",
    "print(f\"\\nNon-null value distribution in filtered data:\")\n",
    "non_null_counts = master_df_filtered[check_cols].notna().sum(axis=1)\n",
    "important_non_null_counts = master_df_filtered[important_cols_check].notna().sum(axis=1)\n",
    "\n",
    "print(f\"- Total non-null values per row: min={non_null_counts.min()}, max={non_null_counts.max()}, mean={non_null_counts.mean():.1f}\")\n",
    "print(f\"- Important non-null values per row: min={important_non_null_counts.min()}, max={important_non_null_counts.max()}, mean={important_non_null_counts.mean():.1f}\")\n",
    "\n",
    "# Check for any remaining sparse rows\n",
    "sparse_rows = []\n",
    "for idx, row in master_df_filtered.iterrows():\n",
    "    non_null = row[check_cols].notna().sum()\n",
    "    important_non_null = row[important_cols_check].notna().sum()\n",
    "    if important_non_null < 8:\n",
    "        sparse_rows.append((row['Date'], important_non_null, non_null))\n",
    "\n",
    "if sparse_rows:\n",
    "    print(f\"\\n⚠️  Found {len(sparse_rows)} rows with fewer than 8 important non-null values:\")\n",
    "    for date, important_count, total_count in sparse_rows:\n",
    "        print(f\"  {date}: {important_count} important, {total_count} total non-null values\")\n",
    "else:\n",
    "    print(f\"\\n✓ All rows have at least 8 important non-null values!\")\n",
    "\n",
    "# Show population growth data alignment check\n",
    "print(f\"\\nPopulation Growth Data Check:\")\n",
    "pop_growth_data = master_df_filtered[['Date', 'US_Population_Growth_Rate_FRED']].dropna()\n",
    "print(f\"- Population growth data points: {len(pop_growth_data)}\")\n",
    "if len(pop_growth_data) > 0:\n",
    "    print(f\"- Date range: {pop_growth_data['Date'].min()} to {pop_growth_data['Date'].max()}\")\n",
    "    print(f\"- Sample values: {pop_growth_data.head(3)['US_Population_Growth_Rate_FRED'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## STL Imputation for Master DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply STL imputation to fill missing CAD_Softwood_Export_to_US values\n",
    "print(\"Applying STL imputation to master dataframe...\")\n",
    "\n",
    "# Check missing values before imputation\n",
    "before_missing = master_df_filtered['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "before_total = len(master_df_filtered)\n",
    "print(f\"\\nBefore STL imputation:\")\n",
    "print(f\"- Total rows: {before_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {before_missing} ({before_missing/before_total*100:.1f}%)\")\n",
    "\n",
    "# Apply STL imputation\n",
    "master_df_final = impute_master_df_softwood_stl(master_df_filtered)\n",
    "\n",
    "# Check for and remove any duplicate rows that may have been created\n",
    "print(\"\\nChecking for duplicate rows...\")\n",
    "initial_rows = len(master_df_final)\n",
    "duplicate_mask = master_df_final.duplicated(subset=['Date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate row(s). Removing duplicates...\")\n",
    "    duplicate_dates = master_df_final[duplicate_mask]['Date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    master_df_final = master_df_final[~duplicate_mask].reset_index(drop=True)\n",
    "    final_rows = len(master_df_final)\n",
    "    print(f\"Removed {initial_rows - final_rows} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Check missing values after imputation\n",
    "after_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "after_total = len(master_df_final)\n",
    "print(f\"\\nAfter STL imputation:\")\n",
    "print(f\"- Total rows: {after_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {after_missing} ({after_missing/after_total*100:.1f}%)\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "if after_missing == 0:\n",
    "    print(\"✓ SUCCESS: All missing CAD_Softwood_Export_to_US values have been filled!\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: {after_missing} missing values still remain\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nImputation Summary:\")\n",
    "print(f\"- Values imputed: {before_missing - after_missing}\")\n",
    "print(f\"- Imputation success rate: {((before_missing - after_missing) / before_missing * 100) if before_missing > 0 else 100:.1f}%\")\n",
    "\n",
    "# Display sample of the final data\n",
    "print(f\"\\nSample of final data with imputed values:\")\n",
    "sample_data = master_df_final[['Date', 'CAD_Softwood_Export_to_US']].head(10)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Master DataFrame Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original total rows: {len(master_df)}\")\n",
    "print(f\"Filtered total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Final total rows (after STL imputation): {len(master_df_final)}\")\n",
    "print(f\"Total columns: {len(master_df_final.columns)} (Date + {len(master_df_final.columns)-1} variables)\")\n",
    "print(f\"Date range: {master_df_final['Date'].min()} to {master_df_final['Date'].max()}\")\n",
    "print(f\"\\nColumns: {', '.join(master_df_final.columns.tolist())}\")\n",
    "\n",
    "# Show softwood data completeness\n",
    "softwood_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "print(f\"\\nCAD_Softwood_Export_to_US completeness: {((len(master_df_final) - softwood_missing) / len(master_df_final) * 100):.1f}% ({softwood_missing} missing values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_file = os.path.join(output_dir, 'bloomberg_master_dataframe.csv')\n",
    "master_df_final.to_csv(output_file, index=False)\n",
    "print(f\"Master dataframe saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_excel = os.path.join(output_dir, 'bloomberg_master_dataframe.xlsx')\n",
    "master_df_final.to_excel(output_excel, index=False)\n",
    "print(f\"Master dataframe saved to: {output_excel}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasci)",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

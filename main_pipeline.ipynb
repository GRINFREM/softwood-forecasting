{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your Excel file\n",
    "file_path = 'data/raw/Bloomberg_Data.xlsx'\n",
    "\n",
    "# Define which sheets use column C instead of column B\n",
    "use_column_c = [\n",
    "    \"US_Building_Permits\",\n",
    "    \"US _BP_Single_Housing\",\n",
    "    \"US_Housing_Start\",\n",
    "    \"US_New_Home_Sales\",\n",
    "    \"US_Existing_Home _Sales\",\n",
    "    \"US Existing_Single_Home_Sales\",\n",
    "    \"CAD_Housing_Start\"\n",
    "]\n",
    "\n",
    "# Define sheets to ignore\n",
    "sheets_to_ignore = [\n",
    "    \"US_Population_Growth_Rate_Bloom\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Core Data Processing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(date):\n",
    "    \"\"\"Normalize date to end of month, with special handling for quarterly data\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    # Get the last day of the month\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Create end of month date\n",
    "    if month == 12:\n",
    "        end_of_month = datetime(year, 12, 31)\n",
    "    else:\n",
    "        end_of_month = datetime(year, month + 1, 1) - pd.Timedelta(days=1)\n",
    "    \n",
    "    return end_of_month.date()\n",
    "\n",
    "def normalize_quarterly_date(date):\n",
    "    \"\"\"Normalize quarterly date to end of quarter\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Map to end of quarter\n",
    "    if month in [1, 2, 3]:  # Q1\n",
    "        return datetime(year, 3, 31).date()\n",
    "    elif month in [4, 5, 6]:  # Q2\n",
    "        return datetime(year, 6, 30).date()\n",
    "    elif month in [7, 8, 9]:  # Q3\n",
    "        return datetime(year, 9, 30).date()\n",
    "    else:  # Q4\n",
    "        return datetime(year, 12, 31).date()\n",
    "\n",
    "def extract_sheet_data(file_path, sheet_name, use_col_c):\n",
    "    \"\"\"Extract data from a specific sheet with improved data cleaning\"\"\"\n",
    "    # Determine which column to use\n",
    "    data_column = 'C' if sheet_name in use_col_c else 'B'\n",
    "    \n",
    "    # Read the sheet starting from row 7 (index 6 in pandas)\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "    \n",
    "    # Extract dates from column A and values from the appropriate column\n",
    "    # Row 7 in Excel is index 6 in pandas (0-indexed)\n",
    "    dates = df.iloc[6:, 0]  # Column A, starting from row 7\n",
    "    \n",
    "    if data_column == 'C':\n",
    "        values = df.iloc[6:, 2]  # Column C\n",
    "    else:\n",
    "        values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create a temporary dataframe\n",
    "    temp_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where value is NaN or empty\n",
    "    temp_df = temp_df.dropna(subset=['value'])\n",
    "    \n",
    "    # Remove rows where date is NaN\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Determine the appropriate date normalization based on sheet name\n",
    "    # Population growth data needs special quarterly normalization\n",
    "    if sheet_name == 'US_Population_Growth_Rate_FRED':\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_quarterly_date)\n",
    "    else:\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Rename value column to sheet name\n",
    "    temp_df = temp_df.rename(columns={'value': sheet_name})\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "def is_row_worth_keeping(row, important_columns, min_important_values=5):\n",
    "    \"\"\"\n",
    "    Determine if a row is worth keeping based on the number of important values.\n",
    "    A row is worth keeping if it has at least min_important_values non-null values\n",
    "    in important columns (excluding CPI-only rows).\n",
    "    \"\"\"\n",
    "    # Count non-null values in important columns\n",
    "    non_null_count = row[important_columns].notna().sum()\n",
    "    \n",
    "    # Special case: if the row only has CPI data, drop it\n",
    "    if non_null_count == 0 and row.get('US_CPI') is not None:\n",
    "        return False\n",
    "    \n",
    "    # Keep rows with sufficient important data\n",
    "    return non_null_count >= min_important_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Canada-US Softwood Lumber Exports Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_softwood_data(file_path):\n",
    "    \"\"\"Extract and process CAD_Softwood_Export_to_US data with STL decomposition for missing values\"\"\"\n",
    "    \n",
    "    # Read the softwood export sheet\n",
    "    df = pd.read_excel(file_path, sheet_name='CAD_Softwood_Export_to_US', header=None)\n",
    "    \n",
    "    # Extract dates and values (data starts at row 6, index 6)\n",
    "    dates = df.iloc[6:, 0]  # Column A\n",
    "    values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create dataframe\n",
    "    softwood_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where both date and value are NaN\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Convert dates to datetime and normalize to end of month\n",
    "    softwood_df['date'] = pd.to_datetime(softwood_df['date'])\n",
    "    softwood_df['date'] = softwood_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    softwood_df = softwood_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Extracted {len(softwood_df)} monthly data points\")\n",
    "    print(f\"Date range: {softwood_df['date'].min()} to {softwood_df['date'].max()}\")\n",
    "    print(f\"Missing values: {softwood_df['value'].isna().sum()}\")\n",
    "    \n",
    "    return softwood_df\n",
    "\n",
    "def impute_missing_values_stl(df):\n",
    "    \"\"\"Impute missing values using STL decomposition with seasonal interpolation\"\"\"\n",
    "    \n",
    "    # Convert values to numeric to ensure proper data type\n",
    "    df_copy = df.copy()\n",
    "    df_copy['value'] = pd.to_numeric(df_copy['value'], errors='coerce')\n",
    "    \n",
    "    # Create a complete date range for monthly data\n",
    "    start_date = df_copy['date'].min()\n",
    "    end_date = df_copy['date'].max()\n",
    "    complete_dates = pd.date_range(start=start_date, end=end_date, freq='ME')  # Use 'ME' instead of 'M'\n",
    "    complete_dates = [normalize_date(d) for d in complete_dates]\n",
    "    \n",
    "    # Create complete dataframe\n",
    "    complete_df = pd.DataFrame({'date': complete_dates})\n",
    "    complete_df = complete_df.merge(df_copy, on='date', how='left')\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = complete_df['value'].notna().sum()\n",
    "    total_count = len(complete_df)\n",
    "    \n",
    "    print(f\"Data completeness: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    if non_null_count < 24:  # Need at least 2 years of data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        complete_df = complete_df.set_index('date')\n",
    "        \n",
    "        # Store original missing mask before filling\n",
    "        original_missing_mask = complete_df['value'].isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = complete_df['value'].ffill().bfill()\n",
    "        \n",
    "        # Perform STL decomposition\n",
    "        try:\n",
    "            # Use the working parameters: seasonal=11, period=12\n",
    "            stl = STL(ts_filled, seasonal=11, period=12, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Use seasonal component for interpolation of missing values\n",
    "            seasonal_component = result.seasonal\n",
    "            trend_component = result.trend\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            # For missing values, use trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                # Fill missing values with trend + seasonal\n",
    "                complete_df.loc[original_missing_mask, 'value'] = (\n",
    "                    trend_component[original_missing_mask] + \n",
    "                    seasonal_component[original_missing_mask]\n",
    "                )\n",
    "            \n",
    "            print(\"STL decomposition completed successfully\")\n",
    "            print(f\"Imputed {original_missing_mask.sum()} missing values using STL\")\n",
    "            \n",
    "            # Show some statistics\n",
    "            print(f\"STL Statistics - Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"STL Statistics - Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    \n",
    "    # Reset index and return\n",
    "    complete_df = complete_df.reset_index()\n",
    "    return complete_df\n",
    "\n",
    "def aggregate_monthly_to_quarterly(df):\n",
    "    \"\"\"Aggregate monthly data to quarterly data\"\"\"\n",
    "    \n",
    "    # Convert date column to datetime for proper resampling\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "    \n",
    "    # Set date as index for resampling\n",
    "    df_indexed = df_copy.set_index('date')\n",
    "    \n",
    "    # Resample to quarterly (end of quarter) and sum the values\n",
    "    quarterly_df = df_indexed.resample('QE').sum().reset_index()  # Use 'QE' instead of 'Q'\n",
    "    \n",
    "    # Convert quarterly dates to end of quarter format\n",
    "    quarterly_df['date'] = quarterly_df['date'].apply(normalize_quarterly_date)\n",
    "    \n",
    "    # Rename the value column\n",
    "    quarterly_df = quarterly_df.rename(columns={'value': 'CAD_Softwood_Export_to_US'})\n",
    "    \n",
    "    print(f\"Aggregated to {len(quarterly_df)} quarterly data points\")\n",
    "    print(f\"Quarterly date range: {quarterly_df['date'].min()} to {quarterly_df['date'].max()}\")\n",
    "    \n",
    "    return quarterly_df\n",
    "\n",
    "def impute_master_df_softwood_stl(master_df):\n",
    "    \"\"\"\n",
    "    Impute missing values in CAD_Softwood_Export_to_US using STL decomposition.\n",
    "    Handles edge missing values by extrapolating trend + seasonal components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    master_df : pandas.DataFrame\n",
    "        Master dataframe with 'Date' and 'CAD_Softwood_Export_to_US' columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Updated dataframe with imputed softwood values\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STL Imputation for Master DataFrame Softwood Values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = master_df.copy()\n",
    "    \n",
    "    # Extract Date and CAD_Softwood_Export_to_US columns\n",
    "    softwood_data = df_copy[['Date', 'CAD_Softwood_Export_to_US']].copy()\n",
    "    \n",
    "    # Convert Date to datetime and sort\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date'])\n",
    "    softwood_data = softwood_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Check initial missing values\n",
    "    initial_missing = softwood_data['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    total_values = len(softwood_data)\n",
    "    \n",
    "    print(f\"Initial analysis:\")\n",
    "    print(f\"- Total data points: {total_values}\")\n",
    "    print(f\"- Missing values: {initial_missing} ({initial_missing/total_values*100:.1f}%)\")\n",
    "    print(f\"- Date range: {softwood_data['Date'].min().date()} to {softwood_data['Date'].max().date()}\")\n",
    "    \n",
    "    if initial_missing == 0:\n",
    "        print(\"No missing values found. Returning original dataframe.\")\n",
    "        return df_copy\n",
    "    \n",
    "    # Convert values to numeric\n",
    "    softwood_data['CAD_Softwood_Export_to_US'] = pd.to_numeric(\n",
    "        softwood_data['CAD_Softwood_Export_to_US'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = softwood_data['CAD_Softwood_Export_to_US'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nSTL Decomposition Setup:\")\n",
    "    print(f\"- Non-null values: {non_null_count}\")\n",
    "    print(f\"- Data completeness: {non_null_count/total_values*100:.1f}%\")\n",
    "    \n",
    "    if non_null_count < 8:  # Need at least 2 years of quarterly data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        softwood_ts = softwood_data.set_index('Date')['CAD_Softwood_Export_to_US']\n",
    "        \n",
    "        # Store original missing mask\n",
    "        original_missing_mask = softwood_ts.isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = softwood_ts.ffill().bfill()\n",
    "        \n",
    "        try:\n",
    "            # Perform STL decomposition with quarterly parameters\n",
    "            print(\"Performing STL decomposition...\")\n",
    "            print(\"- Parameters: seasonal=11, period=4 (quarterly), robust=True\")\n",
    "            \n",
    "            stl = STL(ts_filled, seasonal=11, period=4, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Extract components\n",
    "            trend_component = result.trend\n",
    "            seasonal_component = result.seasonal\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            print(\"STL decomposition completed successfully!\")\n",
    "            print(f\"- Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"- Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            print(f\"- Residual std: {residual_component.std():.0f}\")\n",
    "            \n",
    "            # Impute missing values using trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                imputed_values = trend_component[original_missing_mask] + seasonal_component[original_missing_mask]\n",
    "                softwood_ts.loc[original_missing_mask] = imputed_values\n",
    "                \n",
    "                print(f\"\\nImputation Results:\")\n",
    "                print(f\"- Imputed {original_missing_mask.sum()} missing values\")\n",
    "                print(f\"- Imputed value range: {imputed_values.min():.0f} to {imputed_values.max():.0f}\")\n",
    "                \n",
    "                # Show some examples of imputed values\n",
    "                imputed_indices = softwood_ts.index[original_missing_mask]\n",
    "                print(f\"- Sample imputed dates: {[d.date() for d in imputed_indices[:3]]}\")\n",
    "            \n",
    "            # Update the dataframe\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_ts.values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    \n",
    "    # Ensure Date column is in the same format as the original dataframe\n",
    "    # Convert back to the original Date format (object type with date objects)\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date']).dt.date\n",
    "    \n",
    "    # Create a mapping dictionary for imputed values to avoid merge duplicates\n",
    "    imputed_mapping = dict(zip(softwood_data['Date'], softwood_data['CAD_Softwood_Export_to_US']))\n",
    "    \n",
    "    # Apply imputed values directly to avoid merge duplicates\n",
    "    df_copy['CAD_Softwood_Export_to_US'] = df_copy['Date'].map(imputed_mapping).fillna(df_copy['CAD_Softwood_Export_to_US'])\n",
    "    \n",
    "    # Final verification\n",
    "    final_missing = df_copy['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"- Missing values after imputation: {final_missing}\")\n",
    "    print(f\"- Imputation success: {'✓' if final_missing == 0 else '✗'}\")\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Canada-US Softwood Lumber Exports Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CAD_Softwood_Export_to_US data\n",
    "print(\"Processing CAD_Softwood_Export_to_US data...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract monthly softwood data\n",
    "softwood_monthly = extract_softwood_data(file_path)\n",
    "\n",
    "# Impute missing values using STL decomposition\n",
    "print(\"\\nImputing missing values using STL decomposition...\")\n",
    "softwood_complete = impute_missing_values_stl(softwood_monthly)\n",
    "\n",
    "# Aggregate monthly data to quarterly\n",
    "print(\"\\nAggregating monthly data to quarterly...\")\n",
    "softwood_quarterly = aggregate_monthly_to_quarterly(softwood_complete)\n",
    "\n",
    "# Check for duplicate dates and remove them\n",
    "print(\"\\nChecking for duplicate dates...\")\n",
    "initial_count = len(softwood_quarterly)\n",
    "duplicate_mask = softwood_quarterly.duplicated(subset=['date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate date(s). Removing duplicates...\")\n",
    "    duplicate_dates = softwood_quarterly[duplicate_mask]['date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    softwood_quarterly = softwood_quarterly[~duplicate_mask].reset_index(drop=True)\n",
    "    final_count = len(softwood_quarterly)\n",
    "    print(f\"Removed {initial_count - final_count} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate dates found.\")\n",
    "\n",
    "print(f\"\\nFinal softwood quarterly data: {len(softwood_quarterly)} data points\")\n",
    "print(\"Sample of processed data:\")\n",
    "print(softwood_quarterly.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### STL Decomposition: Mathematical Foundation and Rationale\n",
    "\n",
    "**STL (Seasonal and Trend decomposition using Loess)** is a robust time series decomposition method that separates a time series into three components:\n",
    "\n",
    "#### Mathematical Model\n",
    "For a time series $Y_t$, STL decomposes it as:\n",
    "$$Y_t = T_t + S_t + R_t$$\n",
    "\n",
    "Where:\n",
    "- **$T_t$** = Trend component (long-term movement)\n",
    "- **$S_t$** = Seasonal component (recurring patterns within a year)\n",
    "- **$R_t$** = Residual component (irregular fluctuations)\n",
    "\n",
    "#### Why STL for Canada-US Softwood Lumber Exports Data?\n",
    "\n",
    "1. **Seasonal Nature of Construction**: Softwood lumber exports exhibit strong seasonal patterns due to:\n",
    "   - Construction activity peaks in spring/summer\n",
    "   - Winter slowdowns in building activity\n",
    "   - Weather-dependent construction cycles\n",
    "\n",
    "2. **Robust to Outliers**: STL uses Loess (Locally Weighted Scatterplot Smoothing) which is:\n",
    "   - Less sensitive to extreme values than traditional methods\n",
    "   - Handles irregular patterns better than moving averages\n",
    "   - Preserves local patterns while smoothing global trends\n",
    "\n",
    "3. **Flexible Seasonal Patterns**: Unlike fixed seasonal models, STL:\n",
    "   - Allows seasonal patterns to evolve over time\n",
    "   - Handles changing amplitude of seasonal effects\n",
    "   - Adapts to structural breaks in the data\n",
    "\n",
    "#### Our Implementation Parameters\n",
    "\n",
    "- **`seasonal=11`**: Uses 11-point seasonal window for monthly data\n",
    "- **`period=12`**: Assumes 12-month seasonal cycle (annual pattern)\n",
    "- **`robust=True`**: Uses robust statistics to handle outliers\n",
    "\n",
    "#### Missing Value Imputation Strategy\n",
    "\n",
    "For missing values at time $t$, we estimate:\n",
    "$$\\hat{Y}_t = \\hat{T}_t + \\hat{S}_t$$\n",
    "\n",
    "This approach:\n",
    "- Preserves the underlying seasonal structure\n",
    "- Maintains trend consistency\n",
    "- Provides more realistic estimates than simple interpolation\n",
    "- Accounts for the specific month's typical seasonal behavior\n",
    "\n",
    "#### Advantages Over Alternatives\n",
    "\n",
    "- **vs. Linear Interpolation**: Captures seasonal patterns, not just linear trends\n",
    "- **vs. Moving Averages**: More flexible and robust to outliers\n",
    "- **vs. ARIMA**: Simpler, more interpretable, and handles missing values naturally\n",
    "- **vs. Simple Seasonal Decomposition**: More robust and handles irregular patterns better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file to get all sheet names\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "sheet_names = excel_file.sheet_names\n",
    "\n",
    "print(f\"Found {len(sheet_names)} sheets in the Excel file\\n\")\n",
    "\n",
    "# Extract data from all sheets\n",
    "all_dataframes = []\n",
    "\n",
    "# Add the processed softwood quarterly data first\n",
    "print(f\"Adding processed softwood data...\", end=' ')\n",
    "all_dataframes.append(softwood_quarterly)\n",
    "print(f\"✓ ({len(softwood_quarterly)} data points)\")\n",
    "\n",
    "for sheet_name in sheet_names:\n",
    "    # Skip ignored sheets and the softwood sheet (already processed)\n",
    "    if sheet_name in sheets_to_ignore or sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "        if sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "            print(f\"Skipping: {sheet_name} (processed separately)\")\n",
    "        else:\n",
    "            print(f\"Skipping: {sheet_name} (ignored)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing: {sheet_name}...\", end=' ')\n",
    "    try:\n",
    "        df = extract_sheet_data(file_path, sheet_name, use_column_c)\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"✓ ({len(df)} data points)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on the date column\n",
    "print(\"\\nMerging all data into master dataframe...\")\n",
    "\n",
    "master_df = all_dataframes[0]\n",
    "for df in all_dataframes[1:]:\n",
    "    master_df = master_df.merge(df, on='date', how='outer')\n",
    "\n",
    "# Sort by date (most recent first)\n",
    "master_df = master_df.sort_values('date', ascending=False)\n",
    "\n",
    "# Rename date column to 'Date'\n",
    "master_df = master_df.rename(columns={'date': 'Date'})\n",
    "\n",
    "# Reset index\n",
    "master_df = master_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"Total rows: {len(master_df)}\")\n",
    "\n",
    "# Identify important columns (all except Date and US_CPI)\n",
    "important_cols = [col for col in master_df.columns if col not in ['Date', 'US_CPI']]\n",
    "\n",
    "# Apply improved filtering logic\n",
    "print(f\"\\nApplying improved filtering logic...\")\n",
    "\n",
    "# Create a mask for rows worth keeping\n",
    "keep_mask = master_df.apply(lambda row: is_row_worth_keeping(row, important_cols, min_important_values=8), axis=1)\n",
    "\n",
    "# Filter the dataframe\n",
    "master_df_filtered = master_df[keep_mask].copy()\n",
    "\n",
    "print(f\"\\nAfter improved filtering (minimum 8 important non-null values, excluding CPI-only rows):\")\n",
    "print(f\"Total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Rows removed: {len(master_df) - len(master_df_filtered)}\")\n",
    "\n",
    "# Show some statistics about the filtering\n",
    "print(f\"\\nFiltering statistics:\")\n",
    "print(f\"- Rows with only CPI data: {len(master_df[(master_df[important_cols].notna().sum(axis=1) == 0) & (master_df['US_CPI'].notna())])}\")\n",
    "print(f\"- Rows with 1-7 important values: {len(master_df[(master_df[important_cols].notna().sum(axis=1) >= 1) & (master_df[important_cols].notna().sum(axis=1) < 8)])}\")\n",
    "print(f\"- Rows with 8+ important values: {len(master_df[master_df[important_cols].notna().sum(axis=1) >= 8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData Quality Check - Detailed analysis of filtered data:\")\n",
    "check_cols = [col for col in master_df_filtered.columns if col != 'Date']\n",
    "important_cols_check = [col for col in check_cols if col != 'US_CPI']\n",
    "\n",
    "print(f\"\\nNon-null value distribution in filtered data:\")\n",
    "non_null_counts = master_df_filtered[check_cols].notna().sum(axis=1)\n",
    "important_non_null_counts = master_df_filtered[important_cols_check].notna().sum(axis=1)\n",
    "\n",
    "print(f\"- Total non-null values per row: min={non_null_counts.min()}, max={non_null_counts.max()}, mean={non_null_counts.mean():.1f}\")\n",
    "print(f\"- Important non-null values per row: min={important_non_null_counts.min()}, max={important_non_null_counts.max()}, mean={important_non_null_counts.mean():.1f}\")\n",
    "\n",
    "# Check for any remaining sparse rows\n",
    "sparse_rows = []\n",
    "for idx, row in master_df_filtered.iterrows():\n",
    "    non_null = row[check_cols].notna().sum()\n",
    "    important_non_null = row[important_cols_check].notna().sum()\n",
    "    if important_non_null < 8:\n",
    "        sparse_rows.append((row['Date'], important_non_null, non_null))\n",
    "\n",
    "if sparse_rows:\n",
    "    print(f\"\\n⚠️  Found {len(sparse_rows)} rows with fewer than 8 important non-null values:\")\n",
    "    for date, important_count, total_count in sparse_rows:\n",
    "        print(f\"  {date}: {important_count} important, {total_count} total non-null values\")\n",
    "else:\n",
    "    print(f\"\\n✓ All rows have at least 8 important non-null values!\")\n",
    "\n",
    "# Show population growth data alignment check\n",
    "print(f\"\\nPopulation Growth Data Check:\")\n",
    "pop_growth_data = master_df_filtered[['Date', 'US_Population_Growth_Rate_FRED']].dropna()\n",
    "print(f\"- Population growth data points: {len(pop_growth_data)}\")\n",
    "if len(pop_growth_data) > 0:\n",
    "    print(f\"- Date range: {pop_growth_data['Date'].min()} to {pop_growth_data['Date'].max()}\")\n",
    "    print(f\"- Sample values: {pop_growth_data.head(3)['US_Population_Growth_Rate_FRED'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## STL Imputation for Master DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply STL imputation to fill missing CAD_Softwood_Export_to_US values\n",
    "print(\"Applying STL imputation to master dataframe...\")\n",
    "\n",
    "# Check missing values before imputation\n",
    "before_missing = master_df_filtered['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "before_total = len(master_df_filtered)\n",
    "print(f\"\\nBefore STL imputation:\")\n",
    "print(f\"- Total rows: {before_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {before_missing} ({before_missing/before_total*100:.1f}%)\")\n",
    "\n",
    "# Apply STL imputation\n",
    "master_df_final = impute_master_df_softwood_stl(master_df_filtered)\n",
    "\n",
    "# Check for and remove any duplicate rows that may have been created\n",
    "print(\"\\nChecking for duplicate rows...\")\n",
    "initial_rows = len(master_df_final)\n",
    "duplicate_mask = master_df_final.duplicated(subset=['Date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate row(s). Removing duplicates...\")\n",
    "    duplicate_dates = master_df_final[duplicate_mask]['Date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    master_df_final = master_df_final[~duplicate_mask].reset_index(drop=True)\n",
    "    final_rows = len(master_df_final)\n",
    "    print(f\"Removed {initial_rows - final_rows} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Check missing values after imputation\n",
    "after_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "after_total = len(master_df_final)\n",
    "print(f\"\\nAfter STL imputation:\")\n",
    "print(f\"- Total rows: {after_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {after_missing} ({after_missing/after_total*100:.1f}%)\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "if after_missing == 0:\n",
    "    print(\"✓ SUCCESS: All missing CAD_Softwood_Export_to_US values have been filled!\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: {after_missing} missing values still remain\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nImputation Summary:\")\n",
    "print(f\"- Values imputed: {before_missing - after_missing}\")\n",
    "print(f\"- Imputation success rate: {((before_missing - after_missing) / before_missing * 100) if before_missing > 0 else 100:.1f}%\")\n",
    "\n",
    "# Display sample of the final data\n",
    "print(f\"\\nSample of final data with imputed values:\")\n",
    "sample_data = master_df_final[['Date', 'CAD_Softwood_Export_to_US']].head(10)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Master DataFrame Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original total rows: {len(master_df)}\")\n",
    "print(f\"Filtered total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Final total rows (after STL imputation): {len(master_df_final)}\")\n",
    "print(f\"Total columns: {len(master_df_final.columns)} (Date + {len(master_df_final.columns)-1} variables)\")\n",
    "print(f\"Date range: {master_df_final['Date'].min()} to {master_df_final['Date'].max()}\")\n",
    "print(f\"\\nColumns: {', '.join(master_df_final.columns.tolist())}\")\n",
    "\n",
    "# Show softwood data completeness\n",
    "softwood_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "print(f\"\\nCAD_Softwood_Export_to_US completeness: {((len(master_df_final) - softwood_missing) / len(master_df_final) * 100):.1f}% ({softwood_missing} missing values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_file = os.path.join(output_dir, 'bloomberg_master_dataframe.csv')\n",
    "master_df_final.to_csv(output_file, index=False)\n",
    "print(f\"Master dataframe saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_excel = os.path.join(output_dir, 'bloomberg_master_dataframe.xlsx')\n",
    "master_df_final.to_excel(output_excel, index=False)\n",
    "print(f\"Master dataframe saved to: {output_excel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Setting plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 1. Time Series Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 1.1 Target Variable: Canadian Softwood Exports to US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(master_df_final['Date'], master_df_final['CAD_Softwood_Export_to_US'], \n",
    "        linewidth=2, color='#2c7bb6')\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Quarterly Exports (Thousand Cubic Meters)', fontsize=11)\n",
    "ax.set_title('Canada-US Softwood Lumber Exports Over Time', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Export Statistics:\")\n",
    "print(f\"  Mean: {master_df_final['CAD_Softwood_Export_to_US'].mean():,.0f} FBM\")\n",
    "print(f\"  Std Dev: {master_df_final['CAD_Softwood_Export_to_US'].std():,.0f} FBM\")\n",
    "print(f\"  Min: {master_df_final['CAD_Softwood_Export_to_US'].min():,.0f} FBM\")\n",
    "print(f\"  Max: {master_df_final['CAD_Softwood_Export_to_US'].max():,.0f} FBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 2. Key Economic Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 2.1 US Housing Market Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_vars = ['US_Building_Permits', 'US_Housing_Start', 'US_New_Home_Sales', \n",
    "                'US_Existing_Home _Sales']\n",
    "available_housing = [v for v in housing_vars if v in master_df_final.columns]\n",
    "\n",
    "if available_housing:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, var in enumerate(available_housing):\n",
    "        axes[i].plot(master_df_final['Date'], master_df_final[var], \n",
    "                    linewidth=2, color='#d7191c')\n",
    "        axes[i].set_xlabel('Date', fontsize=10)\n",
    "        axes[i].set_ylabel('Units', fontsize=10)\n",
    "        axes[i].set_title(var.replace('_', ' '), fontsize=11, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 2.2 Price Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_vars = ['US_CPI', 'CAD_CPI', 'CAD_Export_Price_Lumber']\n",
    "available_prices = [v for v in price_vars if v in master_df_final.columns]\n",
    "\n",
    "if len(available_prices) >= 2:\n",
    "    fig, axes = plt.subplots(1, len(available_prices), figsize=(14, 5))\n",
    "    if len(available_prices) == 1:\n",
    "        axes = [axes]  # Make it iterable for single subplot\n",
    "    \n",
    "    for i, var in enumerate(available_prices):\n",
    "        # Convert to numeric, handling any non-numeric values\n",
    "        price_data = pd.to_numeric(master_df_final[var], errors='coerce')\n",
    "        \n",
    "        axes[i].plot(master_df_final['Date'], price_data, \n",
    "                    linewidth=2, color='#fdae61')\n",
    "        axes[i].set_xlabel('Date', fontsize=10)\n",
    "        axes[i].set_ylabel('Index/Price', fontsize=10)\n",
    "        axes[i].set_title(var.replace('_', ' '), fontsize=11, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for price indicators\n",
    "    print(\"\\nPrice Indicators Summary:\")\n",
    "    for var in available_prices:\n",
    "        price_data = pd.to_numeric(master_df_final[var], errors='coerce')\n",
    "        non_null_count = price_data.notna().sum()\n",
    "        print(f\"  {var}: {non_null_count} data points\")\n",
    "        if non_null_count > 0:\n",
    "            print(f\"    Range: {price_data.min():.2f} to {price_data.max():.2f}\")\n",
    "else:\n",
    "    print(\"Insufficient price indicator data available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 3.1 Correlation with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to numeric (excluding Date)\n",
    "print(\"Converting columns to numeric for correlation analysis...\")\n",
    "target = 'CAD_Softwood_Export_to_US'\n",
    "\n",
    "# Get all columns except Date\n",
    "all_cols = [col for col in master_df_final.columns if col != 'Date']\n",
    "\n",
    "# Convert each column to numeric, handling errors\n",
    "for col in all_cols:\n",
    "    if col != target:  # Target is already numeric\n",
    "        master_df_final[col] = pd.to_numeric(master_df_final[col], errors='coerce')\n",
    "\n",
    "# Now select numeric columns (exclude Date)\n",
    "numeric_cols = master_df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Found {len(numeric_cols)} numeric columns: {numeric_cols}\")\n",
    "\n",
    "# Show data completeness after conversion\n",
    "print(f\"\\nData completeness after numeric conversion:\")\n",
    "for col in numeric_cols:\n",
    "    non_null_count = master_df_final[col].notna().sum()\n",
    "    total_count = len(master_df_final)\n",
    "    print(f\"  {col}: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "print(\"Calculating correlations with target variable...\")\n",
    "correlations = []\n",
    "for col in numeric_cols:\n",
    "    if col != target:\n",
    "        # Remove rows with missing values in either column\n",
    "        valid_data = master_df_final[[target, col]].dropna()\n",
    "        if len(valid_data) > 10:  # Need sufficient data points\n",
    "            try:\n",
    "                corr, pval = pearsonr(valid_data[target], valid_data[col])\n",
    "                correlations.append({\n",
    "                    'Variable': col,\n",
    "                    'Correlation': corr,\n",
    "                    'P-value': pval,\n",
    "                    'Significant': pval < 0.05,\n",
    "                    'Data_Points': len(valid_data)\n",
    "                })\n",
    "                print(f\"  {col}: r={corr:.3f}, p={pval:.3f}, n={len(valid_data)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {col}: Error calculating correlation - {e}\")\n",
    "\n",
    "print(f\"\\nFound {len(correlations)} valid correlations\")\n",
    "\n",
    "if len(correlations) > 0:\n",
    "    corr_df = pd.DataFrame(correlations).sort_values('Correlation', \n",
    "                                                      key=abs, \n",
    "                                                      ascending=False)\n",
    "    print(f\"Correlation analysis completed successfully!\")\n",
    "else:\n",
    "    print(\"No valid correlations found. Creating empty DataFrame.\")\n",
    "    corr_df = pd.DataFrame(columns=['Variable', 'Correlation', 'P-value', 'Significant', 'Data_Points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 3.2 Top Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top correlations\n",
    "if len(corr_df) > 0:\n",
    "    top_n = min(10, len(corr_df))  # Use available correlations or 10, whichever is smaller\n",
    "    top_corr = corr_df.head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, len(top_corr) * 0.4)))\n",
    "    colors = ['#2c7bb6' if x > 0 else '#d7191c' for x in top_corr['Correlation']]\n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['Correlation'], color=colors)\n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels([v.replace('_', ' ') for v in top_corr['Variable']], fontsize=9)\n",
    "    ax.set_xlabel('Correlation Coefficient', fontsize=11)\n",
    "    ax.set_title(f'Top {top_n} Correlations with Canadian Softwood Exports', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add correlation values as text on bars\n",
    "    for i, (idx, row) in enumerate(top_corr.iterrows()):\n",
    "        ax.text(row['Correlation'] + (0.01 if row['Correlation'] > 0 else -0.01), \n",
    "                i, f'{row[\"Correlation\"]:.3f}', \n",
    "                va='center', ha='left' if row['Correlation'] > 0 else 'right', \n",
    "                fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop {top_n} Correlations (with p-values and significance):\")\n",
    "    display_cols = ['Variable', 'Correlation', 'P-value', 'Significant', 'Data_Points']\n",
    "    print(corr_df[display_cols].head(top_n).to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    significant_corr = corr_df[corr_df['Significant'] == True]\n",
    "    print(f\"\\nCorrelation Summary:\")\n",
    "    print(f\"  Total correlations calculated: {len(corr_df)}\")\n",
    "    print(f\"  Significant correlations (p < 0.05): {len(significant_corr)}\")\n",
    "    if len(significant_corr) > 0:\n",
    "        print(f\"  Strongest positive correlation: {significant_corr[significant_corr['Correlation'] > 0]['Correlation'].max():.3f}\")\n",
    "        print(f\"  Strongest negative correlation: {significant_corr[significant_corr['Correlation'] < 0]['Correlation'].min():.3f}\")\n",
    "else:\n",
    "    print(\"No correlations available to plot.\")\n",
    "    print(\"This might be due to insufficient data or all variables being constant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 3.3 Correlation Matrix Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key variables for correlation analysis\n",
    "key_vars = ['CAD_Softwood_Export_to_US', 'US_Housing_Start', 'US_Building_Permits', \n",
    "            'US_CPI', 'CAD_Export_Price_Lumber', 'USCAD_Exchange_Rate']\n",
    "\n",
    "# Filter to variables that exist in the dataset\n",
    "available_vars = [var for var in key_vars if var in master_df_final.columns]\n",
    "n_vars = len(available_vars)\n",
    "\n",
    "# Create correlation matrix plot\n",
    "fig, axes = plt.subplots(n_vars, n_vars, figsize=(12, 10))\n",
    "fig.suptitle('Variable Relationships Matrix', fontsize=16, y=0.95)\n",
    "\n",
    "for i in range(n_vars):\n",
    "    for j in range(n_vars):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: show variable name\n",
    "            ax.text(0.5, 0.5, available_vars[i].replace('_', ' '), \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_var = available_vars[j]\n",
    "            y_var = available_vars[i]\n",
    "            \n",
    "            # Get data\n",
    "            data = master_df_final[[x_var, y_var]].dropna()\n",
    "            \n",
    "            if len(data) > 5:\n",
    "                # Create scatter plot\n",
    "                ax.scatter(data[x_var], data[y_var], alpha=0.6, s=30)\n",
    "                \n",
    "                # Add correlation coefficient\n",
    "                if len(data) > 10:\n",
    "                    corr, p_val = pearsonr(data[x_var], data[y_var])\n",
    "                    ax.text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                           transform=ax.transAxes, fontsize=8, \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Set labels\n",
    "            if i == n_vars - 1:  # Bottom row\n",
    "                ax.set_xlabel(x_var.replace('_', ' '), fontsize=8)\n",
    "            if j == 0:  # Left column\n",
    "                ax.set_ylabel(y_var.replace('_', ' '), fontsize=8)\n",
    "            \n",
    "            ax.tick_params(labelsize=6)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 4. Data Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### 4.1 Data Coverage Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data completeness by variable\n",
    "missing_pct = (master_df_final.isnull().sum() / len(master_df_final) * 100).sort_values(ascending=True)\n",
    "missing_pct = missing_pct[missing_pct > 0]  # Only show variables with missing data\n",
    "\n",
    "if len(missing_pct) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(missing_pct)), missing_pct.values)\n",
    "    plt.yticks(range(len(missing_pct)), [v.replace('_', ' ') for v in missing_pct.index])\n",
    "    plt.xlabel('Missing Data (%)')\n",
    "    plt.title('Data Completeness by Variable')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Variables with missing data:\")\n",
    "    for var, pct in missing_pct.items():\n",
    "        print(f\"  {var}: {pct:.1f}% missing\")\n",
    "else:\n",
    "    print(\"No missing data found in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nData Coverage Summary:\")\n",
    "print(f\"  Total observations: {len(master_df_final)}\")\n",
    "print(f\"  Date range: {master_df_final['Date'].min()} to {master_df_final['Date'].max()}\")\n",
    "print(f\"  Number of variables: {len(master_df_final.columns) - 1}\")  # Exclude Date\n",
    "print(f\"  Overall completeness: {(1 - master_df_final.isnull().sum().sum() / (len(master_df_final) * len(master_df_final.columns))) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 5. Relationship Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 5.1 Scatter Plot: Key Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "\"Key relationships\" refers to the strongest correlations between economic indicators and Canadian softwood exports. These help identify which factors most influence export volumes and can guide forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 3 most correlated variables\n",
    "if len(corr_df) >= 3:\n",
    "    top_vars = corr_df.head(3)['Variable'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, var in enumerate(top_vars):\n",
    "        data = master_df_final[[target, var]].dropna()\n",
    "        \n",
    "        axes[i].scatter(data[var], data[target], alpha=0.6)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(data[var], data[target], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[i].plot(data[var], p(data[var]), \"r--\", alpha=0.8)\n",
    "        \n",
    "        axes[i].set_xlabel(var.replace('_', ' '))\n",
    "        axes[i].set_ylabel('Softwood Exports')\n",
    "        corr_val = corr_df[corr_df['Variable'] == var]['Correlation'].values[0]\n",
    "        axes[i].set_title(f'{var.replace(\"_\", \" \")}\\n(r = {corr_val:.3f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softwood-forecasting-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

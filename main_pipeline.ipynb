{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your Excel file\n",
    "file_path = 'data/raw/Bloomberg_Data.xlsx'\n",
    "\n",
    "# Define which sheets use column C instead of column B\n",
    "use_column_c = [\n",
    "    \"US_Building_Permits\",\n",
    "    \"US _BP_Single_Housing\",\n",
    "    \"US_Housing_Start\",\n",
    "    \"US_New_Home_Sales\",\n",
    "    \"US_Existing_Home _Sales\",\n",
    "    \"US Existing_Single_Home_Sales\",\n",
    "    \"CAD_Housing_Start\"\n",
    "]\n",
    "\n",
    "# Define sheets to ignore\n",
    "sheets_to_ignore = [\n",
    "    \"US_Population_Growth_Rate_Bloom\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Core Data Processing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(date):\n",
    "    \"\"\"Normalize date to end of month, with special handling for quarterly data\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    # Get the last day of the month\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Create end of month date\n",
    "    if month == 12:\n",
    "        end_of_month = datetime(year, 12, 31)\n",
    "    else:\n",
    "        end_of_month = datetime(year, month + 1, 1) - pd.Timedelta(days=1)\n",
    "    \n",
    "    return end_of_month.date()\n",
    "\n",
    "def normalize_quarterly_date(date):\n",
    "    \"\"\"Normalize quarterly date to end of quarter\"\"\"\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    \n",
    "    # Convert to datetime if not already\n",
    "    if not isinstance(date, datetime):\n",
    "        date = pd.to_datetime(date)\n",
    "    \n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Map to end of quarter\n",
    "    if month in [1, 2, 3]:  # Q1\n",
    "        return datetime(year, 3, 31).date()\n",
    "    elif month in [4, 5, 6]:  # Q2\n",
    "        return datetime(year, 6, 30).date()\n",
    "    elif month in [7, 8, 9]:  # Q3\n",
    "        return datetime(year, 9, 30).date()\n",
    "    else:  # Q4\n",
    "        return datetime(year, 12, 31).date()\n",
    "\n",
    "def extract_sheet_data(file_path, sheet_name, use_col_c):\n",
    "    \"\"\"Extract data from a specific sheet with improved data cleaning\"\"\"\n",
    "    # Determine which column to use\n",
    "    data_column = 'C' if sheet_name in use_col_c else 'B'\n",
    "    \n",
    "    # Read the sheet starting from row 7 (index 6 in pandas)\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "    \n",
    "    # Extract dates from column A and values from the appropriate column\n",
    "    # Row 7 in Excel is index 6 in pandas (0-indexed)\n",
    "    dates = df.iloc[6:, 0]  # Column A, starting from row 7\n",
    "    \n",
    "    if data_column == 'C':\n",
    "        values = df.iloc[6:, 2]  # Column C\n",
    "    else:\n",
    "        values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create a temporary dataframe\n",
    "    temp_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where value is NaN or empty\n",
    "    temp_df = temp_df.dropna(subset=['value'])\n",
    "    \n",
    "    # Remove rows where date is NaN\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Determine the appropriate date normalization based on sheet name\n",
    "    # Population growth data needs special quarterly normalization\n",
    "    if sheet_name == 'US_Population_Growth_Rate_FRED':\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_quarterly_date)\n",
    "    else:\n",
    "        temp_df['date'] = temp_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    temp_df = temp_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Rename value column to sheet name\n",
    "    temp_df = temp_df.rename(columns={'value': sheet_name})\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "def is_row_worth_keeping(row, important_columns, min_important_values=5):\n",
    "    \"\"\"\n",
    "    Determine if a row is worth keeping based on the number of important values.\n",
    "    A row is worth keeping if it has at least min_important_values non-null values\n",
    "    in important columns (excluding CPI-only rows).\n",
    "    \"\"\"\n",
    "    # Count non-null values in important columns\n",
    "    non_null_count = row[important_columns].notna().sum()\n",
    "    \n",
    "    # Special case: if the row only has CPI data, drop it\n",
    "    if non_null_count == 0 and row.get('US_CPI') is not None:\n",
    "        return False\n",
    "    \n",
    "    # Keep rows with sufficient important data\n",
    "    return non_null_count >= min_important_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Canada-US Softwood Lumber Exports Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_softwood_data(file_path):\n",
    "    \"\"\"Extract and process CAD_Softwood_Export_to_US data with STL decomposition for missing values\"\"\"\n",
    "    \n",
    "    # Read the softwood export sheet\n",
    "    df = pd.read_excel(file_path, sheet_name='CAD_Softwood_Export_to_US', header=None)\n",
    "    \n",
    "    # Extract dates and values (data starts at row 6, index 6)\n",
    "    dates = df.iloc[6:, 0]  # Column A\n",
    "    values = df.iloc[6:, 1]  # Column B\n",
    "    \n",
    "    # Create dataframe\n",
    "    softwood_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Remove rows where both date and value are NaN\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Convert dates to datetime and normalize to end of month\n",
    "    softwood_df['date'] = pd.to_datetime(softwood_df['date'])\n",
    "    softwood_df['date'] = softwood_df['date'].apply(normalize_date)\n",
    "    \n",
    "    # Remove any rows where date normalization failed\n",
    "    softwood_df = softwood_df.dropna(subset=['date'])\n",
    "    \n",
    "    # Sort by date\n",
    "    softwood_df = softwood_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Extracted {len(softwood_df)} monthly data points\")\n",
    "    print(f\"Date range: {softwood_df['date'].min()} to {softwood_df['date'].max()}\")\n",
    "    print(f\"Missing values: {softwood_df['value'].isna().sum()}\")\n",
    "    \n",
    "    return softwood_df\n",
    "\n",
    "def impute_missing_values_stl(df):\n",
    "    \"\"\"Impute missing values using STL decomposition with seasonal interpolation\"\"\"\n",
    "    \n",
    "    # Convert values to numeric to ensure proper data type\n",
    "    df_copy = df.copy()\n",
    "    df_copy['value'] = pd.to_numeric(df_copy['value'], errors='coerce')\n",
    "    \n",
    "    # Create a complete date range for monthly data\n",
    "    start_date = df_copy['date'].min()\n",
    "    end_date = df_copy['date'].max()\n",
    "    complete_dates = pd.date_range(start=start_date, end=end_date, freq='ME')  # Use 'ME' instead of 'M'\n",
    "    complete_dates = [normalize_date(d) for d in complete_dates]\n",
    "    \n",
    "    # Create complete dataframe\n",
    "    complete_df = pd.DataFrame({'date': complete_dates})\n",
    "    complete_df = complete_df.merge(df_copy, on='date', how='left')\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = complete_df['value'].notna().sum()\n",
    "    total_count = len(complete_df)\n",
    "    \n",
    "    print(f\"Data completeness: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    if non_null_count < 24:  # Need at least 2 years of data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        complete_df = complete_df.set_index('date')\n",
    "        \n",
    "        # Store original missing mask before filling\n",
    "        original_missing_mask = complete_df['value'].isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = complete_df['value'].ffill().bfill()\n",
    "        \n",
    "        # Perform STL decomposition\n",
    "        try:\n",
    "            # Use the working parameters: seasonal=11, period=12\n",
    "            stl = STL(ts_filled, seasonal=11, period=12, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Use seasonal component for interpolation of missing values\n",
    "            seasonal_component = result.seasonal\n",
    "            trend_component = result.trend\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            # For missing values, use trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                # Fill missing values with trend + seasonal\n",
    "                complete_df.loc[original_missing_mask, 'value'] = (\n",
    "                    trend_component[original_missing_mask] + \n",
    "                    seasonal_component[original_missing_mask]\n",
    "                )\n",
    "            \n",
    "            print(\"STL decomposition completed successfully\")\n",
    "            print(f\"Imputed {original_missing_mask.sum()} missing values using STL\")\n",
    "            \n",
    "            # Show some statistics\n",
    "            print(f\"STL Statistics - Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"STL Statistics - Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            complete_df['value'] = complete_df['value'].interpolate(method='linear')\n",
    "    \n",
    "    # Reset index and return\n",
    "    complete_df = complete_df.reset_index()\n",
    "    return complete_df\n",
    "\n",
    "def aggregate_monthly_to_quarterly(df):\n",
    "    \"\"\"Aggregate monthly data to quarterly data\"\"\"\n",
    "    \n",
    "    # Convert date column to datetime for proper resampling\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "    \n",
    "    # Set date as index for resampling\n",
    "    df_indexed = df_copy.set_index('date')\n",
    "    \n",
    "    # Resample to quarterly (end of quarter) and sum the values\n",
    "    quarterly_df = df_indexed.resample('QE').sum().reset_index()  # Use 'QE' instead of 'Q'\n",
    "    \n",
    "    # Convert quarterly dates to end of quarter format\n",
    "    quarterly_df['date'] = quarterly_df['date'].apply(normalize_quarterly_date)\n",
    "    \n",
    "    # Rename the value column\n",
    "    quarterly_df = quarterly_df.rename(columns={'value': 'CAD_Softwood_Export_to_US'})\n",
    "    \n",
    "    print(f\"Aggregated to {len(quarterly_df)} quarterly data points\")\n",
    "    print(f\"Quarterly date range: {quarterly_df['date'].min()} to {quarterly_df['date'].max()}\")\n",
    "    \n",
    "    return quarterly_df\n",
    "\n",
    "def impute_master_df_softwood_stl(master_df):\n",
    "    \"\"\"\n",
    "    Impute missing values in CAD_Softwood_Export_to_US using STL decomposition.\n",
    "    Handles edge missing values by extrapolating trend + seasonal components.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    master_df : pandas.DataFrame\n",
    "        Master dataframe with 'Date' and 'CAD_Softwood_Export_to_US' columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Updated dataframe with imputed softwood values\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STL Imputation for Master DataFrame Softwood Values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = master_df.copy()\n",
    "    \n",
    "    # Extract Date and CAD_Softwood_Export_to_US columns\n",
    "    softwood_data = df_copy[['Date', 'CAD_Softwood_Export_to_US']].copy()\n",
    "    \n",
    "    # Convert Date to datetime and sort\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date'])\n",
    "    softwood_data = softwood_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Check initial missing values\n",
    "    initial_missing = softwood_data['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    total_values = len(softwood_data)\n",
    "    \n",
    "    print(f\"Initial analysis:\")\n",
    "    print(f\"- Total data points: {total_values}\")\n",
    "    print(f\"- Missing values: {initial_missing} ({initial_missing/total_values*100:.1f}%)\")\n",
    "    print(f\"- Date range: {softwood_data['Date'].min().date()} to {softwood_data['Date'].max().date()}\")\n",
    "    \n",
    "    if initial_missing == 0:\n",
    "        print(\"No missing values found. Returning original dataframe.\")\n",
    "        return df_copy\n",
    "    \n",
    "    # Convert values to numeric\n",
    "    softwood_data['CAD_Softwood_Export_to_US'] = pd.to_numeric(\n",
    "        softwood_data['CAD_Softwood_Export_to_US'], errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = softwood_data['CAD_Softwood_Export_to_US'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nSTL Decomposition Setup:\")\n",
    "    print(f\"- Non-null values: {non_null_count}\")\n",
    "    print(f\"- Data completeness: {non_null_count/total_values*100:.1f}%\")\n",
    "    \n",
    "    if non_null_count < 8:  # Need at least 2 years of quarterly data for STL\n",
    "        print(\"Warning: Insufficient data for STL decomposition. Using linear interpolation instead.\")\n",
    "        softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        softwood_ts = softwood_data.set_index('Date')['CAD_Softwood_Export_to_US']\n",
    "        \n",
    "        # Store original missing mask\n",
    "        original_missing_mask = softwood_ts.isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = softwood_ts.ffill().bfill()\n",
    "        \n",
    "        try:\n",
    "            # Perform STL decomposition with quarterly parameters\n",
    "            print(\"Performing STL decomposition...\")\n",
    "            print(\"- Parameters: seasonal=11, period=4 (quarterly), robust=True\")\n",
    "            \n",
    "            stl = STL(ts_filled, seasonal=11, period=4, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Extract components\n",
    "            trend_component = result.trend\n",
    "            seasonal_component = result.seasonal\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            print(\"STL decomposition completed successfully!\")\n",
    "            print(f\"- Trend range: {trend_component.min():.0f} to {trend_component.max():.0f}\")\n",
    "            print(f\"- Seasonal range: {seasonal_component.min():.0f} to {seasonal_component.max():.0f}\")\n",
    "            print(f\"- Residual std: {residual_component.std():.0f}\")\n",
    "            \n",
    "            # Impute missing values using trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                imputed_values = trend_component[original_missing_mask] + seasonal_component[original_missing_mask]\n",
    "                softwood_ts.loc[original_missing_mask] = imputed_values\n",
    "                \n",
    "                print(f\"\\nImputation Results:\")\n",
    "                print(f\"- Imputed {original_missing_mask.sum()} missing values\")\n",
    "                print(f\"- Imputed value range: {imputed_values.min():.0f} to {imputed_values.max():.0f}\")\n",
    "                \n",
    "                # Show some examples of imputed values\n",
    "                imputed_indices = softwood_ts.index[original_missing_mask]\n",
    "                print(f\"- Sample imputed dates: {[d.date() for d in imputed_indices[:3]]}\")\n",
    "            \n",
    "            # Update the dataframe\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_ts.values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Falling back to linear interpolation\")\n",
    "            softwood_data['CAD_Softwood_Export_to_US'] = softwood_data['CAD_Softwood_Export_to_US'].interpolate(method='linear')\n",
    "    \n",
    "    # Ensure Date column is in the same format as the original dataframe\n",
    "    # Convert back to the original Date format (object type with date objects)\n",
    "    softwood_data['Date'] = pd.to_datetime(softwood_data['Date']).dt.date\n",
    "    \n",
    "    # Create a mapping dictionary for imputed values to avoid merge duplicates\n",
    "    imputed_mapping = dict(zip(softwood_data['Date'], softwood_data['CAD_Softwood_Export_to_US']))\n",
    "    \n",
    "    # Apply imputed values directly to avoid merge duplicates\n",
    "    df_copy['CAD_Softwood_Export_to_US'] = df_copy['Date'].map(imputed_mapping).fillna(df_copy['CAD_Softwood_Export_to_US'])\n",
    "    \n",
    "    # Final verification\n",
    "    final_missing = df_copy['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"- Missing values after imputation: {final_missing}\")\n",
    "    print(f\"- Imputation success: {'✓' if final_missing == 0 else '✗'}\")\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Canada-US Softwood Lumber Exports Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CAD_Softwood_Export_to_US data\n",
    "print(\"Processing CAD_Softwood_Export_to_US data...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Extract monthly softwood data\n",
    "softwood_monthly = extract_softwood_data(file_path)\n",
    "\n",
    "# Impute missing values using STL decomposition\n",
    "print(\"\\nImputing missing values using STL decomposition...\")\n",
    "softwood_complete = impute_missing_values_stl(softwood_monthly)\n",
    "\n",
    "# Aggregate monthly data to quarterly\n",
    "print(\"\\nAggregating monthly data to quarterly...\")\n",
    "softwood_quarterly = aggregate_monthly_to_quarterly(softwood_complete)\n",
    "\n",
    "# Check for duplicate dates and remove them\n",
    "print(\"\\nChecking for duplicate dates...\")\n",
    "initial_count = len(softwood_quarterly)\n",
    "duplicate_mask = softwood_quarterly.duplicated(subset=['date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate date(s). Removing duplicates...\")\n",
    "    duplicate_dates = softwood_quarterly[duplicate_mask]['date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    softwood_quarterly = softwood_quarterly[~duplicate_mask].reset_index(drop=True)\n",
    "    final_count = len(softwood_quarterly)\n",
    "    print(f\"Removed {initial_count - final_count} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate dates found.\")\n",
    "\n",
    "print(f\"\\nFinal softwood quarterly data: {len(softwood_quarterly)} data points\")\n",
    "print(\"Sample of processed data:\")\n",
    "print(softwood_quarterly.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### STL Decomposition: Mathematical Foundation and Rationale\n",
    "\n",
    "**STL (Seasonal and Trend decomposition using Loess)** is a robust time series decomposition method that separates a time series into three components:\n",
    "\n",
    "#### Mathematical Model\n",
    "For a time series $Y_t$, STL decomposes it as:\n",
    "$$Y_t = T_t + S_t + R_t$$\n",
    "\n",
    "Where:\n",
    "- **$T_t$** = Trend component (long-term movement)\n",
    "- **$S_t$** = Seasonal component (recurring patterns within a year)\n",
    "- **$R_t$** = Residual component (irregular fluctuations)\n",
    "\n",
    "#### Why STL for Canada-US Softwood Lumber Exports Data?\n",
    "\n",
    "1. **Seasonal Nature of Construction**: Softwood lumber exports exhibit strong seasonal patterns due to:\n",
    "   - Construction activity peaks in spring/summer\n",
    "   - Winter slowdowns in building activity\n",
    "   - Weather-dependent construction cycles\n",
    "\n",
    "2. **Robust to Outliers**: STL uses Loess (Locally Weighted Scatterplot Smoothing) which is:\n",
    "   - Less sensitive to extreme values than traditional methods\n",
    "   - Handles irregular patterns better than moving averages\n",
    "   - Preserves local patterns while smoothing global trends\n",
    "\n",
    "3. **Flexible Seasonal Patterns**: Unlike fixed seasonal models, STL:\n",
    "   - Allows seasonal patterns to evolve over time\n",
    "   - Handles changing amplitude of seasonal effects\n",
    "   - Adapts to structural breaks in the data\n",
    "\n",
    "#### Our Implementation Parameters\n",
    "\n",
    "- **`seasonal=11`**: Uses 11-point seasonal window for monthly data\n",
    "- **`period=12`**: Assumes 12-month seasonal cycle (annual pattern)\n",
    "- **`robust=True`**: Uses robust statistics to handle outliers\n",
    "\n",
    "#### Missing Value Imputation Strategy\n",
    "\n",
    "For missing values at time $t$, we estimate:\n",
    "$$\\hat{Y}_t = \\hat{T}_t + \\hat{S}_t$$\n",
    "\n",
    "This approach:\n",
    "- Preserves the underlying seasonal structure\n",
    "- Maintains trend consistency\n",
    "- Provides more realistic estimates than simple interpolation\n",
    "- Accounts for the specific month's typical seasonal behavior\n",
    "\n",
    "#### Advantages Over Alternatives\n",
    "\n",
    "- **vs. Linear Interpolation**: Captures seasonal patterns, not just linear trends\n",
    "- **vs. Moving Averages**: More flexible and robust to outliers\n",
    "- **vs. ARIMA**: Simpler, more interpretable, and handles missing values naturally\n",
    "- **vs. Simple Seasonal Decomposition**: More robust and handles irregular patterns better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file to get all sheet names\n",
    "excel_file = pd.ExcelFile(file_path)\n",
    "sheet_names = excel_file.sheet_names\n",
    "\n",
    "print(f\"Found {len(sheet_names)} sheets in the Excel file\\n\")\n",
    "\n",
    "# Extract data from all sheets\n",
    "all_dataframes = []\n",
    "\n",
    "# Add the processed softwood quarterly data first\n",
    "print(f\"Adding processed softwood data...\", end=' ')\n",
    "all_dataframes.append(softwood_quarterly)\n",
    "print(f\"✓ ({len(softwood_quarterly)} data points)\")\n",
    "\n",
    "for sheet_name in sheet_names:\n",
    "    # Skip ignored sheets and the softwood sheet (already processed)\n",
    "    if sheet_name in sheets_to_ignore or sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "        if sheet_name == 'CAD_Softwood_Export_to_US':\n",
    "            print(f\"Skipping: {sheet_name} (processed separately)\")\n",
    "        else:\n",
    "            print(f\"Skipping: {sheet_name} (ignored)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing: {sheet_name}...\", end=' ')\n",
    "    try:\n",
    "        df = extract_sheet_data(file_path, sheet_name, use_column_c)\n",
    "        all_dataframes.append(df)\n",
    "        print(f\"✓ ({len(df)} data points)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on the date column\n",
    "print(\"\\nMerging all data into master dataframe...\")\n",
    "\n",
    "master_df = all_dataframes[0]\n",
    "for df in all_dataframes[1:]:\n",
    "    master_df = master_df.merge(df, on='date', how='outer')\n",
    "\n",
    "# Sort by date (most recent first)\n",
    "master_df = master_df.sort_values('date', ascending=False)\n",
    "\n",
    "# Rename date column to 'Date'\n",
    "master_df = master_df.rename(columns={'date': 'Date'})\n",
    "\n",
    "# Reset index\n",
    "master_df = master_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"Total rows: {len(master_df)}\")\n",
    "\n",
    "# Identify important columns (all except Date and US_CPI)\n",
    "important_cols = [col for col in master_df.columns if col not in ['Date', 'US_CPI']]\n",
    "\n",
    "# Apply improved filtering logic\n",
    "print(f\"\\nApplying improved filtering logic...\")\n",
    "\n",
    "# Create a mask for rows worth keeping\n",
    "keep_mask = master_df.apply(lambda row: is_row_worth_keeping(row, important_cols, min_important_values=8), axis=1)\n",
    "\n",
    "# Filter the dataframe\n",
    "master_df_filtered = master_df[keep_mask].copy()\n",
    "\n",
    "print(f\"\\nAfter improved filtering (minimum 8 important non-null values, excluding CPI-only rows):\")\n",
    "print(f\"Total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Rows removed: {len(master_df) - len(master_df_filtered)}\")\n",
    "\n",
    "# Show some statistics about the filtering\n",
    "print(f\"\\nFiltering statistics:\")\n",
    "print(f\"- Rows with only CPI data: {len(master_df[(master_df[important_cols].notna().sum(axis=1) == 0) & (master_df['US_CPI'].notna())])}\")\n",
    "print(f\"- Rows with 1-7 important values: {len(master_df[(master_df[important_cols].notna().sum(axis=1) >= 1) & (master_df[important_cols].notna().sum(axis=1) < 8)])}\")\n",
    "print(f\"- Rows with 8+ important values: {len(master_df[master_df[important_cols].notna().sum(axis=1) >= 8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData Quality Check - Detailed analysis of filtered data:\")\n",
    "check_cols = [col for col in master_df_filtered.columns if col != 'Date']\n",
    "important_cols_check = [col for col in check_cols if col != 'US_CPI']\n",
    "\n",
    "print(f\"\\nNon-null value distribution in filtered data:\")\n",
    "non_null_counts = master_df_filtered[check_cols].notna().sum(axis=1)\n",
    "important_non_null_counts = master_df_filtered[important_cols_check].notna().sum(axis=1)\n",
    "\n",
    "print(f\"- Total non-null values per row: min={non_null_counts.min()}, max={non_null_counts.max()}, mean={non_null_counts.mean():.1f}\")\n",
    "print(f\"- Important non-null values per row: min={important_non_null_counts.min()}, max={important_non_null_counts.max()}, mean={important_non_null_counts.mean():.1f}\")\n",
    "\n",
    "# Check for any remaining sparse rows\n",
    "sparse_rows = []\n",
    "for idx, row in master_df_filtered.iterrows():\n",
    "    non_null = row[check_cols].notna().sum()\n",
    "    important_non_null = row[important_cols_check].notna().sum()\n",
    "    if important_non_null < 8:\n",
    "        sparse_rows.append((row['Date'], important_non_null, non_null))\n",
    "\n",
    "if sparse_rows:\n",
    "    print(f\"\\n⚠️  Found {len(sparse_rows)} rows with fewer than 8 important non-null values:\")\n",
    "    for date, important_count, total_count in sparse_rows:\n",
    "        print(f\"  {date}: {important_count} important, {total_count} total non-null values\")\n",
    "else:\n",
    "    print(f\"\\n✓ All rows have at least 8 important non-null values!\")\n",
    "\n",
    "# Show population growth data alignment check\n",
    "print(f\"\\nPopulation Growth Data Check:\")\n",
    "pop_growth_data = master_df_filtered[['Date', 'US_Population_Growth_Rate_FRED']].dropna()\n",
    "print(f\"- Population growth data points: {len(pop_growth_data)}\")\n",
    "if len(pop_growth_data) > 0:\n",
    "    print(f\"- Date range: {pop_growth_data['Date'].min()} to {pop_growth_data['Date'].max()}\")\n",
    "    print(f\"- Sample values: {pop_growth_data.head(3)['US_Population_Growth_Rate_FRED'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## STL Imputation for Master DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply STL imputation to fill missing CAD_Softwood_Export_to_US values\n",
    "print(\"Applying STL imputation to master dataframe...\")\n",
    "\n",
    "# Check missing values before imputation\n",
    "before_missing = master_df_filtered['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "before_total = len(master_df_filtered)\n",
    "print(f\"\\nBefore STL imputation:\")\n",
    "print(f\"- Total rows: {before_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {before_missing} ({before_missing/before_total*100:.1f}%)\")\n",
    "\n",
    "# Apply STL imputation\n",
    "master_df_final = impute_master_df_softwood_stl(master_df_filtered)\n",
    "\n",
    "# Check for and remove any duplicate rows that may have been created\n",
    "print(\"\\nChecking for duplicate rows...\")\n",
    "initial_rows = len(master_df_final)\n",
    "duplicate_mask = master_df_final.duplicated(subset=['Date'], keep='first')\n",
    "duplicate_count = duplicate_mask.sum()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Found {duplicate_count} duplicate row(s). Removing duplicates...\")\n",
    "    duplicate_dates = master_df_final[duplicate_mask]['Date'].tolist()\n",
    "    print(f\"Duplicate dates: {duplicate_dates}\")\n",
    "    \n",
    "    # Keep only the first occurrence of each date\n",
    "    master_df_final = master_df_final[~duplicate_mask].reset_index(drop=True)\n",
    "    final_rows = len(master_df_final)\n",
    "    print(f\"Removed {initial_rows - final_rows} duplicate row(s).\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Check missing values after imputation\n",
    "after_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "after_total = len(master_df_final)\n",
    "print(f\"\\nAfter STL imputation:\")\n",
    "print(f\"- Total rows: {after_total}\")\n",
    "print(f\"- Missing CAD_Softwood_Export_to_US values: {after_missing} ({after_missing/after_total*100:.1f}%)\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "if after_missing == 0:\n",
    "    print(\"✓ SUCCESS: All missing CAD_Softwood_Export_to_US values have been filled!\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: {after_missing} missing values still remain\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nImputation Summary:\")\n",
    "print(f\"- Values imputed: {before_missing - after_missing}\")\n",
    "print(f\"- Imputation success rate: {((before_missing - after_missing) / before_missing * 100) if before_missing > 0 else 100:.1f}%\")\n",
    "\n",
    "# Display sample of the final data\n",
    "print(f\"\\nSample of final data with imputed values:\")\n",
    "sample_data = master_df_final[['Date', 'CAD_Softwood_Export_to_US']].head(10)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Modeling Pipeline\n",
    "\n",
    "## Step 1: Missing Value Analysis and Imputation\n",
    "\n",
    "Before we can build our predictive models, we need to handle missing values in our feature variables. While the target variable (CAD_Softwood_Export_to_US) has been successfully imputed using STL decomposition, several predictor variables still contain missing values.\n",
    "\n",
    "This analysis will:\n",
    "1. Identify which features have missing data\n",
    "2. Analyze the missingness patterns\n",
    "3. Determine the best imputation strategy for each variable\n",
    "4. Implement the chosen imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Analysis\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all numeric columns except Date and target\n",
    "feature_cols = [col for col in master_df_final.columns\n",
    "                if col not in ['Date', 'CAD_Softwood_Export_to_US']]\n",
    "\n",
    "print(f\"\\nAnalyzing {len(feature_cols)} predictor variables...\")\n",
    "print(f\"Dataset: {len(master_df_final)} quarterly observations\")\n",
    "print(f\"Date range: {master_df_final['Date'].min()} to {master_df_final['Date'].max()}\\n\")\n",
    "\n",
    "# Calculate missing data statistics\n",
    "missing_stats = []\n",
    "for col in feature_cols:\n",
    "    total_count = len(master_df_final)\n",
    "    missing_count = master_df_final[col].isna().sum()\n",
    "    missing_pct = (missing_count / total_count) * 100\n",
    "    present_count = total_count - missing_count\n",
    "\n",
    "    missing_stats.append({\n",
    "        'Variable': col,\n",
    "        'Total': total_count,\n",
    "        'Present': present_count,\n",
    "        'Missing': missing_count,\n",
    "        'Missing_Pct': missing_pct\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by missing percentage\n",
    "missing_df = pd.DataFrame(missing_stats).sort_values('Missing_Pct', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Missing Data Summary (sorted by missing percentage):\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in missing_df.iterrows():\n",
    "    if row['Missing'] > 0:\n",
    "        print(f\"{row['Variable']:40s} | {row['Present']:2d}/{row['Total']:2d} | \"\n",
    "              f\"Missing: {row['Missing']:2d} ({row['Missing_Pct']:5.1f}%)\")\n",
    "\n",
    "# Count features by missing data category\n",
    "no_missing = len(missing_df[missing_df['Missing'] == 0])\n",
    "low_missing = len(missing_df[(missing_df['Missing'] > 0) & (missing_df['Missing_Pct'] <= 10)])\n",
    "medium_missing = len(missing_df[(missing_df['Missing_Pct'] > 10) & (missing_df['Missing_Pct'] <= 30)])\n",
    "high_missing = len(missing_df[missing_df['Missing_Pct'] > 30])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING DATA CATEGORIES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Complete features (0% missing):           {no_missing} variables\")\n",
    "print(f\"Low missingness (1-10% missing):          {low_missing} variables\")\n",
    "print(f\"Medium missingness (10-30% missing):      {medium_missing} variables\")\n",
    "print(f\"High missingness (>30% missing):          {high_missing} variables\")\n",
    "\n",
    "# Visualize missing data pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of missing percentages\n",
    "vars_with_missing = missing_df[missing_df['Missing'] > 0]\n",
    "if len(vars_with_missing) > 0:\n",
    "    colors_missing = ['#d7191c' if x > 30 else '#fdae61' if x > 10 else '#fee08b'\n",
    "                      for x in vars_with_missing['Missing_Pct']]\n",
    "    ax1.barh(range(len(vars_with_missing)), vars_with_missing['Missing_Pct'],\n",
    "             color=colors_missing)\n",
    "    ax1.set_yticks(range(len(vars_with_missing)))\n",
    "    ax1.set_yticklabels([v.replace('_', ' ') for v in vars_with_missing['Variable']],\n",
    "                         fontsize=9)\n",
    "    ax1.set_xlabel('Missing Data (%)', fontsize=11)\n",
    "    ax1.set_title('Missing Data by Variable', fontsize=12, fontweight='bold')\n",
    "    ax1.axvline(x=10, color='orange', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "    ax1.axvline(x=30, color='red', linestyle='--', alpha=0.5, label='30% threshold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Heatmap of missing data pattern over time\n",
    "# Create binary missing data matrix\n",
    "missing_matrix = master_df_final[feature_cols].isna().astype(int)\n",
    "missing_by_var = missing_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "vars_to_show = missing_by_var[missing_by_var > 0].index.tolist()\n",
    "\n",
    "if len(vars_to_show) > 0:\n",
    "    missing_subset = missing_matrix[vars_to_show].T\n",
    "    sns.heatmap(missing_subset, cmap=['#2c7bb6', '#d7191c'],\n",
    "                cbar_kws={'label': 'Missing (1) / Present (0)'},\n",
    "                yticklabels=[v.replace('_', ' ') for v in vars_to_show],\n",
    "                xticklabels=False, ax=ax2)\n",
    "    ax2.set_title('Missing Data Pattern Over Time', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Time (Quarterly Observations)', fontsize=11)\n",
    "    ax2.set_ylabel('Variables', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Return summary for decision-making\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTATION STRATEGY RECOMMENDATIONS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, row in missing_df[missing_df['Missing'] > 0].iterrows():\n",
    "    var_name = row['Variable']\n",
    "    miss_pct = row['Missing_Pct']\n",
    "\n",
    "    if miss_pct > 50:\n",
    "        recommendation = \"DROPPING, too much missing data\"\n",
    "    elif miss_pct > 30:\n",
    "        recommendation = \"Forward fill or median imputation\"\n",
    "    elif miss_pct > 10:\n",
    "        recommendation = \"Forward fill (time series) or mean/median imputation\"\n",
    "    else:\n",
    "        recommendation = \"Forward fill or mean imputation (minimal impact)\"\n",
    "\n",
    "    print(f\"{var_name:40s} ({miss_pct:5.1f}% missing): {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1.1 Missing Value Imputation Strategy\n",
    "\n",
    "Based on the missing data analysis, we will implement the following strategy:\n",
    "\n",
    "**Variables to DROP:**\n",
    "- `CAD_Export_Price_Lumber` (57.0% missing) - Too much missing data to reliably impute\n",
    "\n",
    "**Variables to IMPUTE using STL Decomposition:**\n",
    "- `CAD_Building Permits` (26.6% missing, 21 missing quarters)\n",
    "- `CAD_BP_Single_Housing` (26.6% missing, 21 missing quarters)\n",
    "- `US_Households_Number` (3.8% missing, 3 missing quarters)\n",
    "\n",
    "**Rationale:** STL decomposition is appropriate for these time series variables because it:\n",
    "- Preserves seasonal patterns inherent in quarterly economic data\n",
    "- Captures trend components (growth/decline over time)\n",
    "- Handles outliers robustly\n",
    "- Maintains consistency with our target variable imputation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values using STL decomposition (reusable for any variable)\n",
    "def impute_variable_stl(df, variable_name, period=4, seasonal=11):\n",
    "    \"\"\"\n",
    "    Impute missing values in a variable using STL decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with 'Date' column and the variable to impute\n",
    "    variable_name : str\n",
    "        Name of the column to impute\n",
    "    period : int\n",
    "        Seasonal period (4 for quarterly data)\n",
    "    seasonal : int\n",
    "        Seasonal window parameter for STL\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Updated dataframe with imputed values\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STL IMPUTATION: {variable_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Extract Date and variable columns\n",
    "    var_data = df_copy[['Date', variable_name]].copy()\n",
    "    \n",
    "    # Convert Date to datetime and sort\n",
    "    var_data['Date'] = pd.to_datetime(var_data['Date'])\n",
    "    var_data = var_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Check initial missing values\n",
    "    initial_missing = var_data[variable_name].isna().sum()\n",
    "    total_values = len(var_data)\n",
    "    \n",
    "    print(f\"\\nInitial analysis:\")\n",
    "    print(f\"  Total data points: {total_values}\")\n",
    "    print(f\"  Missing values: {initial_missing} ({initial_missing/total_values*100:.1f}%)\")\n",
    "    print(f\"  Date range: {var_data['Date'].min().date()} to {var_data['Date'].max().date()}\")\n",
    "    \n",
    "    if initial_missing == 0:\n",
    "        print(\"  No missing values found. Skipping imputation.\")\n",
    "        return df_copy\n",
    "    \n",
    "    # Convert values to numeric\n",
    "    var_data[variable_name] = pd.to_numeric(var_data[variable_name], errors='coerce')\n",
    "    \n",
    "    # Check if we have enough data for STL decomposition\n",
    "    non_null_count = var_data[variable_name].notna().sum()\n",
    "    \n",
    "    print(f\"\\nSTL Decomposition Setup:\")\n",
    "    print(f\"  Non-null values: {non_null_count}\")\n",
    "    print(f\"  Data completeness: {non_null_count/total_values*100:.1f}%\")\n",
    "    \n",
    "    if non_null_count < 8:  # Need at least 2 years of quarterly data\n",
    "        print(\"  WARNING: Insufficient data for STL decomposition.\")\n",
    "        print(\"  Using linear interpolation instead...\")\n",
    "        var_data[variable_name] = var_data[variable_name].interpolate(method='linear')\n",
    "    else:\n",
    "        # Prepare data for STL decomposition\n",
    "        var_ts = var_data.set_index('Date')[variable_name]\n",
    "        \n",
    "        # Store original missing mask\n",
    "        original_missing_mask = var_ts.isna()\n",
    "        \n",
    "        # Forward fill and backward fill to handle edge cases for STL\n",
    "        ts_filled = var_ts.ffill().bfill()\n",
    "        \n",
    "        try:\n",
    "            # Perform STL decomposition with quarterly parameters\n",
    "            print(f\"  Performing STL decomposition...\")\n",
    "            print(f\"  Parameters: seasonal={seasonal}, period={period} (quarterly), robust=True\")\n",
    "            \n",
    "            stl = STL(ts_filled, seasonal=seasonal, period=period, robust=True)\n",
    "            result = stl.fit()\n",
    "            \n",
    "            # Extract components\n",
    "            trend_component = result.trend\n",
    "            seasonal_component = result.seasonal\n",
    "            residual_component = result.resid\n",
    "            \n",
    "            print(\"  STL decomposition completed successfully!\")\n",
    "            print(f\"  Trend range: {trend_component.min():.2f} to {trend_component.max():.2f}\")\n",
    "            print(f\"  Seasonal range: {seasonal_component.min():.2f} to {seasonal_component.max():.2f}\")\n",
    "            print(f\"  Residual std: {residual_component.std():.2f}\")\n",
    "            \n",
    "            # Impute missing values using trend + seasonal components\n",
    "            if original_missing_mask.any():\n",
    "                imputed_values = (trend_component[original_missing_mask] + \n",
    "                                 seasonal_component[original_missing_mask])\n",
    "                var_ts.loc[original_missing_mask] = imputed_values\n",
    "                \n",
    "                print(f\"\\nImputation Results:\")\n",
    "                print(f\"  Imputed {original_missing_mask.sum()} missing values\")\n",
    "                print(f\"  Imputed value range: {imputed_values.min():.2f} to {imputed_values.max():.2f}\")\n",
    "                \n",
    "                # Show some examples of imputed values\n",
    "                imputed_indices = var_ts.index[original_missing_mask]\n",
    "                if len(imputed_indices) <= 5:\n",
    "                    print(f\"  Imputed dates: {[d.date() for d in imputed_indices]}\")\n",
    "                else:\n",
    "                    print(f\"  Sample imputed dates: {[d.date() for d in imputed_indices[:3]]} ...\")\n",
    "            \n",
    "            # Update the dataframe\n",
    "            var_data[variable_name] = var_ts.values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  STL decomposition failed: {e}\")\n",
    "            print(\"  Falling back to linear interpolation...\")\n",
    "            var_data[variable_name] = var_data[variable_name].interpolate(method='linear')\n",
    "    \n",
    "    # Convert Date back to original format\n",
    "    var_data['Date'] = var_data['Date'].dt.date\n",
    "    \n",
    "    # Create a mapping dictionary for imputed values\n",
    "    imputed_mapping = dict(zip(var_data['Date'], var_data[variable_name]))\n",
    "    \n",
    "    # Apply imputed values to the original dataframe\n",
    "    df_copy[variable_name] = df_copy['Date'].map(imputed_mapping).fillna(df_copy[variable_name])\n",
    "    \n",
    "    # Final verification\n",
    "    final_missing = df_copy[variable_name].isna().sum()\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Missing values after imputation: {final_missing}\")\n",
    "    print(f\"  Imputation success: {'✓' if final_missing == 0 else '✗'}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# Step 1: Drop CAD_Export_Price_Lumber\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: DROPPING VARIABLES WITH EXCESSIVE MISSING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBefore dropping:\")\n",
    "print(f\"  Total features: {len(master_df_final.columns) - 1}\")  # Exclude Date\n",
    "print(f\"  CAD_Export_Price_Lumber missing: {master_df_final['CAD_Export_Price_Lumber'].isna().sum()}/79 (57.0%)\")\n",
    "\n",
    "# Create a copy and drop the variable\n",
    "df_imputed = master_df_final.drop(columns=['CAD_Export_Price_Lumber']).copy()\n",
    "\n",
    "print(f\"\\nAfter dropping:\")\n",
    "print(f\"  Total features: {len(df_imputed.columns) - 1}\")  # Exclude Date\n",
    "print(f\"  Dropped: CAD_Export_Price_Lumber\")\n",
    "\n",
    "\n",
    "# Step 2: Impute using STL decomposition\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: STL DECOMPOSITION IMPUTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Impute CAD_Building Permits\n",
    "df_imputed = impute_variable_stl(df_imputed, 'CAD_Building Permits', period=4, seasonal=11)\n",
    "\n",
    "# Impute CAD_BP_Single_Housing\n",
    "df_imputed = impute_variable_stl(df_imputed, 'CAD_BP_Single_Housing', period=4, seasonal=11)\n",
    "\n",
    "# Impute US_Households_Number\n",
    "df_imputed = impute_variable_stl(df_imputed, 'US_Households_Number', period=4, seasonal=11)\n",
    "\n",
    "\n",
    "# Step 3: Verification and Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPUTATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "remaining_missing = df_imputed.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "\n",
    "if len(remaining_missing) == 0:\n",
    "    print(\"\\n✓ SUCCESS: All missing values have been imputed!\")\n",
    "    print(f\"  Dataset is now 100% complete with {len(df_imputed)} observations\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: {len(remaining_missing)} variables still have missing values:\")\n",
    "    for var, count in remaining_missing.items():\n",
    "        print(f\"  {var}: {count} missing\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\nFinal Dataset Summary:\")\n",
    "print(f\"  Total rows: {len(df_imputed)}\")\n",
    "print(f\"  Total columns: {len(df_imputed.columns)} (1 Date + {len(df_imputed.columns)-1} features)\")\n",
    "print(f\"  Date range: {df_imputed['Date'].min()} to {df_imputed['Date'].max()}\")\n",
    "print(f\"  Overall completeness: {(1 - df_imputed.isnull().sum().sum() / (len(df_imputed) * len(df_imputed.columns))) * 100:.1f}%\")\n",
    "\n",
    "# Display sample of imputed data\n",
    "print(f\"\\nSample of imputed dataset (first 5 rows):\")\n",
    "print(df_imputed[['Date', 'CAD_Building Permits', 'CAD_BP_Single_Housing', 'US_Households_Number']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Master DataFrame Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original total rows: {len(master_df)}\")\n",
    "print(f\"Filtered total rows: {len(master_df_filtered)}\")\n",
    "print(f\"Final total rows (after STL imputation): {len(master_df_final)}\")\n",
    "print(f\"Total columns: {len(master_df_final.columns)} (Date + {len(master_df_final.columns)-1} variables)\")\n",
    "print(f\"Date range: {master_df_final['Date'].min()} to {master_df_final['Date'].max()}\")\n",
    "print(f\"\\nColumns: {', '.join(master_df_final.columns.tolist())}\")\n",
    "\n",
    "# Show softwood data completeness\n",
    "softwood_missing = master_df_final['CAD_Softwood_Export_to_US'].isna().sum()\n",
    "print(f\"\\nCAD_Softwood_Export_to_US completeness: {((len(master_df_final) - softwood_missing) / len(master_df_final) * 100):.1f}% ({softwood_missing} missing values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_file = os.path.join(output_dir, 'bloomberg_master_dataframe.csv')\n",
    "master_df_final.to_csv(output_file, index=False)\n",
    "print(f\"Master dataframe saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "output_excel = os.path.join(output_dir, 'bloomberg_master_dataframe.xlsx')\n",
    "master_df_final.to_excel(output_excel, index=False)\n",
    "print(f\"Master dataframe saved to: {output_excel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Setting plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 1. Time Series Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 1.1 Target Variable: Canadian Softwood Exports to US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(master_df_final['Date'], master_df_final['CAD_Softwood_Export_to_US'], \n",
    "        linewidth=2, color='#2c7bb6')\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Quarterly Exports (Thousand Cubic Meters)', fontsize=11)\n",
    "ax.set_title('Canada-US Softwood Lumber Exports Over Time', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Export Statistics:\")\n",
    "print(f\"  Mean: {master_df_final['CAD_Softwood_Export_to_US'].mean():,.0f} FBM\")\n",
    "print(f\"  Std Dev: {master_df_final['CAD_Softwood_Export_to_US'].std():,.0f} FBM\")\n",
    "print(f\"  Min: {master_df_final['CAD_Softwood_Export_to_US'].min():,.0f} FBM\")\n",
    "print(f\"  Max: {master_df_final['CAD_Softwood_Export_to_US'].max():,.0f} FBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 2. Key Economic Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 2.1 US Housing Market Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_vars = ['US_Building_Permits', 'US_Housing_Start', 'US_New_Home_Sales', \n",
    "                'US_Existing_Home _Sales']\n",
    "available_housing = [v for v in housing_vars if v in master_df_final.columns]\n",
    "\n",
    "if available_housing:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, var in enumerate(available_housing):\n",
    "        axes[i].plot(master_df_final['Date'], master_df_final[var], \n",
    "                    linewidth=2, color='#d7191c')\n",
    "        axes[i].set_xlabel('Date', fontsize=10)\n",
    "        axes[i].set_ylabel('Units', fontsize=10)\n",
    "        axes[i].set_title(var.replace('_', ' '), fontsize=11, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 2.2 Price Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_vars = ['US_CPI', 'CAD_CPI', 'CAD_Export_Price_Lumber']\n",
    "available_prices = [v for v in price_vars if v in master_df_final.columns]\n",
    "\n",
    "if len(available_prices) >= 2:\n",
    "    fig, axes = plt.subplots(1, len(available_prices), figsize=(14, 5))\n",
    "    if len(available_prices) == 1:\n",
    "        axes = [axes]  # Make it iterable for single subplot\n",
    "    \n",
    "    for i, var in enumerate(available_prices):\n",
    "        # Convert to numeric, handling any non-numeric values\n",
    "        price_data = pd.to_numeric(master_df_final[var], errors='coerce')\n",
    "        \n",
    "        axes[i].plot(master_df_final['Date'], price_data, \n",
    "                    linewidth=2, color='#fdae61')\n",
    "        axes[i].set_xlabel('Date', fontsize=10)\n",
    "        axes[i].set_ylabel('Index/Price', fontsize=10)\n",
    "        axes[i].set_title(var.replace('_', ' '), fontsize=11, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for price indicators\n",
    "    print(\"\\nPrice Indicators Summary:\")\n",
    "    for var in available_prices:\n",
    "        price_data = pd.to_numeric(master_df_final[var], errors='coerce')\n",
    "        non_null_count = price_data.notna().sum()\n",
    "        print(f\"  {var}: {non_null_count} data points\")\n",
    "        if non_null_count > 0:\n",
    "            print(f\"    Range: {price_data.min():.2f} to {price_data.max():.2f}\")\n",
    "else:\n",
    "    print(\"Insufficient price indicator data available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### 3.1 Correlation with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to numeric (excluding Date)\n",
    "print(\"Converting columns to numeric for correlation analysis...\")\n",
    "target = 'CAD_Softwood_Export_to_US'\n",
    "\n",
    "# Get all columns except Date\n",
    "all_cols = [col for col in master_df_final.columns if col != 'Date']\n",
    "\n",
    "# Convert each column to numeric, handling errors\n",
    "for col in all_cols:\n",
    "    if col != target:  # Target is already numeric\n",
    "        master_df_final[col] = pd.to_numeric(master_df_final[col], errors='coerce')\n",
    "\n",
    "# Now select numeric columns (exclude Date)\n",
    "numeric_cols = master_df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Found {len(numeric_cols)} numeric columns: {numeric_cols}\")\n",
    "\n",
    "# Show data completeness after conversion\n",
    "print(f\"\\nData completeness after numeric conversion:\")\n",
    "for col in numeric_cols:\n",
    "    non_null_count = master_df_final[col].notna().sum()\n",
    "    total_count = len(master_df_final)\n",
    "    print(f\"  {col}: {non_null_count}/{total_count} ({non_null_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "print(\"Calculating correlations with target variable...\")\n",
    "correlations = []\n",
    "for col in numeric_cols:\n",
    "    if col != target:\n",
    "        # Remove rows with missing values in either column\n",
    "        valid_data = master_df_final[[target, col]].dropna()\n",
    "        if len(valid_data) > 10:  # Need sufficient data points\n",
    "            try:\n",
    "                corr, pval = pearsonr(valid_data[target], valid_data[col])\n",
    "                correlations.append({\n",
    "                    'Variable': col,\n",
    "                    'Correlation': corr,\n",
    "                    'P-value': pval,\n",
    "                    'Significant': pval < 0.05,\n",
    "                    'Data_Points': len(valid_data)\n",
    "                })\n",
    "                print(f\"  {col}: r={corr:.3f}, p={pval:.3f}, n={len(valid_data)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {col}: Error calculating correlation - {e}\")\n",
    "\n",
    "print(f\"\\nFound {len(correlations)} valid correlations\")\n",
    "\n",
    "if len(correlations) > 0:\n",
    "    corr_df = pd.DataFrame(correlations).sort_values('Correlation', \n",
    "                                                      key=abs, \n",
    "                                                      ascending=False)\n",
    "    print(f\"Correlation analysis completed successfully!\")\n",
    "else:\n",
    "    print(\"No valid correlations found. Creating empty DataFrame.\")\n",
    "    corr_df = pd.DataFrame(columns=['Variable', 'Correlation', 'P-value', 'Significant', 'Data_Points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 3.2 Top Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top correlations\n",
    "if len(corr_df) > 0:\n",
    "    top_n = min(10, len(corr_df))  # Use available correlations or 10, whichever is smaller\n",
    "    top_corr = corr_df.head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, len(top_corr) * 0.4)))\n",
    "    colors = ['#2c7bb6' if x > 0 else '#d7191c' for x in top_corr['Correlation']]\n",
    "    bars = ax.barh(range(len(top_corr)), top_corr['Correlation'], color=colors)\n",
    "    ax.set_yticks(range(len(top_corr)))\n",
    "    ax.set_yticklabels([v.replace('_', ' ') for v in top_corr['Variable']], fontsize=9)\n",
    "    ax.set_xlabel('Correlation Coefficient', fontsize=11)\n",
    "    ax.set_title(f'Top {top_n} Correlations with Canadian Softwood Exports', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add correlation values as text on bars\n",
    "    for i, (idx, row) in enumerate(top_corr.iterrows()):\n",
    "        ax.text(row['Correlation'] + (0.01 if row['Correlation'] > 0 else -0.01), \n",
    "                i, f'{row[\"Correlation\"]:.3f}', \n",
    "                va='center', ha='left' if row['Correlation'] > 0 else 'right', \n",
    "                fontsize=8, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nTop {top_n} Correlations (with p-values and significance):\")\n",
    "    display_cols = ['Variable', 'Correlation', 'P-value', 'Significant', 'Data_Points']\n",
    "    print(corr_df[display_cols].head(top_n).to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    significant_corr = corr_df[corr_df['Significant'] == True]\n",
    "    print(f\"\\nCorrelation Summary:\")\n",
    "    print(f\"  Total correlations calculated: {len(corr_df)}\")\n",
    "    print(f\"  Significant correlations (p < 0.05): {len(significant_corr)}\")\n",
    "    if len(significant_corr) > 0:\n",
    "        print(f\"  Strongest positive correlation: {significant_corr[significant_corr['Correlation'] > 0]['Correlation'].max():.3f}\")\n",
    "        print(f\"  Strongest negative correlation: {significant_corr[significant_corr['Correlation'] < 0]['Correlation'].min():.3f}\")\n",
    "else:\n",
    "    print(\"No correlations available to plot.\")\n",
    "    print(\"This might be due to insufficient data or all variables being constant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 3.3 Correlation Matrix Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key variables for correlation analysis\n",
    "key_vars = ['CAD_Softwood_Export_to_US', 'US_Housing_Start', 'US_Building_Permits', \n",
    "            'US_CPI', 'CAD_Export_Price_Lumber', 'USCAD_Exchange_Rate']\n",
    "\n",
    "# Filter to variables that exist in the dataset\n",
    "available_vars = [var for var in key_vars if var in master_df_final.columns]\n",
    "n_vars = len(available_vars)\n",
    "\n",
    "# Create correlation matrix plot\n",
    "fig, axes = plt.subplots(n_vars, n_vars, figsize=(12, 10))\n",
    "fig.suptitle('Variable Relationships Matrix', fontsize=16, y=0.95)\n",
    "\n",
    "for i in range(n_vars):\n",
    "    for j in range(n_vars):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        if i == j:\n",
    "            # Diagonal: show variable name\n",
    "            ax.text(0.5, 0.5, available_vars[i].replace('_', ' '), \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # Off-diagonal: scatter plot\n",
    "            x_var = available_vars[j]\n",
    "            y_var = available_vars[i]\n",
    "            \n",
    "            # Get data\n",
    "            data = master_df_final[[x_var, y_var]].dropna()\n",
    "            \n",
    "            if len(data) > 5:\n",
    "                # Create scatter plot\n",
    "                ax.scatter(data[x_var], data[y_var], alpha=0.6, s=30)\n",
    "                \n",
    "                # Add correlation coefficient\n",
    "                if len(data) > 10:\n",
    "                    corr, p_val = pearsonr(data[x_var], data[y_var])\n",
    "                    ax.text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                           transform=ax.transAxes, fontsize=8, \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Set labels\n",
    "            if i == n_vars - 1:  # Bottom row\n",
    "                ax.set_xlabel(x_var.replace('_', ' '), fontsize=8)\n",
    "            if j == 0:  # Left column\n",
    "                ax.set_ylabel(y_var.replace('_', ' '), fontsize=8)\n",
    "            \n",
    "            ax.tick_params(labelsize=6)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 4. Data Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 4.1 Data Coverage Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### 5.1 Scatter Plot: Key Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "\"Key relationships\" refers to the strongest correlations between economic indicators and Canadian softwood exports. These help identify which factors most influence export volumes and can guide forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 3 most correlated variables\n",
    "if len(corr_df) >= 3:\n",
    "    top_vars = corr_df.head(3)['Variable'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, var in enumerate(top_vars):\n",
    "        data = master_df_final[[target, var]].dropna()\n",
    "        \n",
    "        axes[i].scatter(data[var], data[target], alpha=0.6)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(data[var], data[target], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[i].plot(data[var], p(data[var]), \"r--\", alpha=0.8)\n",
    "        \n",
    "        axes[i].set_xlabel(var.replace('_', ' '))\n",
    "        axes[i].set_ylabel('Softwood Exports')\n",
    "        corr_val = corr_df[corr_df['Variable'] == var]['Correlation'].values[0]\n",
    "        axes[i].set_title(f'{var.replace(\"_\", \" \")}\\n(r = {corr_val:.3f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Step 2: Train/Validation/Test Split (60/20/20)\n",
    "\n",
    "For time series data, we must use **chronological splitting** to prevent data leakage. Random shuffling would violate the temporal dependency structure and lead to overly optimistic performance estimates.\n",
    "\n",
    "### Splitting Strategy:\n",
    "- **Training Set (60%)**: Used to train models - oldest data\n",
    "- **Validation Set (20%)**: Used for hyperparameter tuning and model selection - middle period\n",
    "- **Test Set (20%)**: Final evaluation only - most recent data\n",
    "\n",
    "### Why Chronological Split?\n",
    "1. **Preserves temporal ordering**: Models are trained on past data to predict future\n",
    "2. **Realistic evaluation**: Mimics real-world forecasting scenario\n",
    "3. **Prevents data leakage**: Future information never informs past predictions\n",
    "4. **Respects autocorrelation**: Time series dependencies remain intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split (60/20/20 Chronological)\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT (60/20/20 CHRONOLOGICAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort by date to ensure chronological order (ascending - oldest to newest)\n",
    "df_imputed_sorted = df_imputed.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Total observations: {len(df_imputed_sorted)}\")\n",
    "print(f\"  Date range: {df_imputed_sorted['Date'].min()} to {df_imputed_sorted['Date'].max()}\")\n",
    "print(f\"  Total features: {len(df_imputed_sorted.columns) - 2}\")  # Exclude Date and target\n",
    "\n",
    "# Calculate split indices\n",
    "n_total = len(df_imputed_sorted)\n",
    "n_train = int(n_total * 0.60)\n",
    "n_val = int(n_total * 0.20)\n",
    "n_test = n_total - n_train - n_val  # Ensure all observations are used\n",
    "\n",
    "print(f\"\\nSplit Sizes:\")\n",
    "print(f\"  Training:   {n_train} observations ({n_train/n_total*100:.1f}%)\")\n",
    "print(f\"  Validation: {n_val} observations ({n_val/n_total*100:.1f}%)\")\n",
    "print(f\"  Test:       {n_test} observations ({n_test/n_total*100:.1f}%)\")\n",
    "\n",
    "# Perform chronological split\n",
    "train_df = df_imputed_sorted.iloc[:n_train].copy()\n",
    "val_df = df_imputed_sorted.iloc[n_train:n_train+n_val].copy()\n",
    "test_df = df_imputed_sorted.iloc[n_train+n_val:].copy()\n",
    "\n",
    "# Display date ranges for each split\n",
    "print(f\"\\nDate Ranges:\")\n",
    "print(f\"  Training:   {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
    "print(f\"  Validation: {val_df['Date'].min()} to {val_df['Date'].max()}\")\n",
    "print(f\"  Test:       {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
    "\n",
    "# Separate features (X) and target (y) for each split\n",
    "target_col = 'CAD_Softwood_Export_to_US'\n",
    "feature_cols = [col for col in df_imputed_sorted.columns if col not in ['Date', target_col]]\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_val = val_df[target_col].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_col].copy()\n",
    "\n",
    "print(f\"\\nFeature Matrix Shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} (rows × features)\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget Vector Shapes:\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val:   {y_val.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")\n",
    "\n",
    "# Verify no data leakage - dates should not overlap\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATA LEAKAGE CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "train_end = train_df['Date'].max()\n",
    "val_start = val_df['Date'].min()\n",
    "val_end = val_df['Date'].max()\n",
    "test_start = test_df['Date'].min()\n",
    "\n",
    "if train_end < val_start and val_end < test_start:\n",
    "    print(\"✓ PASS: No temporal overlap between splits\")\n",
    "    print(f\"  Training ends:     {train_end}\")\n",
    "    print(f\"  Validation starts: {val_start}\")\n",
    "    print(f\"  Validation ends:   {val_end}\")\n",
    "    print(f\"  Test starts:       {test_start}\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Temporal overlap detected!\")\n",
    "    print(f\"  Training ends:     {train_end}\")\n",
    "    print(f\"  Validation starts: {val_start}\")\n",
    "    print(f\"  Validation ends:   {val_end}\")\n",
    "    print(f\"  Test starts:       {test_start}\")\n",
    "\n",
    "# Summary statistics for target variable across splits\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TARGET VARIABLE DISTRIBUTION ACROSS SPLITS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Mean:   {y_train.mean():,.0f}\")\n",
    "print(f\"  Std:    {y_train.std():,.0f}\")\n",
    "print(f\"  Min:    {y_train.min():,.0f}\")\n",
    "print(f\"  Max:    {y_train.max():,.0f}\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Mean:   {y_val.mean():,.0f}\")\n",
    "print(f\"  Std:    {y_val.std():,.0f}\")\n",
    "print(f\"  Min:    {y_val.min():,.0f}\")\n",
    "print(f\"  Max:    {y_val.max():,.0f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Mean:   {y_test.mean():,.0f}\")\n",
    "print(f\"  Std:    {y_test.std():,.0f}\")\n",
    "print(f\"  Min:    {y_test.min():,.0f}\")\n",
    "print(f\"  Max:    {y_test.max():,.0f}\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Plot all data\n",
    "ax.plot(train_df['Date'], y_train, 'o-', label='Training', color='#2c7bb6', linewidth=2, markersize=5)\n",
    "ax.plot(val_df['Date'], y_val, 'o-', label='Validation', color='#fdae61', linewidth=2, markersize=5)\n",
    "ax.plot(test_df['Date'], y_test, 'o-', label='Test', color='#d7191c', linewidth=2, markersize=5)\n",
    "\n",
    "# Add vertical lines to separate splits\n",
    "ax.axvline(x=train_df['Date'].max(), color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axvline(x=val_df['Date'].max(), color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Softwood Exports (Thousand Cubic Meters)', fontsize=11)\n",
    "ax.set_title('Train/Validation/Test Split (Chronological)', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SPLIT COMPLETE - READY FOR FEATURE ENGINEERING\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "We will create features that capture:\n",
    "1. **Temporal patterns**: Trend and seasonality\n",
    "2. **Historical dependencies**: Lag features\n",
    "3. **Domain knowledge**: Economic relationships and interactions\n",
    "\n",
    "### Feature Engineering Strategy:\n",
    "\n",
    "**3.1 Time-Based Features:**\n",
    "- Linear trend (captures long-term growth/decline)\n",
    "- Quarterly dummy variables (captures seasonality)\n",
    "\n",
    "**3.2 Lag Features:**\n",
    "- Target lags (past export values predict future)\n",
    "- Predictor lags (past economic conditions predict future exports)\n",
    "\n",
    "**3.3 Derived Features:**\n",
    "- Moving averages (smoothed trends)\n",
    "- Year-over-year growth rates (economic dynamics)\n",
    "- Interaction terms (multiplicative effects)\n",
    "\n",
    "Features must be engineered on the **training set first**, then applied to validation and test sets to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Time-Based Features (Trend + Seasonality)\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3.1: TIME-BASED FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We need to work with the full sorted dataframe to maintain temporal order\n",
    "# Then split again after feature engineering\n",
    "\n",
    "# Start with the sorted dataframe from Step 2\n",
    "df_features = df_imputed_sorted.copy()\n",
    "\n",
    "print(f\"\\nStarting dataset: {df_features.shape}\")\n",
    "\n",
    "# Feature 1: Linear Trend (0, 1, 2, ..., n-1)\n",
    "# Captures long-term growth or decline in exports\n",
    "df_features['trend'] = np.arange(len(df_features))\n",
    "\n",
    "print(f\"\\n1. TREND VARIABLE:\")\n",
    "print(f\"   Created linear trend variable\")\n",
    "print(f\"   Range: {df_features['trend'].min()} to {df_features['trend'].max()}\")\n",
    "print(f\"   Interpretation: Captures systematic increase/decrease over time\")\n",
    "\n",
    "# Feature 2: Quarterly Dummy Variables\n",
    "# Extract quarter from date\n",
    "df_features['Date_dt'] = pd.to_datetime(df_features['Date'])\n",
    "df_features['quarter'] = df_features['Date_dt'].dt.quarter\n",
    "\n",
    "# Create dummy variables (drop Q1 to avoid multicollinearity)\n",
    "quarter_dummies = pd.get_dummies(df_features['quarter'], prefix='Q', drop_first=True)\n",
    "df_features = pd.concat([df_features, quarter_dummies], axis=1)\n",
    "\n",
    "print(f\"\\n2. QUARTERLY SEASONALITY:\")\n",
    "print(f\"   Created dummy variables: Q_2, Q_3, Q_4 (Q1 is baseline)\")\n",
    "print(f\"   Interpretation:\")\n",
    "print(f\"   - Q_2 = 1 if Q2, 0 otherwise (Spring)\")\n",
    "print(f\"   - Q_3 = 1 if Q3, 0 otherwise (Summer)\")\n",
    "print(f\"   - Q_4 = 1 if Q4, 0 otherwise (Fall)\")\n",
    "print(f\"   - Q1 (Winter) is captured in the intercept\")\n",
    "\n",
    "# Drop temporary columns\n",
    "df_features = df_features.drop(['Date_dt', 'quarter'], axis=1)\n",
    "\n",
    "# Summary\n",
    "new_features = ['trend', 'Q_2', 'Q_3', 'Q_4']\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TIME-BASED FEATURES CREATED: {len(new_features)}\")\n",
    "print(f\"{'='*70}\")\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nUpdated dataset shape: {df_features.shape}\")\n",
    "print(f\"Total features added: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### 3.2 Lag Features\n",
    "\n",
    "Lag features capture **temporal dependencies** - the idea that past values help predict future values.\n",
    "\n",
    "**Why Lags Are Important:**\n",
    "- **Target lags**: Past export volumes influence future exports (autocorrelation)\n",
    "- **Predictor lags**: Economic indicators take time to affect exports (e.g., housing permits → construction → lumber demand)\n",
    "\n",
    "**Strategy:**\n",
    "- **Target variable lags**: lag-1 (previous quarter), lag-4 (same quarter last year)\n",
    "- **Key predictor lags**: lag-1 for top correlated housing variables\n",
    "- **Trade-off**: More lags = more features but fewer observations (due to NaN values)\n",
    "\n",
    "**Note**: Lag features create missing values at the beginning of the time series, which we'll handle by dropping those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Lag Features\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3.2: LAG FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nStarting dataset: {df_features.shape}\")\n",
    "print(f\"Date range: {df_features['Date'].min()} to {df_features['Date'].max()}\")\n",
    "\n",
    "# List to track new lag features\n",
    "lag_features_created = []\n",
    "\n",
    "# Create lag features for TARGET variable (CAD_Softwood_Export_to_US)\n",
    "print(f\"\\n1. TARGET VARIABLE LAGS:\")\n",
    "\n",
    "target_col = 'CAD_Softwood_Export_to_US'\n",
    "\n",
    "# Lag-1: Previous quarter's exports\n",
    "df_features[f'{target_col}_lag1'] = df_features[target_col].shift(1)\n",
    "lag_features_created.append(f'{target_col}_lag1')\n",
    "print(f\"   ✓ Created {target_col}_lag1 (previous quarter)\")\n",
    "\n",
    "# Lag-2: Two quarters ago\n",
    "df_features[f'{target_col}_lag2'] = df_features[target_col].shift(2)\n",
    "lag_features_created.append(f'{target_col}_lag2')\n",
    "print(f\"   ✓ Created {target_col}_lag2 (two quarters ago)\")\n",
    "\n",
    "# Lag-4: Same quarter last year (captures yearly seasonality)\n",
    "df_features[f'{target_col}_lag4'] = df_features[target_col].shift(4)\n",
    "lag_features_created.append(f'{target_col}_lag4')\n",
    "print(f\"   ✓ Created {target_col}_lag4 (same quarter last year)\")\n",
    "\n",
    "# Create lag features for KEY PREDICTORS\n",
    "# Based on correlation analysis, these had the strongest relationships with target:\n",
    "# - US_BP_Single_Housing (r=0.752)\n",
    "# - US_New_Home_Sales (r=0.744)\n",
    "# - US_Housing_Start (r=0.732)\n",
    "\n",
    "print(f\"\\n2. KEY PREDICTOR LAGS (lag-1 only to limit feature growth):\")\n",
    "\n",
    "key_predictors = [\n",
    "    'US _BP_Single_Housing',\n",
    "    'US_New_Home_Sales', \n",
    "    'US_Housing_Start',\n",
    "    'US_Building_Permits'\n",
    "]\n",
    "\n",
    "for pred in key_predictors:\n",
    "    if pred in df_features.columns:\n",
    "        lag_col = f'{pred}_lag1'\n",
    "        df_features[lag_col] = df_features[pred].shift(1)\n",
    "        lag_features_created.append(lag_col)\n",
    "        print(f\"   ✓ Created {lag_col}\")\n",
    "\n",
    "# Check for missing values created by lagging\n",
    "print(f\"\\n3. MISSING VALUES ANALYSIS:\")\n",
    "print(f\"   Lag features create NaN values at the beginning of the time series\")\n",
    "\n",
    "# Count NaN values in lag features\n",
    "for lag_feat in lag_features_created:\n",
    "    nan_count = df_features[lag_feat].isna().sum()\n",
    "    print(f\"   {lag_feat}: {nan_count} NaN values\")\n",
    "\n",
    "# Drop rows with ANY missing values in lag features\n",
    "# This is necessary because we can't make predictions without complete feature sets\n",
    "print(f\"\\n4. HANDLING MISSING VALUES:\")\n",
    "print(f\"   Before dropping: {len(df_features)} rows\")\n",
    "\n",
    "# Store original date range\n",
    "original_start = df_features['Date'].min()\n",
    "original_end = df_features['Date'].max()\n",
    "\n",
    "# Drop rows with NaN in lag features\n",
    "df_features_clean = df_features.dropna(subset=lag_features_created).reset_index(drop=True)\n",
    "\n",
    "print(f\"   After dropping:  {len(df_features_clean)} rows\")\n",
    "print(f\"   Rows lost:       {len(df_features) - len(df_features_clean)}\")\n",
    "print(f\"   Original date range: {original_start} to {original_end}\")\n",
    "print(f\"   New date range:      {df_features_clean['Date'].min()} to {df_features_clean['Date'].max()}\")\n",
    "\n",
    "# Update the main dataframe\n",
    "df_features = df_features_clean.copy()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"LAG FEATURES CREATED: {len(lag_features_created)}\")\n",
    "print(f\"{'='*70}\")\n",
    "for feat in lag_features_created:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nUpdated dataset shape: {df_features.shape}\")\n",
    "print(f\"Total lag features added: {len(lag_features_created)}\")\n",
    "print(f\"\\nNote: We lost the first few observations due to lagging, but this is\")\n",
    "print(f\"      necessary to avoid using future information in our predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 3.3 Derived Features\n",
    "\n",
    "Derived features combine existing variables to create new insights based on **domain knowledge** about economics and housing markets.\n",
    "\n",
    "**Strategy:**\n",
    "1. **Moving Averages**: Smooth short-term fluctuations to reveal trends\n",
    "2. **Year-over-Year Growth Rates**: Capture economic momentum (expansion/contraction)\n",
    "3. **Interaction Terms**: Capture multiplicative effects between variables\n",
    "\n",
    "**Rationale:**\n",
    "- **Moving averages** reduce noise in housing indicators\n",
    "- **Growth rates** are often better predictors than absolute levels for economic data\n",
    "- **Interactions** capture compound effects (e.g., high mortgage rates × many permits)\n",
    "\n",
    "**Constraint**: We limit derived features to avoid overfitting given our sample size (~75 observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Derived Features\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 3.3: DERIVED FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nStarting dataset: {df_features.shape}\")\n",
    "\n",
    "# List to track new derived features\n",
    "derived_features_created = []\n",
    "\n",
    "# 1. MOVING AVERAGES (2-quarter rolling average for key housing indicators)\n",
    "print(f\"\\n1. MOVING AVERAGES (2-quarter rolling window):\")\n",
    "print(f\"   Purpose: Smooth short-term fluctuations, reduce noise\")\n",
    "\n",
    "ma_variables = [\n",
    "    'US_Housing_Start',\n",
    "    'US_Building_Permits',\n",
    "    'US_New_Home_Sales'\n",
    "]\n",
    "\n",
    "for var in ma_variables:\n",
    "    if var in df_features.columns:\n",
    "        ma_col = f'{var}_MA2'\n",
    "        df_features[ma_col] = df_features[var].rolling(window=2, min_periods=1).mean()\n",
    "        derived_features_created.append(ma_col)\n",
    "        print(f\"   ✓ Created {ma_col}\")\n",
    "\n",
    "\n",
    "# 2. YEAR-OVER-YEAR GROWTH RATES (4-quarter change)\n",
    "print(f\"\\n2. YEAR-OVER-YEAR GROWTH RATES:\")\n",
    "print(f\"   Purpose: Capture economic momentum and change direction\")\n",
    "\n",
    "# For target variable\n",
    "target_col = 'CAD_Softwood_Export_to_US'\n",
    "yoy_target = f'{target_col}_YoY_growth'\n",
    "df_features[yoy_target] = df_features[target_col].pct_change(periods=4) * 100\n",
    "derived_features_created.append(yoy_target)\n",
    "print(f\"   ✓ Created {yoy_target} (% change from same quarter last year)\")\n",
    "\n",
    "# For key economic indicators\n",
    "yoy_variables = ['US_GDP', 'CAD_GDP']\n",
    "\n",
    "for var in yoy_variables:\n",
    "    if var in df_features.columns:\n",
    "        yoy_col = f'{var}_YoY_growth'\n",
    "        df_features[yoy_col] = df_features[var].pct_change(periods=4) * 100\n",
    "        derived_features_created.append(yoy_col)\n",
    "        print(f\"   ✓ Created {yoy_col}\")\n",
    "\n",
    "\n",
    "# 3. INTERACTION TERMS (multiplicative effects)\n",
    "print(f\"\\n3. INTERACTION TERMS:\")\n",
    "print(f\"   Purpose: Capture compound effects between related variables\")\n",
    "\n",
    "# Interaction 1: Housing starts × Mortgage rates\n",
    "# Rationale: Impact of housing activity depends on financing costs\n",
    "if 'US_Housing_Start' in df_features.columns and 'US_Mortgage_Interest_30Y' in df_features.columns:\n",
    "    interaction1 = 'US_Housing_Start_x_Mortgage_Rate'\n",
    "    df_features[interaction1] = df_features['US_Housing_Start'] * df_features['US_Mortgage_Interest_30Y']\n",
    "    derived_features_created.append(interaction1)\n",
    "    print(f\"   ✓ Created {interaction1}\")\n",
    "    print(f\"      Rationale: Housing activity impact varies with financing costs\")\n",
    "\n",
    "# Interaction 2: Exchange rate × Housing activity\n",
    "# Rationale: Canadian exports to US depend on both demand (housing) and price competitiveness (exchange rate)\n",
    "if 'USCAD_Exchange_Rate' in df_features.columns and 'US_Building_Permits' in df_features.columns:\n",
    "    interaction2 = 'Exchange_Rate_x_Building_Permits'\n",
    "    df_features[interaction2] = df_features['USCAD_Exchange_Rate'] * df_features['US_Building_Permits']\n",
    "    derived_features_created.append(interaction2)\n",
    "    print(f\"   ✓ Created {interaction2}\")\n",
    "    print(f\"      Rationale: Export competitiveness depends on both price and demand\")\n",
    "\n",
    "\n",
    "# 4. HANDLE MISSING VALUES FROM DERIVED FEATURES\n",
    "print(f\"\\n4. HANDLING MISSING VALUES FROM DERIVED FEATURES:\")\n",
    "\n",
    "# Count NaN values in derived features\n",
    "nan_counts = {}\n",
    "for feat in derived_features_created:\n",
    "    nan_count = df_features[feat].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        nan_counts[feat] = nan_count\n",
    "\n",
    "if len(nan_counts) > 0:\n",
    "    print(f\"   Missing values detected in derived features:\")\n",
    "    for feat, count in nan_counts.items():\n",
    "        print(f\"   - {feat}: {count} NaN values\")\n",
    "    \n",
    "    print(f\"\\n   Before dropping: {len(df_features)} rows\")\n",
    "    df_features_clean = df_features.dropna().reset_index(drop=True)\n",
    "    print(f\"   After dropping:  {len(df_features_clean)} rows\")\n",
    "    print(f\"   Rows lost:       {len(df_features) - len(df_features_clean)}\")\n",
    "    \n",
    "    df_features = df_features_clean.copy()\n",
    "else:\n",
    "    print(f\"   No missing values detected in derived features\")\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DERIVED FEATURES CREATED: {len(derived_features_created)}\")\n",
    "print(f\"{'='*70}\")\n",
    "for feat in derived_features_created:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_features.shape}\")\n",
    "print(f\"Total derived features added: {len(derived_features_created)}\")\n",
    "\n",
    "# Overall feature engineering summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FEATURE ENGINEERING COMPLETE - SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Count all engineered features\n",
    "time_based = ['trend', 'Q_2', 'Q_3', 'Q_4']\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  Time-based features:  {len(time_based)}\")\n",
    "print(f\"  Lag features:         7\")\n",
    "print(f\"  Derived features:     {len(derived_features_created)}\")\n",
    "print(f\"  Total new features:   {len(time_based) + 7 + len(derived_features_created)}\")\n",
    "\n",
    "print(f\"\\nFinal Dataset:\")\n",
    "print(f\"  Total rows:    {len(df_features)}\")\n",
    "print(f\"  Total columns: {len(df_features.columns)}\")\n",
    "print(f\"  Date range:    {df_features['Date'].min()} to {df_features['Date'].max()}\")\n",
    "\n",
    "# Show sample of engineered features\n",
    "print(f\"\\nSample of engineered dataset (first 3 rows):\")\n",
    "sample_cols = ['Date', target_col, 'trend', 'Q_2', f'{target_col}_lag1', \n",
    "               'US_Housing_Start_MA2', f'{target_col}_YoY_growth']\n",
    "available_sample_cols = [col for col in sample_cols if col in df_features.columns]\n",
    "print(df_features[available_sample_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Step 4: Re-split Data and Apply Feature Scaling\n",
    "\n",
    "After feature engineering, we need to:\n",
    "1. **Re-split** the data (since we lost observations from lagging/YoY growth)\n",
    "2. **Scale features** using StandardScaler (required for Ridge, Lasso, ElasticNet)\n",
    "\n",
    "### Why Re-split?\n",
    "- Feature engineering reduced our dataset from 79 → ~71 observations\n",
    "- Need to maintain 60/20/20 split on the final cleaned dataset\n",
    "\n",
    "### Why Feature Scaling?\n",
    "- **Regularized models** (Ridge, Lasso, ElasticNet) are sensitive to feature scale\n",
    "- Variables with larger ranges dominate the penalty term\n",
    "- StandardScaler: transforms features to mean=0, std=1\n",
    "- **Important**: Fit scaler on training set only, then transform val/test (prevent data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Re-split Data and Apply Feature Scaling\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: RE-SPLIT DATA AND APPLY FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. RE-SPLIT DATA (60/20/20 chronological)\n",
    "print(f\"\\n1. RE-SPLITTING DATA AFTER FEATURE ENGINEERING:\")\n",
    "print(f\"   Dataset is already sorted chronologically from feature engineering\")\n",
    "\n",
    "print(f\"\\nFinal dataset for modeling:\")\n",
    "print(f\"  Total rows: {len(df_features)}\")\n",
    "print(f\"  Date range: {df_features['Date'].min()} to {df_features['Date'].max()}\")\n",
    "\n",
    "# Calculate new split indices\n",
    "n_total = len(df_features)\n",
    "n_train = int(n_total * 0.60)\n",
    "n_val = int(n_total * 0.20)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "print(f\"\\nNew split sizes:\")\n",
    "print(f\"  Training:   {n_train} observations ({n_train/n_total*100:.1f}%)\")\n",
    "print(f\"  Validation: {n_val} observations ({n_val/n_total*100:.1f}%)\")\n",
    "print(f\"  Test:       {n_test} observations ({n_test/n_total*100:.1f}%)\")\n",
    "\n",
    "# Perform split\n",
    "train_df_final = df_features.iloc[:n_train].copy()\n",
    "val_df_final = df_features.iloc[n_train:n_train+n_val].copy()\n",
    "test_df_final = df_features.iloc[n_train+n_val:].copy()\n",
    "\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Training:   {train_df_final['Date'].min()} to {train_df_final['Date'].max()}\")\n",
    "print(f\"  Validation: {val_df_final['Date'].min()} to {val_df_final['Date'].max()}\")\n",
    "print(f\"  Test:       {test_df_final['Date'].min()} to {test_df_final['Date'].max()}\")\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'CAD_Softwood_Export_to_US'\n",
    "feature_cols_final = [col for col in df_features.columns if col not in ['Date', target_col]]\n",
    "\n",
    "X_train_final = train_df_final[feature_cols_final].copy()\n",
    "y_train_final = train_df_final[target_col].copy()\n",
    "\n",
    "X_val_final = val_df_final[feature_cols_final].copy()\n",
    "y_val_final = val_df_final[target_col].copy()\n",
    "\n",
    "X_test_final = test_df_final[feature_cols_final].copy()\n",
    "y_test_final = test_df_final[target_col].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"  X_train: {X_train_final.shape}\")\n",
    "print(f\"  X_val:   {X_val_final.shape}\")\n",
    "print(f\"  X_test:  {X_test_final.shape}\")\n",
    "\n",
    "\n",
    "# DATA QUALITY CHECK BEFORE SCALING\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATA QUALITY CHECK BEFORE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First, ensure all columns are numeric\n",
    "print(\"\\n1. Converting all features to numeric:\")\n",
    "non_numeric_cols = X_train_final.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"   Found {len(non_numeric_cols)} non-numeric columns:\")\n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"   - {col}: {X_train_final[col].dtype}\")\n",
    "        # Convert to numeric, coercing errors to NaN\n",
    "        X_train_final[col] = pd.to_numeric(X_train_final[col], errors='coerce')\n",
    "        X_val_final[col] = pd.to_numeric(X_val_final[col], errors='coerce')\n",
    "        X_test_final[col] = pd.to_numeric(X_test_final[col], errors='coerce')\n",
    "    print(\"   ✓ Converted all columns to numeric\")\n",
    "else:\n",
    "    print(\"   ✓ All columns are already numeric\")\n",
    "\n",
    "# Check for inf and NaN values in features\n",
    "print(\"\\n2. Checking for problematic values in training features:\")\n",
    "\n",
    "# Check for infinity\n",
    "inf_cols = []\n",
    "for col in X_train_final.columns:\n",
    "    if np.isinf(X_train_final[col]).any():\n",
    "        inf_count = np.isinf(X_train_final[col]).sum()\n",
    "        inf_cols.append((col, inf_count))\n",
    "        print(f\"   ⚠️  {col}: {inf_count} infinity values\")\n",
    "\n",
    "if len(inf_cols) == 0:\n",
    "    print(\"   ✓ No infinity values found\")\n",
    "\n",
    "# Check for NaN values\n",
    "nan_cols = []\n",
    "for col in X_train_final.columns:\n",
    "    if X_train_final[col].isna().any():\n",
    "        nan_count = X_train_final[col].isna().sum()\n",
    "        nan_cols.append((col, nan_count))\n",
    "        print(f\"   ⚠️  {col}: {nan_count} NaN values\")\n",
    "\n",
    "if len(nan_cols) == 0:\n",
    "    print(\"   ✓ No NaN values found\")\n",
    "\n",
    "# Check for extremely large values\n",
    "print(\"\\n3. Checking for extremely large values:\")\n",
    "large_value_cols = []\n",
    "for col in X_train_final.columns:\n",
    "    max_val = X_train_final[col].abs().max()\n",
    "    if max_val > 1e10:  # Values larger than 10 billion\n",
    "        large_value_cols.append((col, max_val))\n",
    "        print(f\"   ⚠️  {col}: max absolute value = {max_val:.2e}\")\n",
    "\n",
    "if len(large_value_cols) == 0:\n",
    "    print(\"   ✓ No extremely large values found\")\n",
    "\n",
    "\n",
    "# CLEAN THE DATA\n",
    "print(\"\\n4. CLEANING DATA:\")\n",
    "\n",
    "# Replace infinity with NaN\n",
    "print(\"   Replacing infinity values with NaN...\")\n",
    "X_train_final = X_train_final.replace([np.inf, -np.inf], np.nan)\n",
    "X_val_final = X_val_final.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_final = X_test_final.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Check how many NaN values we have\n",
    "total_nan = X_train_final.isna().sum().sum()\n",
    "print(f\"   Total NaN values in training set: {total_nan}\")\n",
    "\n",
    "if total_nan > 0:\n",
    "    # Option 1: Drop columns with too many NaN/inf values (>30%)\n",
    "    print(\"\\n   Analyzing problematic columns:\")\n",
    "    problematic_cols = []\n",
    "    for col in X_train_final.columns:\n",
    "        nan_pct = X_train_final[col].isna().sum() / len(X_train_final) * 100\n",
    "        if nan_pct > 30:\n",
    "            problematic_cols.append(col)\n",
    "            print(f\"   - {col}: {nan_pct:.1f}% missing → DROPPING\")\n",
    "    \n",
    "    if len(problematic_cols) > 0:\n",
    "        print(f\"\\n   Dropping {len(problematic_cols)} problematic columns\")\n",
    "        X_train_final = X_train_final.drop(columns=problematic_cols)\n",
    "        X_val_final = X_val_final.drop(columns=problematic_cols)\n",
    "        X_test_final = X_test_final.drop(columns=problematic_cols)\n",
    "    \n",
    "    # Option 2: Impute remaining NaN values with median\n",
    "    remaining_nan = X_train_final.isna().sum().sum()\n",
    "    if remaining_nan > 0:\n",
    "        print(f\"\\n   Imputing {remaining_nan} remaining NaN values with column median...\")\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        \n",
    "        # Fit on training, transform all\n",
    "        X_train_final_values = imputer.fit_transform(X_train_final)\n",
    "        X_val_final_values = imputer.transform(X_val_final)\n",
    "        X_test_final_values = imputer.transform(X_test_final)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        X_train_final = pd.DataFrame(X_train_final_values, \n",
    "                                      columns=X_train_final.columns, \n",
    "                                      index=X_train_final.index)\n",
    "        X_val_final = pd.DataFrame(X_val_final_values, \n",
    "                                    columns=X_val_final.columns, \n",
    "                                    index=X_val_final.index)\n",
    "        X_test_final = pd.DataFrame(X_test_final_values, \n",
    "                                     columns=X_test_final.columns, \n",
    "                                     index=X_test_final.index)\n",
    "        print(\"   ✓ Imputation complete\")\n",
    "\n",
    "# Final verification\n",
    "print(\"\\n5. FINAL VERIFICATION:\")\n",
    "final_inf = np.isinf(X_train_final.values).sum()\n",
    "final_nan = X_train_final.isna().sum().sum()\n",
    "print(f\"   Training set - Infinity values: {final_inf}\")\n",
    "print(f\"   Training set - NaN values: {final_nan}\")\n",
    "\n",
    "if final_inf == 0 and final_nan == 0:\n",
    "    print(\"   ✓ Data is clean and ready for scaling!\")\n",
    "else:\n",
    "    print(\"   ⚠️  WARNING: Data still has problematic values\")\n",
    "\n",
    "print(f\"\\nFinal feature count: {X_train_final.shape[1]} features\")\n",
    "print(f\"Final observation count: Train={len(X_train_final)}, Val={len(X_val_final)}, Test={len(X_test_final)}\")\n",
    "\n",
    "\n",
    "# 2. FEATURE SCALING\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"2. FEATURE SCALING (StandardScaler)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on TRAINING data only (prevent data leakage)\n",
    "print(f\"\\nFitting scaler on training data...\")\n",
    "scaler.fit(X_train_final)\n",
    "\n",
    "# Transform all three sets\n",
    "X_train_scaled = scaler.transform(X_train_final)\n",
    "X_val_scaled = scaler.transform(X_val_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_final.columns, index=X_train_final.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val_final.columns, index=X_val_final.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_final.columns, index=X_test_final.index)\n",
    "\n",
    "print(f\"✓ Scaling complete\")\n",
    "\n",
    "# Verify scaling worked (training set should have mean≈0, std≈1)\n",
    "print(f\"\\nScaling verification (training set):\")\n",
    "print(f\"  Mean of features: {X_train_scaled.mean().mean():.6f} (should be ≈0)\")\n",
    "print(f\"  Std of features:  {X_train_scaled.std().mean():.6f} (should be ≈1)\")\n",
    "\n",
    "# Show sample statistics for a few features\n",
    "print(f\"\\nSample feature statistics after scaling (training set):\")\n",
    "sample_features = X_train_final.columns[:5]  # First 5 features\n",
    "for feat in sample_features:\n",
    "    mean_val = X_train_scaled[feat].mean()\n",
    "    std_val = X_train_scaled[feat].std()\n",
    "    print(f\"  {feat}: mean={mean_val:.3f}, std={std_val:.3f}\")\n",
    "\n",
    "\n",
    "# 3. SUMMARY\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATA PREPARATION COMPLETE - READY FOR MODELING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nFinal datasets:\")\n",
    "print(f\"  Training:   {X_train_scaled.shape[0]} observations × {X_train_scaled.shape[1]} features\")\n",
    "print(f\"  Validation: {X_val_scaled.shape[0]} observations × {X_val_scaled.shape[1]} features\")\n",
    "print(f\"  Test:       {X_test_scaled.shape[0]} observations × {X_test_scaled.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nAvailable datasets:\")\n",
    "print(f\"  Scaled features:   X_train_scaled, X_val_scaled, X_test_scaled\")\n",
    "print(f\"  Unscaled features: X_train_final, X_val_final, X_test_final\")\n",
    "print(f\"  Target values:     y_train_final, y_val_final, y_test_final\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  - Use SCALED data for: Ridge, Lasso, ElasticNet\")\n",
    "print(f\"  - Use UNSCALED data for: OLS regression (with statsmodels)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Step 5.1: Multicollinearity Diagnostics (VIF Analysis)\n",
    "\n",
    "After observing the baseline OLS model results, we need to check for **multicollinearity** - when predictor variables are highly correlated with each other. Multicollinearity can cause:\n",
    "\n",
    "1. **Inflated coefficients** (extremely large positive/negative values)\n",
    "2. **Unstable estimates** (small data changes cause large coefficient changes)  \n",
    "3. **Poor generalization** (overfitting to training data)\n",
    "\n",
    "### Variance Inflation Factor (VIF)\n",
    "\n",
    "VIF measures how much a feature's variance is inflated due to correlation with other features:\n",
    "\n",
    "- **VIF = 1**: No correlation with other features\n",
    "- **VIF = 1-5**: Moderate correlation (acceptable)\n",
    "- **VIF = 5-10**: High correlation (concerning)\n",
    "- **VIF > 10**: Severe multicollinearity (problematic)\n",
    "\n",
    "**Formula**: VIF_i = 1 / (1 - R²_i), where R²_i is from regressing feature i against all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Calculate VIF for Baseline Features\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 5.1: MULTICOLLINEARITY DIAGNOSTICS (VIF)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each feature (excluding constant)\n",
    "print(\"\\n1. CALCULATING VIF SCORES:\")\n",
    "print(\"   VIF measures how much variance is inflated due to multicollinearity\")\n",
    "print(\"   VIF < 5: Low correlation (good)\")\n",
    "print(\"   VIF 5-10: Moderate correlation (concerning)\")\n",
    "print(\"   VIF > 10: Severe multicollinearity (problematic)\\n\")\n",
    "\n",
    "# Use the baseline features (without constant)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train_baseline.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_baseline.values, i) \n",
    "                   for i in range(X_train_baseline.shape[1])]\n",
    "\n",
    "# Sort by VIF (highest first)\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display VIF scores with color-coded warnings\n",
    "print(f\"{'Feature':<40s} {'VIF':>12s}  {'Status':>15s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in vif_data.iterrows():\n",
    "    feat = row['Feature']\n",
    "    vif = row['VIF']\n",
    "    \n",
    "    if vif > 10:\n",
    "        status = \"⚠️  SEVERE\"\n",
    "        color_marker = \"🔴\"\n",
    "    elif vif > 5:\n",
    "        status = \"⚠️  MODERATE\"\n",
    "        color_marker = \"🟡\"\n",
    "    else:\n",
    "        status = \"✓ GOOD\"\n",
    "        color_marker = \"🟢\"\n",
    "    \n",
    "    print(f\"{feat:<40s} {vif:12.2f}  {color_marker} {status}\")\n",
    "\n",
    "# Summary statistics\n",
    "severe_count = (vif_data['VIF'] > 10).sum()\n",
    "moderate_count = ((vif_data['VIF'] > 5) & (vif_data['VIF'] <= 10)).sum()\n",
    "good_count = (vif_data['VIF'] <= 5).sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. VIF SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Features with SEVERE multicollinearity (VIF > 10):   {severe_count}\")\n",
    "print(f\"  Features with MODERATE multicollinearity (VIF 5-10): {moderate_count}\")\n",
    "print(f\"  Features with LOW multicollinearity (VIF < 5):       {good_count}\")\n",
    "print(f\"\\n  Average VIF: {vif_data['VIF'].mean():.2f}\")\n",
    "print(f\"  Maximum VIF: {vif_data['VIF'].max():.2f}\")\n",
    "\n",
    "\n",
    "# 3. Visualize VIF scores\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. VIF VISUALIZATION:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create color-coded bars\n",
    "colors = ['#d73027' if vif > 10 else '#fee08b' if vif > 5 else '#1a9850' \n",
    "          for vif in vif_data['VIF']]\n",
    "\n",
    "bars = ax.barh(range(len(vif_data)), vif_data['VIF'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='Moderate threshold (VIF=5)')\n",
    "ax.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Severe threshold (VIF=10)')\n",
    "\n",
    "# Formatting\n",
    "ax.set_yticks(range(len(vif_data)))\n",
    "ax.set_yticklabels([f.replace('_', ' ')[:35] for f in vif_data['Feature']], fontsize=9)\n",
    "ax.set_xlabel('Variance Inflation Factor (VIF)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Multicollinearity Diagnostics: VIF Scores for Baseline Features', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add VIF values as text on bars\n",
    "for i, (idx, row) in enumerate(vif_data.iterrows()):\n",
    "    vif = row['VIF']\n",
    "    ax.text(vif + 0.5, i, f'{vif:.1f}', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify problematic features\n",
    "severe_features = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
    "if severe_features:\n",
    "    print(\"\\n⚠️  SEVERE MULTICOLLINEARITY DETECTED!\")\n",
    "    print(\"   Features with VIF > 10:\")\n",
    "    for feat in severe_features:\n",
    "        vif_val = vif_data[vif_data['Feature'] == feat]['VIF'].values[0]\n",
    "        print(f\"   - {feat}: VIF = {vif_val:.2f}\")\n",
    "    print(\"\\n   These features are highly correlated with other predictors.\")\n",
    "    print(\"   This explains the inflated coefficients in the OLS model.\")\n",
    "    print(\"   Regularization (Ridge/Lasso) will help address this issue.\")\n",
    "else:\n",
    "    print(\"✓ No severe multicollinearity detected (all VIF < 10)\")\n",
    "\n",
    "print(\"\\n✓ VIF analysis complete! Ready to proceed to regularized models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Step 6: Time Series Modeling with SARIMAX\n",
    "\n",
    "### Key Insight from Baseline OLS Results\n",
    "\n",
    "The baseline OLS model revealed **severe overfitting** (Validation R² = -2.19, indicating predictions worse than the mean). This is due to:\n",
    "\n",
    "1. **Insufficient observations** (42 train obs ÷ 12 features = 3.5 obs/feature, need 10-20)\n",
    "2. **Severe multicollinearity** (VIF analysis will show highly correlated predictors)\n",
    "3. **Ignoring temporal structure** - treating time series as cross-sectional data\n",
    "\n",
    "### Why SARIMAX is the Appropriate Approach\n",
    "\n",
    "Our data is **quarterly time series** with:\n",
    "- ✓ **Seasonality**: Quarterly patterns in housing/construction (Q1-Q4)\n",
    "- ✓ **Trend**: Long-term changes in export volumes\n",
    "- ✓ **Autocorrelation**: Current values depend on past values\n",
    "- ✓ **Exogenous predictors**: Economic indicators (housing starts, exchange rates, mortgage rates)\n",
    "\n",
    "**SARIMAX** = Seasonal AutoRegressive Integrated Moving Average with eXogenous variables\n",
    "\n",
    "**Model notation**: SARIMAX(p,d,q)(P,D,Q)[s]\n",
    "- **(p,d,q)**: Non-seasonal components\n",
    "  - p = AR order (autoregressive - how many past values to use)\n",
    "  - d = differencing order (to remove trend)\n",
    "  - q = MA order (moving average - how many past errors to use)\n",
    "- **(P,D,Q)**: Seasonal components\n",
    "  - P = seasonal AR order\n",
    "  - D = seasonal differencing\n",
    "  - Q = seasonal MA order\n",
    "- **[s]**: Seasonality period (4 for quarterly data)\n",
    "\n",
    "### Modeling Strategy\n",
    "\n",
    "1. **ACF/PACF Analysis**: Identify temporal patterns and estimate initial parameters\n",
    "2. **Baseline SARIMA**: Build model without exogenous variables (benchmark)\n",
    "3. **SARIMAX**: Add economic predictors (housing indicators, exchange rates)\n",
    "4. **Auto ARIMA**: Automatically find optimal (p,d,q)(P,D,Q) parameters\n",
    "5. **Model Comparison**: Use AIC/BIC and validation performance\n",
    "6. **Residual Diagnostics**: Check for white noise (Ljung-Box test)\n",
    "7. **Forecasting**: Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Step 6.1: ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: ACF and PACF Analysis for SARIMA Parameter Identification\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 6.1: ACF/PACF ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare time series data (use full dataset before train/val/test split)\n",
    "# We'll use the imputed data sorted by date\n",
    "df_ts = df_imputed.sort_values('Date').copy()\n",
    "df_ts['Date'] = pd.to_datetime(df_ts['Date'])\n",
    "df_ts = df_ts.set_index('Date')\n",
    "\n",
    "# Extract target variable\n",
    "ts_data = df_ts['CAD_Softwood_Export_to_US']\n",
    "\n",
    "print(\"\\n1. TIME SERIES OVERVIEW:\")\n",
    "print(f\"   Time period: {ts_data.index.min()} to {ts_data.index.max()}\")\n",
    "print(f\"   Frequency: Quarterly\")\n",
    "print(f\"   Number of observations: {len(ts_data)}\")\n",
    "print(f\"   Number of years: {len(ts_data)/4:.1f}\")\n",
    "\n",
    "\n",
    "# 2. STATIONARITY TEST (Augmented Dickey-Fuller)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STATIONARITY TEST (Augmented Dickey-Fuller)\")\n",
    "print(\"=\"*70)\n",
    "print(\"   H0: Time series has a unit root (non-stationary)\")\n",
    "print(\"   H1: Time series is stationary\\n\")\n",
    "\n",
    "adf_result = adfuller(ts_data.dropna(), autolag='AIC')\n",
    "print(f\"   ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"   p-value: {adf_result[1]:.4f}\")\n",
    "print(f\"   Critical values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"      {key}: {value:.4f}\")\n",
    "\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"\\n   ✓ STATIONARY: p-value < 0.05, reject H0\")\n",
    "    print(\"   → Series is stationary (d=0 likely sufficient)\")\n",
    "else:\n",
    "    print(\"\\n   ⚠️  NON-STATIONARY: p-value >= 0.05, fail to reject H0\")\n",
    "    print(\"   → Need differencing (d=1 or d=2)\")\n",
    "\n",
    "\n",
    "# 3. ACF AND PACF PLOTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. ACF AND PACF VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Original Time Series\n",
    "ax1 = axes[0, 0]\n",
    "ts_data.plot(ax=ax1, linewidth=2, color='#2c7bb6')\n",
    "ax1.set_title('Original Time Series: Canadian Softwood Exports to US', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Date', fontsize=10)\n",
    "ax1.set_ylabel('Export Volume', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: First Difference (to check if differencing helps)\n",
    "ax2 = axes[0, 1]\n",
    "ts_diff = ts_data.diff().dropna()\n",
    "ts_diff.plot(ax=ax2, linewidth=2, color='#d7191c')\n",
    "ax2.set_title('First Differenced Series (d=1)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Date', fontsize=10)\n",
    "ax2.set_ylabel('Change in Exports', fontsize=10)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: ACF (Autocorrelation Function)\n",
    "ax3 = axes[1, 0]\n",
    "plot_acf(ts_data.dropna(), lags=20, ax=ax3, alpha=0.05)\n",
    "ax3.set_title('ACF Plot: Identifies MA(q) order', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Lag', fontsize=10)\n",
    "ax3.set_ylabel('Autocorrelation', fontsize=10)\n",
    "\n",
    "# Plot 4: PACF (Partial Autocorrelation Function)\n",
    "ax4 = axes[1, 1]\n",
    "plot_pacf(ts_data.dropna(), lags=20, ax=ax4, alpha=0.05, method='ywm')\n",
    "ax4.set_title('PACF Plot: Identifies AR(p) order', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Lag', fontsize=10)\n",
    "ax4.set_ylabel('Partial Autocorrelation', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. INTERPRETATION GUIDE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. ACF/PACF INTERPRETATION GUIDE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📊 How to Read ACF/PACF Plots:\")\n",
    "print(\"   • ACF shows correlation between observation and its lags\")\n",
    "print(\"   • PACF shows direct correlation, removing intermediate lags\")\n",
    "print(\"   • Blue shaded area = 95% confidence interval (significance threshold)\")\n",
    "print(\"   • Spikes outside blue area = statistically significant\")\n",
    "\n",
    "print(\"\\n📐 Parameter Identification:\")\n",
    "print(\"   NON-SEASONAL COMPONENTS (p,d,q):\")\n",
    "print(\"   • p (AR order): Number of significant lags in PACF before cutoff\")\n",
    "print(\"   • d (differencing): 0 if stationary, 1 or 2 if not\")\n",
    "print(\"   • q (MA order): Number of significant lags in ACF before cutoff\")\n",
    "\n",
    "print(\"\\n   SEASONAL COMPONENTS (P,D,Q)[4]:\")\n",
    "print(\"   • Look for spikes at seasonal lags (4, 8, 12 for quarterly)\")\n",
    "print(\"   • P: Significant spike at lag 4 in PACF\")\n",
    "print(\"   • D: 1 if strong seasonal pattern, 0 otherwise\")\n",
    "print(\"   • Q: Significant spike at lag 4 in ACF\")\n",
    "\n",
    "print(\"\\n🔍 What to Look For:\")\n",
    "print(\"   • Gradual decay in ACF → suggests AR component (p > 0)\")\n",
    "print(\"   • Sharp cutoff in ACF → suggests MA component (q > 0)\")\n",
    "print(\"   • Sharp cutoff in PACF → confirms AR order\")\n",
    "print(\"   • Spikes at lags 4,8,12 → seasonal patterns (quarterly)\")\n",
    "\n",
    "\n",
    "# 5. INITIAL PARAMETER SUGGESTIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. SUGGESTED STARTING PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n   Based on visual inspection of ACF/PACF plots:\")\n",
    "print(\"   • Examine the plots above to identify:\")\n",
    "print(\"     - How many lags are significant in PACF? → suggests p\")\n",
    "print(\"     - How many lags are significant in ACF? → suggests q\")\n",
    "print(\"     - Is there a spike at lag 4? → suggests P or Q = 1\")\n",
    "print(\"     - Is series stationary? → determines d\")\n",
    "\n",
    "print(\"\\n   Typical starting values for quarterly data:\")\n",
    "print(\"   • SARIMA(1,1,1)(1,1,1)[4] - comprehensive seasonal model\")\n",
    "print(\"   • SARIMA(0,1,1)(0,1,1)[4] - simpler seasonal model\")\n",
    "print(\"   • SARIMA(1,1,0)(1,1,0)[4] - AR-focused model\")\n",
    "\n",
    "print(\"\\n   We will use auto_arima to systematically test combinations\")\n",
    "print(\"   and select the model with the lowest AIC/BIC.\")\n",
    "\n",
    "print(\"\\n✓ ACF/PACF analysis complete!\")\n",
    "print(\"   Ready to build SARIMA models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Step 6.2: Baseline SARIMA Model (No Exogenous Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Baseline SARIMA Model (Auto ARIMA)\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 6.2: BASELINE SARIMA MODEL (NO EXOGENOUS VARIABLES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "%pip install pmdarima --quiet\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. PREPARE TIME SERIES DATA WITH TRAIN/VAL/TEST SPLIT\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(\"   Using chronological 60/20/20 split for time series\\n\")\n",
    "\n",
    "# Use the imputed data (no feature engineering for SARIMA - it handles lags internally)\n",
    "ts_full = df_imputed.sort_values('Date').copy()\n",
    "ts_full['Date'] = pd.to_datetime(ts_full['Date'])\n",
    "ts_full = ts_full.set_index('Date')\n",
    "\n",
    "# Extract target variable\n",
    "y_series = ts_full['CAD_Softwood_Export_to_US'].copy()\n",
    "\n",
    "# Chronological split (60/20/20)\n",
    "n_total = len(y_series)\n",
    "n_train = int(n_total * 0.60)  # 47 observations\n",
    "n_val = int(n_total * 0.20)    # 16 observations\n",
    "n_test = n_total - n_train - n_val  # Remaining\n",
    "\n",
    "# Split data\n",
    "y_train_ts = y_series.iloc[:n_train]\n",
    "y_val_ts = y_series.iloc[n_train:n_train+n_val]\n",
    "y_test_ts = y_series.iloc[n_train+n_val:]\n",
    "\n",
    "print(f\"   Training set:   {y_train_ts.index.min()} to {y_train_ts.index.max()} ({len(y_train_ts)} obs)\")\n",
    "print(f\"   Validation set: {y_val_ts.index.min()} to {y_val_ts.index.max()} ({len(y_val_ts)} obs)\")\n",
    "print(f\"   Test set:       {y_test_ts.index.min()} to {y_test_ts.index.max()} ({len(y_test_ts)} obs)\")\n",
    "\n",
    "\n",
    "# 2. AUTO ARIMA - AUTOMATICALLY SELECT BEST PARAMETERS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. AUTO ARIMA: FINDING OPTIMAL PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Searching for best SARIMA(p,d,q)(P,D,Q)[4] parameters...\")\n",
    "print(\"   This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Run auto_arima\n",
    "auto_model = auto_arima(\n",
    "    y_train_ts,\n",
    "    start_p=0, max_p=3,           # Test p from 0 to 3\n",
    "    start_q=0, max_q=3,           # Test q from 0 to 3\n",
    "    start_P=0, max_P=2,           # Test seasonal P from 0 to 2\n",
    "    start_Q=0, max_Q=2,           # Test seasonal Q from 0 to 2\n",
    "    max_d=2, max_D=1,             # Max differencing\n",
    "    seasonal=True,                # Enable seasonal component\n",
    "    m=4,                          # Seasonal period = 4 (quarterly)\n",
    "    trace=True,                   # Print search progress\n",
    "    error_action='ignore',        # Ignore warnings\n",
    "    suppress_warnings=True,\n",
    "    stepwise=True,                # Use stepwise search (faster)\n",
    "    information_criterion='aic',  # Use AIC for model selection\n",
    "    n_jobs=-1                     # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Auto ARIMA complete!\")\n",
    "print(f\"\\n   Best Model: SARIMA{auto_model.order}{auto_model.seasonal_order}\")\n",
    "print(f\"   AIC: {auto_model.aic():.2f}\")\n",
    "print(f\"   BIC: {auto_model.bic():.2f}\")\n",
    "\n",
    "\n",
    "# 3. FIT FINAL MODEL AND MAKE PREDICTIONS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get in-sample predictions (training set)\n",
    "train_pred = auto_model.predict_in_sample()\n",
    "\n",
    "# Forecast validation set\n",
    "n_periods_val = len(y_val_ts)\n",
    "val_pred, val_conf_int = auto_model.predict(n_periods=n_periods_val, return_conf_int=True)\n",
    "\n",
    "# Calculate training metrics\n",
    "train_mae = mean_absolute_error(y_train_ts, train_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_ts, train_pred))\n",
    "train_mape = mean_absolute_percentage_error(y_train_ts, train_pred) * 100\n",
    "\n",
    "print(f\"\\nTraining Set Performance:\")\n",
    "print(f\"  MAE:   {train_mae:,.0f} thousand cubic meters\")\n",
    "print(f\"  RMSE:  {train_rmse:,.0f} thousand cubic meters\")\n",
    "print(f\"  MAPE:  {train_mape:.2f}%\")\n",
    "\n",
    "# Calculate validation metrics\n",
    "val_mae = mean_absolute_error(y_val_ts, val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_ts, val_pred))\n",
    "val_mape = mean_absolute_percentage_error(y_val_ts, val_pred) * 100\n",
    "\n",
    "print(f\"\\nValidation Set Performance:\")\n",
    "print(f\"  MAE:   {val_mae:,.0f} thousand cubic meters\")\n",
    "print(f\"  RMSE:  {val_rmse:,.0f} thousand cubic meters\")\n",
    "print(f\"  MAPE:  {val_mape:.2f}%\")\n",
    "\n",
    "# Generalization check\n",
    "print(f\"\\nGeneralization Check:\")\n",
    "mape_diff = val_mape - train_mape\n",
    "print(f\"  MAPE difference (val - train): {mape_diff:.2f}%\")\n",
    "if mape_diff < 5:\n",
    "    print(f\"  ✓ Good generalization (difference < 5%)\")\n",
    "elif mape_diff < 10:\n",
    "    print(f\"  ⚠️  Slight overfitting (difference 5-10%)\")\n",
    "else:\n",
    "    print(f\"  ⚠️  Overfitting detected (difference > 10%)\")\n",
    "\n",
    "\n",
    "# 4. RESIDUAL DIAGNOSTICS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. RESIDUAL DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get residuals\n",
    "residuals = auto_model.resid()\n",
    "\n",
    "# Ljung-Box test for white noise\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "\n",
    "print(\"\\nLjung-Box Test (Are residuals white noise?):\")\n",
    "print(f\"  H0: Residuals are white noise (no autocorrelation)\")\n",
    "print(f\"  Test statistic: {lb_test['lb_stat'].values[0]:.4f}\")\n",
    "print(f\"  p-value: {lb_test['lb_pvalue'].values[0]:.4f}\")\n",
    "\n",
    "if lb_test['lb_pvalue'].values[0] > 0.05:\n",
    "    print(\"  ✓ PASS: Residuals are white noise (p > 0.05)\")\n",
    "    print(\"  → Model has captured all temporal patterns\")\n",
    "else:\n",
    "    print(\"  ⚠️  FAIL: Residuals show autocorrelation (p < 0.05)\")\n",
    "    print(\"  → Model may be missing some patterns\")\n",
    "\n",
    "\n",
    "# 5. VISUALIZATION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. FORECAST VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training + Validation Forecast\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(y_train_ts.index, y_train_ts.values, label='Training Data', linewidth=2, color='#2c7bb6')\n",
    "ax1.plot(y_val_ts.index, y_val_ts.values, label='Actual (Validation)', linewidth=2, color='#1a9850')\n",
    "ax1.plot(y_val_ts.index, val_pred, label='Forecast', linewidth=2, color='#d73027', linestyle='--')\n",
    "ax1.fill_between(y_val_ts.index, val_conf_int[:, 0], val_conf_int[:, 1], \n",
    "                 alpha=0.3, color='#d73027', label='95% Confidence Interval')\n",
    "ax1.set_title(f'SARIMA{auto_model.order}{auto_model.seasonal_order} Forecast', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Date', fontsize=10)\n",
    "ax1.set_ylabel('Export Volume', fontsize=10)\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Actual vs Predicted (Validation)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_val_ts.values, val_pred, alpha=0.7, s=100, edgecolors='black')\n",
    "min_val = min(y_val_ts.min(), val_pred.min())\n",
    "max_val = max(y_val_ts.max(), val_pred.max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax2.set_title('Actual vs Predicted (Validation Set)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Actual', fontsize=10)\n",
    "ax2.set_ylabel('Predicted', fontsize=10)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals Over Time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(residuals.index, residuals.values, linewidth=1, color='#2c7bb6')\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Date', fontsize=10)\n",
    "ax3.set_ylabel('Residual', fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residual Histogram\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(residuals, bins=15, edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Residual', fontsize=10)\n",
    "ax4.set_ylabel('Frequency', fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Baseline SARIMA model complete!\")\n",
    "print(f\"   Best Model: SARIMA{auto_model.order}{auto_model.seasonal_order}\")\n",
    "print(f\"   Validation MAPE: {val_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### Step 6.3: SARIMAX with Exogenous Variables\n",
    "\n",
    "Now we'll add **economic predictors** to improve forecast accuracy. The SARIMAX model uses:\n",
    "- **Internal patterns**: Autocorrelation (SARIMA component)\n",
    "- **External drivers**: Economic indicators (exogenous variables)\n",
    "\n",
    "**Key exogenous variables to include**:\n",
    "- US housing indicators (housing starts, building permits, new home sales)\n",
    "- Economic factors (exchange rate, mortgage rates, GDP)\n",
    "- Canadian housing activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: SARIMAX with Exogenous Variables\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 6.3: SARIMAX WITH EXOGENOUS VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. SELECT EXOGENOUS VARIABLES\n",
    "print(\"\\n1. SELECTING EXOGENOUS VARIABLES:\")\n",
    "print(\"   Choosing key economic indicators based on business relevance\\n\")\n",
    "\n",
    "# Select meaningful exogenous predictors (no lags - SARIMAX handles that)\n",
    "exog_features = [\n",
    "    # US Housing Market (primary drivers of lumber demand)\n",
    "    'US_Housing_Start',\n",
    "    'US_Building_Permits',\n",
    "    'US_New_Home_Sales',\n",
    "    'US _BP_Single_Housing',\n",
    "    \n",
    "    # Economic Factors\n",
    "    'USCAD_Exchange_Rate',\n",
    "    'US_Mortgage_Interest_30Y',\n",
    "    'US_GDP',\n",
    "    'CAD_GDP',\n",
    "    \n",
    "    # Canadian Housing (competing domestic demand)\n",
    "    'CAD_Housing_Start',\n",
    "    'CAD_Building Permits'\n",
    "]\n",
    "\n",
    "# Filter to features that exist\n",
    "available_exog = [f for f in exog_features if f in ts_full.columns]\n",
    "print(f\"   Selected {len(available_exog)} exogenous variables:\")\n",
    "for feat in available_exog:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "# Prepare exogenous variable matrices\n",
    "X_exog = ts_full[available_exog].copy()\n",
    "\n",
    "# Split exogenous variables (same split as target)\n",
    "X_train_exog = X_exog.iloc[:n_train].copy()\n",
    "X_val_exog = X_exog.iloc[n_train:n_train+n_val].copy()\n",
    "X_test_exog = X_exog.iloc[n_train+n_val:].copy()\n",
    "\n",
    "# CHECK FOR MISSING VALUES\n",
    "print(f\"\\n2. DATA QUALITY CHECK:\")\n",
    "missing_counts = X_train_exog.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(\"   ⚠️  Missing values detected in exogenous variables:\")\n",
    "    for col in missing_counts[missing_counts > 0].index:\n",
    "        print(f\"   - {col}: {missing_counts[col]} missing\")\n",
    "    print(\"\\n   → Filling missing values with forward fill + backward fill\")\n",
    "    X_train_exog = X_train_exog.fillna(method='ffill').fillna(method='bfill')\n",
    "    X_val_exog = X_val_exog.fillna(method='ffill').fillna(method='bfill')\n",
    "    X_test_exog = X_test_exog.fillna(method='ffill').fillna(method='bfill')\n",
    "else:\n",
    "    print(\"   ✓ No missing values in exogenous variables\")\n",
    "\n",
    "# SCALE EXOGENOUS VARIABLES (CRITICAL for SARIMAX!)\n",
    "print(\"\\n3. SCALING EXOGENOUS VARIABLES:\")\n",
    "print(\"   Standardizing features (mean=0, std=1) for numerical stability\\n\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_exog = StandardScaler()\n",
    "X_train_exog_scaled = scaler_exog.fit_transform(X_train_exog)\n",
    "X_val_exog_scaled = scaler_exog.transform(X_val_exog)\n",
    "X_test_exog_scaled = scaler_exog.transform(X_test_exog)\n",
    "\n",
    "# Convert back to DataFrame to preserve column names\n",
    "X_train_exog_scaled = pd.DataFrame(X_train_exog_scaled, columns=available_exog, index=X_train_exog.index)\n",
    "X_val_exog_scaled = pd.DataFrame(X_val_exog_scaled, columns=available_exog, index=X_val_exog.index)\n",
    "X_test_exog_scaled = pd.DataFrame(X_test_exog_scaled, columns=available_exog, index=X_test_exog.index)\n",
    "\n",
    "print(f\"   Scaled data shapes:\")\n",
    "print(f\"   Training:   {X_train_exog_scaled.shape}\")\n",
    "print(f\"   Validation: {X_val_exog_scaled.shape}\")\n",
    "print(f\"   Test:       {X_test_exog_scaled.shape}\")\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\n   Verification (training set):\")\n",
    "print(f\"   Mean (should be ~0): {X_train_exog_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Std (should be ~1):  {X_train_exog_scaled.std().mean():.6f}\")\n",
    "\n",
    "\n",
    "# 4. AUTO ARIMA WITH EXOGENOUS VARIABLES\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. AUTO ARIMA WITH SCALED EXOGENOUS VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Searching for best SARIMAX parameters...\")\n",
    "print(\"   This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Run auto_arima with scaled exogenous variables\n",
    "try:\n",
    "    auto_model_X = auto_arima(\n",
    "        y_train_ts,\n",
    "        X=X_train_exog_scaled,        # Use SCALED exogenous variables\n",
    "        start_p=0, max_p=2,           # Reduced range for stability\n",
    "        start_q=0, max_q=2,\n",
    "        start_P=0, max_P=1,\n",
    "        start_Q=0, max_Q=1,\n",
    "        max_d=2, max_D=1,\n",
    "        seasonal=True,\n",
    "        m=4,\n",
    "        trace=True,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "        information_criterion='aic',\n",
    "        n_jobs=1                      # Use single core for stability\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Auto ARIMA (with exog) complete!\")\n",
    "    print(f\"\\n   Best Model: SARIMAX{auto_model_X.order}{auto_model_X.seasonal_order}\")\n",
    "    print(f\"   AIC: {auto_model_X.aic():.2f}\")\n",
    "    print(f\"   BIC: {auto_model_X.bic():.2f}\")\n",
    "    \n",
    "    sarimax_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Auto ARIMA with exogenous variables failed: {str(e)[:100]}\")\n",
    "    print(\"   → This can happen with too many exogenous variables or high multicollinearity\")\n",
    "    print(\"   → Trying with reduced set of exogenous variables...\")\n",
    "    \n",
    "    # Try with just top 5 most important features\n",
    "    reduced_exog = ['US_Housing_Start', 'USCAD_Exchange_Rate', 'US_Mortgage_Interest_30Y', \n",
    "                    'US_Building_Permits', 'US_GDP']\n",
    "    reduced_exog = [f for f in reduced_exog if f in available_exog]\n",
    "    \n",
    "    X_train_reduced = X_train_exog_scaled[reduced_exog]\n",
    "    X_val_reduced = X_val_exog_scaled[reduced_exog]\n",
    "    \n",
    "    print(f\"\\n   Trying with {len(reduced_exog)} key variables:\")\n",
    "    for feat in reduced_exog:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "    try:\n",
    "        auto_model_X = auto_arima(\n",
    "            y_train_ts,\n",
    "            X=X_train_reduced,\n",
    "            start_p=0, max_p=2,\n",
    "            start_q=0, max_q=2,\n",
    "            start_P=0, max_P=1,\n",
    "            start_Q=0, max_Q=1,\n",
    "            max_d=2, max_D=1,\n",
    "            seasonal=True,\n",
    "            m=4,\n",
    "            trace=True,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True,\n",
    "            stepwise=True,\n",
    "            information_criterion='aic',\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✓ Auto ARIMA (with reduced exog) complete!\")\n",
    "        print(f\"\\n   Best Model: SARIMAX{auto_model_X.order}{auto_model_X.seasonal_order}\")\n",
    "        print(f\"   AIC: {auto_model_X.aic():.2f}\")\n",
    "        print(f\"   BIC: {auto_model_X.bic():.2f}\")\n",
    "        \n",
    "        # Update to use reduced set\n",
    "        X_val_exog_scaled = X_val_reduced\n",
    "        available_exog = reduced_exog\n",
    "        sarimax_success = True\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"\\n⚠️  Still failed with reduced variables: {str(e2)[:100]}\")\n",
    "        print(\"   → Will skip SARIMAX and use SARIMA only for comparison\")\n",
    "        sarimax_success = False\n",
    "\n",
    "\n",
    "# Only proceed if SARIMAX succeeded\n",
    "if sarimax_success:\n",
    "    # 5. MODEL EVALUATION\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"5. MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get in-sample predictions\n",
    "    train_pred_X = auto_model_X.predict_in_sample(X=X_train_exog_scaled[available_exog] if len(available_exog) < len(X_train_exog_scaled.columns) else X_train_exog_scaled)\n",
    "    \n",
    "    # Forecast validation set\n",
    "    val_pred_X, val_conf_int_X = auto_model_X.predict(\n",
    "        n_periods=n_periods_val, \n",
    "        X=X_val_exog_scaled,\n",
    "        return_conf_int=True\n",
    "    )\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_mae_X = mean_absolute_error(y_train_ts, train_pred_X)\n",
    "    train_rmse_X = np.sqrt(mean_squared_error(y_train_ts, train_pred_X))\n",
    "    train_mape_X = mean_absolute_percentage_error(y_train_ts, train_pred_X) * 100\n",
    "    \n",
    "    print(f\"\\nTraining Set Performance:\")\n",
    "    print(f\"  MAE:   {train_mae_X:,.0f} thousand cubic meters\")\n",
    "    print(f\"  RMSE:  {train_rmse_X:,.0f} thousand cubic meters\")\n",
    "    print(f\"  MAPE:  {train_mape_X:.2f}%\")\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_mae_X = mean_absolute_error(y_val_ts, val_pred_X)\n",
    "    val_rmse_X = np.sqrt(mean_squared_error(y_val_ts, val_pred_X))\n",
    "    val_mape_X = mean_absolute_percentage_error(y_val_ts, val_pred_X) * 100\n",
    "    \n",
    "    print(f\"\\nValidation Set Performance:\")\n",
    "    print(f\"  MAE:   {val_mae_X:,.0f} thousand cubic meters\")\n",
    "    print(f\"  RMSE:  {val_rmse_X:,.0f} thousand cubic meters\")\n",
    "    print(f\"  MAPE:  {val_mape_X:.2f}%\")\n",
    "    \n",
    "    # Generalization check\n",
    "    mape_diff_X = val_mape_X - train_mape_X\n",
    "    print(f\"\\nGeneralization Check:\")\n",
    "    print(f\"  MAPE difference (val - train): {mape_diff_X:.2f}%\")\n",
    "    if mape_diff_X < 5:\n",
    "        print(f\"  ✓ Good generalization (difference < 5%)\")\n",
    "    elif mape_diff_X < 10:\n",
    "        print(f\"  ⚠️  Slight overfitting (difference 5-10%)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Overfitting detected (difference > 10%)\")\n",
    "    \n",
    "    \n",
    "    # 6. COMPARE SARIMA VS SARIMAX\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"6. MODEL COMPARISON: SARIMA vs SARIMAX\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['SARIMA (no exog)', f'SARIMAX ({len(available_exog)} exog)'],\n",
    "        'Specification': [\n",
    "            f\"SARIMA{auto_model.order}{auto_model.seasonal_order}\",\n",
    "            f\"SARIMAX{auto_model_X.order}{auto_model_X.seasonal_order}\"\n",
    "        ],\n",
    "        'AIC': [auto_model.aic(), auto_model_X.aic()],\n",
    "        'BIC': [auto_model.bic(), auto_model_X.bic()],\n",
    "        'Train_MAPE': [train_mape, train_mape_X],\n",
    "        'Val_MAPE': [val_mape, val_mape_X],\n",
    "        'Val_MAE': [val_mae, val_mae_X],\n",
    "        'Val_RMSE': [val_rmse, val_rmse_X]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\", comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Determine best model\n",
    "    if val_mape_X < val_mape:\n",
    "        improvement = ((val_mape - val_mape_X) / val_mape) * 100\n",
    "        print(f\"\\n✓ SARIMAX WINS!\")\n",
    "        print(f\"   Validation MAPE improved by {improvement:.1f}%\")\n",
    "        print(f\"   Exogenous variables add predictive value\")\n",
    "        best_model = 'SARIMAX'\n",
    "        best_model_obj = auto_model_X\n",
    "        best_val_pred = val_pred_X\n",
    "        best_val_conf = val_conf_int_X\n",
    "    else:\n",
    "        difference = val_mape_X - val_mape\n",
    "        print(f\"\\n⚠️  SARIMA WINS\")\n",
    "        print(f\"   SARIMAX is {difference:.1f}% worse on validation MAPE\")\n",
    "        print(f\"   Exogenous variables may not add value (or cause overfitting)\")\n",
    "        best_model = 'SARIMA'\n",
    "        best_model_obj = auto_model\n",
    "        best_val_pred = val_pred\n",
    "        best_val_conf = val_conf_int\n",
    "    \n",
    "    \n",
    "    # 7. VISUALIZATION\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"7. SARIMAX FORECAST VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: SARIMAX Forecast\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(y_train_ts.index, y_train_ts.values, label='Training Data', linewidth=2, color='#2c7bb6')\n",
    "    ax1.plot(y_val_ts.index, y_val_ts.values, label='Actual (Validation)', linewidth=2, color='#1a9850')\n",
    "    ax1.plot(y_val_ts.index, val_pred_X, label='SARIMAX Forecast', linewidth=2, color='#d73027', linestyle='--')\n",
    "    ax1.fill_between(y_val_ts.index, val_conf_int_X[:, 0], val_conf_int_X[:, 1], \n",
    "                     alpha=0.3, color='#d73027', label='95% CI')\n",
    "    ax1.set_title(f'SARIMAX{auto_model_X.order}{auto_model_X.seasonal_order} with {len(available_exog)} Exog Vars', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Date', fontsize=10)\n",
    "    ax1.set_ylabel('Export Volume', fontsize=10)\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: SARIMA vs SARIMAX Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(y_val_ts.index, y_val_ts.values, label='Actual', linewidth=2, color='black')\n",
    "    ax2.plot(y_val_ts.index, val_pred, label=f'SARIMA (MAPE={val_mape:.1f}%)', \n",
    "             linewidth=2, color='#2c7bb6', linestyle='--', alpha=0.7)\n",
    "    ax2.plot(y_val_ts.index, val_pred_X, label=f'SARIMAX (MAPE={val_mape_X:.1f}%)', \n",
    "             linewidth=2, color='#d73027', linestyle='--', alpha=0.7)\n",
    "    ax2.set_title('Model Comparison: SARIMA vs SARIMAX', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Date', fontsize=10)\n",
    "    ax2.set_ylabel('Export Volume', fontsize=10)\n",
    "    ax2.legend(loc='best', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Actual vs Predicted\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.scatter(y_val_ts.values, val_pred_X, alpha=0.7, s=100, edgecolors='black')\n",
    "    min_val = min(y_val_ts.min(), val_pred_X.min())\n",
    "    max_val = max(y_val_ts.max(), val_pred_X.max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    ax3.set_title('Actual vs Predicted (SARIMAX)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Actual', fontsize=10)\n",
    "    ax3.set_ylabel('Predicted', fontsize=10)\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Performance Metrics Bar Chart\n",
    "    ax4 = axes[1, 1]\n",
    "    metrics = ['MAPE (%)', 'MAE (÷1000)', 'RMSE (÷1000)']\n",
    "    sarima_vals = [val_mape, val_mae/1000, val_rmse/1000]\n",
    "    sarimax_vals = [val_mape_X, val_mae_X/1000, val_rmse_X/1000]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, sarima_vals, width, label='SARIMA', color='#2c7bb6')\n",
    "    ax4.bar(x + width/2, sarimax_vals, width, label='SARIMAX', color='#d73027')\n",
    "    ax4.set_title('Validation Performance Comparison', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Value', fontsize=10)\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(metrics, fontsize=9)\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ SARIMAX model complete!\")\n",
    "    print(f\"   Best overall model: {best_model}\")\n",
    "    print(f\"   Ready for final test set evaluation\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️  SARIMAX failed to fit - using SARIMA only\")\n",
    "    best_model = 'SARIMA'\n",
    "    best_model_obj = auto_model\n",
    "    best_val_pred = val_pred\n",
    "    best_val_conf = val_conf_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### Step 7: Final Model Evaluation on Test Set\n",
    "\n",
    "Now we evaluate the best model on the **held-out test set** - data the model has never seen. This provides an unbiased estimate of real-world forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Final Model Evaluation on Test Set\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 7: FINAL MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBest Model Selected: {best_model}\")\n",
    "print(f\"   Specification: {best_model_obj.order}{best_model_obj.seasonal_order}\")\n",
    "\n",
    "# 1. FORECAST ON TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. FORECASTING ON HELD-OUT TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"   Test set represents future periods the model has never seen\")\n",
    "print(f\"   Test period: {y_test_ts.index.min()} to {y_test_ts.index.max()}\")\n",
    "print(f\"   Number of periods: {len(y_test_ts)}\\n\")\n",
    "\n",
    "# Determine if we need exogenous variables\n",
    "if best_model == 'SARIMAX':\n",
    "    print(\"   Using SARIMAX - providing exogenous variables for test period\")\n",
    "    # Refit model on train + validation combined for final test\n",
    "    y_train_val_combined = pd.concat([y_train_ts, y_val_ts])\n",
    "    \n",
    "    # Get the appropriate exogenous variables\n",
    "    if 'X_train_reduced' in locals():\n",
    "        # Use reduced set if that's what worked\n",
    "        X_train_val_combined = pd.concat([X_train_exog_scaled[available_exog], X_val_exog_scaled])\n",
    "        X_test_final = X_test_exog_scaled[available_exog]\n",
    "    else:\n",
    "        X_train_val_combined = pd.concat([X_train_exog_scaled, X_val_exog_scaled])\n",
    "        X_test_final = X_test_exog_scaled\n",
    "    \n",
    "    # Refit on train+val\n",
    "    print(f\"   Refitting model on combined train+validation ({len(y_train_val_combined)} obs)...\")\n",
    "    final_model = auto_arima(\n",
    "        y_train_val_combined,\n",
    "        X=X_train_val_combined,\n",
    "        start_p=best_model_obj.order[0], max_p=best_model_obj.order[0],  # Use best params\n",
    "        start_q=best_model_obj.order[2], max_q=best_model_obj.order[2],\n",
    "        start_P=best_model_obj.seasonal_order[0], max_P=best_model_obj.seasonal_order[0],\n",
    "        start_Q=best_model_obj.seasonal_order[2], max_Q=best_model_obj.seasonal_order[2],\n",
    "        d=best_model_obj.order[1],\n",
    "        D=best_model_obj.seasonal_order[1],\n",
    "        seasonal=True,\n",
    "        m=4,\n",
    "        trace=False,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True\n",
    "    )\n",
    "    \n",
    "    # Forecast test set\n",
    "    test_pred, test_conf_int = final_model.predict(\n",
    "        n_periods=len(y_test_ts),\n",
    "        X=X_test_final,\n",
    "        return_conf_int=True\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"   Using SARIMA - no exogenous variables needed\")\n",
    "    # Refit on train + validation combined\n",
    "    y_train_val_combined = pd.concat([y_train_ts, y_val_ts])\n",
    "    \n",
    "    print(f\"   Refitting model on combined train+validation ({len(y_train_val_combined)} obs)...\")\n",
    "    final_model = auto_arima(\n",
    "        y_train_val_combined,\n",
    "        start_p=best_model_obj.order[0], max_p=best_model_obj.order[0],\n",
    "        start_q=best_model_obj.order[2], max_q=best_model_obj.order[2],\n",
    "        start_P=best_model_obj.seasonal_order[0], max_P=best_model_obj.seasonal_order[0],\n",
    "        start_Q=best_model_obj.seasonal_order[2], max_Q=best_model_obj.seasonal_order[2],\n",
    "        d=best_model_obj.order[1],\n",
    "        D=best_model_obj.seasonal_order[1],\n",
    "        seasonal=True,\n",
    "        m=4,\n",
    "        trace=False,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True\n",
    "    )\n",
    "    \n",
    "    # Forecast test set\n",
    "    test_pred, test_conf_int = final_model.predict(\n",
    "        n_periods=len(y_test_ts),\n",
    "        return_conf_int=True\n",
    "    )\n",
    "\n",
    "print(\"   ✓ Test set forecast complete!\")\n",
    "\n",
    "\n",
    "# 2. CALCULATE TEST SET METRICS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. TEST SET PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test_ts, test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_ts, test_pred))\n",
    "test_mape = mean_absolute_percentage_error(y_test_ts, test_pred) * 100\n",
    "\n",
    "print(f\"\\nFinal Test Set Performance:\")\n",
    "print(f\"  MAE:   {test_mae:,.0f} thousand cubic meters\")\n",
    "print(f\"  RMSE:  {test_rmse:,.0f} thousand cubic meters\")\n",
    "print(f\"  MAPE:  {test_mape:.2f}%\")\n",
    "\n",
    "# Compare with validation performance\n",
    "print(f\"\\nComparison to Validation Set:\")\n",
    "if best_model == 'SARIMAX' and sarimax_success:\n",
    "    print(f\"  Validation MAPE: {val_mape_X:.2f}%\")\n",
    "    mape_diff_final = abs(test_mape - val_mape_X)\n",
    "else:\n",
    "    print(f\"  Validation MAPE: {val_mape:.2f}%\")\n",
    "    mape_diff_final = abs(test_mape - val_mape)\n",
    "\n",
    "print(f\"  Test MAPE:       {test_mape:.2f}%\")\n",
    "print(f\"  Difference:      {mape_diff_final:.2f}%\")\n",
    "\n",
    "if mape_diff_final < 3:\n",
    "    print(\"  ✓ Consistent performance across validation and test\")\n",
    "elif mape_diff_final < 5:\n",
    "    print(\"  ⚠️  Slight difference (but acceptable)\")\n",
    "else:\n",
    "    print(\"  ⚠️  Significant difference - model may be sensitive to data splits\")\n",
    "\n",
    "\n",
    "# 3. RESIDUAL DIAGNOSTICS ON TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. RESIDUAL DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate test residuals\n",
    "test_residuals = y_test_ts.values - test_pred\n",
    "\n",
    "# Ljung-Box test for test residuals\n",
    "lb_test_final = acorr_ljungbox(test_residuals, lags=[5], return_df=True)\n",
    "\n",
    "print(\"\\nLjung-Box Test (Test Set Residuals):\")\n",
    "print(f\"  H0: Residuals are white noise (no autocorrelation)\")\n",
    "print(f\"  Test statistic: {lb_test_final['lb_stat'].values[0]:.4f}\")\n",
    "print(f\"  p-value: {lb_test_final['lb_pvalue'].values[0]:.4f}\")\n",
    "\n",
    "if lb_test_final['lb_pvalue'].values[0] > 0.05:\n",
    "    print(\"  ✓ PASS: Residuals are white noise (p > 0.05)\")\n",
    "    print(\"  → Model has captured all temporal patterns\")\n",
    "else:\n",
    "    print(\"  ⚠️  FAIL: Residuals show autocorrelation (p < 0.05)\")\n",
    "    print(\"  → Model may be missing some patterns\")\n",
    "\n",
    "# Residual statistics\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean:     {test_residuals.mean():,.0f} (should be close to 0)\")\n",
    "print(f\"  Std Dev:  {test_residuals.std():,.0f}\")\n",
    "print(f\"  Min:      {test_residuals.min():,.0f}\")\n",
    "print(f\"  Max:      {test_residuals.max():,.0f}\")\n",
    "\n",
    "\n",
    "# 4. COMPREHENSIVE VISUALIZATION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. COMPREHENSIVE FORECAST VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Full Time Series with Forecast\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(y_train_ts.index, y_train_ts.values, label='Training Data', linewidth=2, color='#2c7bb6')\n",
    "ax1.plot(y_val_ts.index, y_val_ts.values, label='Validation Data', linewidth=2, color='#fdae61')\n",
    "ax1.plot(y_test_ts.index, y_test_ts.values, label='Test Data (Actual)', linewidth=2, color='#1a9850', marker='o')\n",
    "ax1.plot(y_test_ts.index, test_pred, label='Test Forecast', linewidth=2, color='#d73027', \n",
    "         linestyle='--', marker='s')\n",
    "ax1.fill_between(y_test_ts.index, test_conf_int[:, 0], test_conf_int[:, 1], \n",
    "                 alpha=0.2, color='#d73027', label='95% Confidence Interval')\n",
    "ax1.set_title(f'{best_model} Model: Full Time Series with Test Set Forecast', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date', fontsize=11)\n",
    "ax1.set_ylabel('Canadian Softwood Exports (thousand cubic meters)', fontsize=11)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test Set Forecast Zoom\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "# Show last few validation points for context\n",
    "context_start = max(0, len(y_val_ts) - 4)\n",
    "context_dates = y_val_ts.index[context_start:]\n",
    "context_values = y_val_ts.values[context_start:]\n",
    "ax2.plot(context_dates, context_values, label='Validation (context)', \n",
    "         linewidth=2, color='#fdae61', alpha=0.5)\n",
    "ax2.plot(y_test_ts.index, y_test_ts.values, label='Actual', linewidth=2.5, \n",
    "         color='#1a9850', marker='o', markersize=8)\n",
    "ax2.plot(y_test_ts.index, test_pred, label='Forecast', linewidth=2.5, \n",
    "         color='#d73027', linestyle='--', marker='s', markersize=8)\n",
    "ax2.fill_between(y_test_ts.index, test_conf_int[:, 0], test_conf_int[:, 1], \n",
    "                 alpha=0.3, color='#d73027', label='95% CI')\n",
    "ax2.set_title(f'Test Set Forecast Detail (MAPE={test_mape:.2f}%)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Date', fontsize=10)\n",
    "ax2.set_ylabel('Export Volume', fontsize=10)\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actual vs Predicted (Test Set)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.scatter(y_test_ts.values, test_pred, alpha=0.8, s=150, edgecolors='black', linewidth=1.5)\n",
    "min_val = min(y_test_ts.min(), test_pred.min())\n",
    "max_val = max(y_test_ts.max(), test_pred.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2.5, label='Perfect Prediction')\n",
    "# Add error bars\n",
    "for i in range(len(y_test_ts)):\n",
    "    ax3.plot([y_test_ts.values[i], y_test_ts.values[i]], \n",
    "             [y_test_ts.values[i], test_pred[i]], \n",
    "             color='gray', alpha=0.3, linewidth=1)\n",
    "ax3.set_title('Actual vs Predicted (Test Set)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Actual Exports', fontsize=10)\n",
    "ax3.set_ylabel('Predicted Exports', fontsize=10)\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Test Residuals Over Time\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "ax4.plot(y_test_ts.index, test_residuals, linewidth=2, color='#2c7bb6', marker='o')\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.axhline(y=test_residuals.std(), color='orange', linestyle=':', linewidth=1.5, alpha=0.7, label='±1 Std Dev')\n",
    "ax4.axhline(y=-test_residuals.std(), color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "ax4.set_title('Test Set Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Date', fontsize=10)\n",
    "ax4.set_ylabel('Residual (Actual - Predicted)', fontsize=10)\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Residual Distribution\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "ax5.hist(test_residuals, bins=8, edgecolor='black', alpha=0.7, color='#2c7bb6')\n",
    "ax5.axvline(x=0, color='red', linestyle='--', linewidth=2.5, label='Zero')\n",
    "ax5.axvline(x=test_residuals.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean={test_residuals.mean():.0f}')\n",
    "ax5.set_title('Test Set Residual Distribution', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlabel('Residual', fontsize=10)\n",
    "ax5.set_ylabel('Frequency', fontsize=10)\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 5. FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📊 Best Model: {best_model}\")\n",
    "print(f\"   Specification: {best_model_obj.order}{best_model_obj.seasonal_order}\")\n",
    "if best_model == 'SARIMAX':\n",
    "    print(f\"   Exogenous Variables: {len(available_exog)}\")\n",
    "    print(f\"   Variables: {', '.join(available_exog[:5])}{'...' if len(available_exog) > 5 else ''}\")\n",
    "\n",
    "print(f\"\\n📈 Performance Metrics:\")\n",
    "print(f\"   Training MAPE:   {train_mape if best_model == 'SARIMA' else train_mape_X:.2f}%\")\n",
    "print(f\"   Validation MAPE: {val_mape if best_model == 'SARIMA' else val_mape_X:.2f}%\")\n",
    "print(f\"   Test MAPE:       {test_mape:.2f}%\")\n",
    "\n",
    "print(f\"\\n✅ Model Quality:\")\n",
    "if test_mape < 10:\n",
    "    print(f\"   Excellent forecast accuracy (MAPE < 10%)\")\n",
    "elif test_mape < 15:\n",
    "    print(f\"   Good forecast accuracy (MAPE < 15%)\")\n",
    "elif test_mape < 20:\n",
    "    print(f\"   Acceptable forecast accuracy (MAPE < 20%)\")\n",
    "else:\n",
    "    print(f\"   Moderate forecast accuracy (MAPE >= 20%)\")\n",
    "\n",
    "print(f\"\\n🎯 Business Insights:\")\n",
    "print(f\"   Average forecast error: ±{test_mae:,.0f} thousand cubic meters\")\n",
    "print(f\"   95% of forecasts within: ±{1.96 * test_rmse:,.0f} thousand cubic meters\")\n",
    "\n",
    "print(\"\\n✓ Final model evaluation complete!\")\n",
    "print(\"   Model is ready for production forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## Step 8: Detailed Forecast Results Table\n",
    "\n",
    "View the actual predicted values for each quarter in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Detailed Forecast Results\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 8: DETAILED FORECAST PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTarget Variable: Canadian Softwood Lumber Exports to US\")\n",
    "print(\"Unit: Thousand cubic meters\")\n",
    "print(f\"Forecast Period: {y_test_ts.index.min()} to {y_test_ts.index.max()}\\n\")\n",
    "\n",
    "# Create detailed forecast table\n",
    "forecast_results = pd.DataFrame({\n",
    "    'Date': y_test_ts.index,\n",
    "    'Quarter': [f\"Q{date.quarter} {date.year}\" for date in y_test_ts.index],\n",
    "    'Actual_Exports': y_test_ts.values,\n",
    "    'Predicted_Exports': test_pred,\n",
    "    'Lower_95%_CI': test_conf_int[:, 0],\n",
    "    'Upper_95%_CI': test_conf_int[:, 1],\n",
    "    'Forecast_Error': y_test_ts.values - test_pred,\n",
    "    'Absolute_Error': np.abs(y_test_ts.values - test_pred),\n",
    "    'Percentage_Error': ((y_test_ts.values - test_pred) / y_test_ts.values) * 100\n",
    "})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FORECAST RESULTS TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll values in thousand cubic meters\\n\")\n",
    "\n",
    "# Display formatted table\n",
    "print(f\"{'Quarter':<12} {'Actual':>15} {'Predicted':>15} {'Error':>15} {'Error %':>10} {'95% CI Range':>30}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for idx, row in forecast_results.iterrows():\n",
    "    ci_range = f\"[{row['Lower_95%_CI']:,.0f}, {row['Upper_95%_CI']:,.0f}]\"\n",
    "    print(f\"{row['Quarter']:<12} {row['Actual_Exports']:>15,.0f} {row['Predicted_Exports']:>15,.0f} \"\n",
    "          f\"{row['Forecast_Error']:>15,.0f} {row['Percentage_Error']:>9.2f}% {ci_range:>30}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORECAST SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nAverage Actual Exports:     {forecast_results['Actual_Exports'].mean():>15,.0f} thousand m³\")\n",
    "print(f\"Average Predicted Exports:  {forecast_results['Predicted_Exports'].mean():>15,.0f} thousand m³\")\n",
    "print(f\"\\nMean Absolute Error (MAE):  {forecast_results['Absolute_Error'].mean():>15,.0f} thousand m³\")\n",
    "print(f\"Root Mean Squared Error:    {test_rmse:>15,.0f} thousand m³\")\n",
    "print(f\"Mean Absolute % Error:      {forecast_results['Percentage_Error'].abs().mean():>14.2f}%\")\n",
    "\n",
    "print(f\"\\nSmallest Error:  {forecast_results['Absolute_Error'].min():>15,.0f} thousand m³ ({forecast_results.loc[forecast_results['Absolute_Error'].idxmin(), 'Quarter']})\")\n",
    "print(f\"Largest Error:   {forecast_results['Absolute_Error'].max():>15,.0f} thousand m³ ({forecast_results.loc[forecast_results['Absolute_Error'].idxmax(), 'Quarter']})\")\n",
    "\n",
    "# Check if actuals fall within confidence intervals\n",
    "within_ci = ((forecast_results['Actual_Exports'] >= forecast_results['Lower_95%_CI']) & \n",
    "             (forecast_results['Actual_Exports'] <= forecast_results['Upper_95%_CI'])).sum()\n",
    "ci_coverage = (within_ci / len(forecast_results)) * 100\n",
    "\n",
    "print(f\"\\nConfidence Interval Coverage: {within_ci}/{len(forecast_results)} ({ci_coverage:.1f}%)\")\n",
    "if ci_coverage >= 90:\n",
    "    print(\"  ✓ Good coverage - confidence intervals are reliable\")\n",
    "else:\n",
    "    print(\"  ⚠️  Low coverage - confidence intervals may be too narrow\")\n",
    "\n",
    "\n",
    "# Business interpretation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_actual = forecast_results['Actual_Exports'].mean()\n",
    "avg_predicted = forecast_results['Predicted_Exports'].mean()\n",
    "bias = avg_predicted - avg_actual\n",
    "\n",
    "print(f\"\\n📊 Forecast Accuracy:\")\n",
    "if test_mape < 10:\n",
    "    print(f\"   Excellent - Model achieves {test_mape:.2f}% MAPE\")\n",
    "    print(f\"   Forecasts are highly reliable for business planning\")\n",
    "elif test_mape < 15:\n",
    "    print(f\"   Good - Model achieves {test_mape:.2f}% MAPE\")\n",
    "    print(f\"   Forecasts are suitable for strategic planning\")\n",
    "else:\n",
    "    print(f\"   Moderate - Model achieves {test_mape:.2f}% MAPE\")\n",
    "    print(f\"   Forecasts provide directional guidance\")\n",
    "\n",
    "print(f\"\\n📈 Forecast Bias:\")\n",
    "if abs(bias) < avg_actual * 0.05:  # Less than 5% bias\n",
    "    print(f\"   Minimal bias ({bias:,.0f} thousand m³)\")\n",
    "    print(f\"   Model is well-calibrated (not systematically over/under-predicting)\")\n",
    "elif bias > 0:\n",
    "    print(f\"   Slight over-prediction (+{bias:,.0f} thousand m³ on average)\")\n",
    "    print(f\"   Model tends to predict slightly higher than actual\")\n",
    "else:\n",
    "    print(f\"   Slight under-prediction ({bias:,.0f} thousand m³ on average)\")\n",
    "    print(f\"   Model tends to predict slightly lower than actual\")\n",
    "\n",
    "print(f\"\\n🎯 Forecast Range:\")\n",
    "print(f\"   Typical forecast: {avg_predicted:,.0f} ± {1.96 * test_rmse:,.0f} thousand m³ (95% CI)\")\n",
    "print(f\"   Expected error range: ±{test_mape:.1f}% on average\")\n",
    "\n",
    "print(\"\\n✓ Forecast results complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softwood-forecasting-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
